"""
Celery Worker Configuration and Tasks

This module handles asynchronous processing of reviews:
1. Translation via Qwen API
2. Sentiment analysis
3. Database updates
"""
import logging
import time
import random
from typing import Optional
from functools import wraps

from celery import Celery
from sqlalchemy import create_engine, select, update, and_, func
from sqlalchemy.orm import sessionmaker
import redis
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ¯ æ™ºèƒ½è¯„è®ºåˆ†ç±»å™¨ï¼ˆæ ¹æ®é•¿åº¦å’Œè´¨é‡å·®å¼‚åŒ–å¤„ç†ï¼‰
# ============================================================================

class ReviewClassifier:
    """
    è¯„è®ºåˆ†ç±»å™¨ï¼šæ ¹æ®è¯„è®ºé•¿åº¦å’Œè´¨é‡åˆ†ç±»ï¼Œé‡‡ç”¨å·®å¼‚åŒ–ç¿»è¯‘ç­–ç•¥
    
    åˆ†ç±»æ ‡å‡†ï¼š
    - VIP è¯„è®ºï¼šé•¿è¯„è®ºï¼ˆ> 200å­—ï¼‰æˆ–æç«¯æ˜Ÿçº§çš„è¯¦ç»†è¯„è®ºï¼ˆ1/5æ˜Ÿ ä¸” > 100å­—ï¼‰
      â†’ å•ç‹¬ç¿»è¯‘ï¼Œä¿è¯è´¨é‡
    
    - æ ‡å‡†è¯„è®ºï¼šä¸­ç­‰é•¿åº¦ï¼ˆ50-200å­—ï¼‰
      â†’ 5æ¡ä¸€æ‰¹ç¿»è¯‘ï¼Œå¹³è¡¡è´¨é‡å’Œæ•ˆç‡
    
    - çŸ­è¯„è®ºï¼šç®€çŸ­è¡¨è¾¾ï¼ˆâ‰¤ 50å­—ï¼‰
      â†’ 20æ¡ä¸€æ‰¹ç¿»è¯‘ï¼Œæœ€å¤§åŒ–æ•ˆç‡
    
    ä¼˜åŠ¿ï¼š
    - è´¨é‡ä¿è¯ï¼šé‡è¦è¯„è®ºå•ç‹¬ç¿»è¯‘ï¼Œä¸é™ä½è´¨é‡
    - æ•ˆç‡æœ€å¤§åŒ–ï¼šçŸ­è¯„è®ºå¤§æ‰¹é‡å¤„ç†ï¼ŒQPS æ¶ˆè€—é™ä½ 20 å€
    - çµæ´»å¹³è¡¡ï¼šä¸­ç­‰è¯„è®ºé€‚åº¦æ‰¹é‡ï¼Œå…¼é¡¾è´¨é‡å’Œæ•ˆç‡
    """
    
    # å¯é…ç½®çš„åˆ†ç±»é˜ˆå€¼
    VIP_LENGTH_THRESHOLD = 200        # VIP è¯„è®ºæœ€ä½å­—æ•°
    VIP_EXTREME_RATING_LENGTH = 100   # æç«¯æ˜Ÿçº§è¯„è®ºçš„å­—æ•°é˜ˆå€¼
    EXTREME_RATINGS = [1, 5]          # æç«¯æ˜Ÿçº§ï¼ˆå·®è¯„/å¥½è¯„ï¼‰
    
    STANDARD_MIN_LENGTH = 50          # æ ‡å‡†è¯„è®ºæœ€ä½å­—æ•°
    STANDARD_MAX_LENGTH = 200         # æ ‡å‡†è¯„è®ºæœ€é«˜å­—æ•°
    
    SHORT_MAX_LENGTH = 50             # çŸ­è¯„è®ºæœ€é«˜å­—æ•°
    
    # æ‰¹é‡å¤§å°é…ç½®
    BATCH_SIZE_VIP = 1       # VIP è¯„è®ºï¼šå•ç‹¬ç¿»è¯‘
    BATCH_SIZE_STANDARD = 5  # æ ‡å‡†è¯„è®ºï¼š5 æ¡ä¸€æ‰¹
    BATCH_SIZE_SHORT = 20    # çŸ­è¯„è®ºï¼š20 æ¡ä¸€æ‰¹
    
    @classmethod
    def classify(cls, review) -> str:
        """
        è¯„è®ºåˆ†ç±»
        
        Args:
            review: Review å¯¹è±¡
        
        Returns:
            'vip': é«˜è´¨é‡é•¿è¯„è®º
            'standard': ä¸­ç­‰è¯„è®º
            'short': çŸ­è¯„è®º
        """
        text = review.body_original or ""
        text_length = len(text.strip())
        rating = review.rating
        
        # VIP è¯„è®ºï¼šé•¿è¯„è®ºæˆ–æç«¯æ˜Ÿçº§çš„è¯¦ç»†è¯„è®º
        if text_length > cls.VIP_LENGTH_THRESHOLD:
            return 'vip'
        
        if text_length > cls.VIP_EXTREME_RATING_LENGTH and rating in cls.EXTREME_RATINGS:
            return 'vip'
        
        # çŸ­è¯„è®º
        if text_length <= cls.SHORT_MAX_LENGTH:
            return 'short'
        
        # æ ‡å‡†è¯„è®ºï¼ˆé»˜è®¤ï¼‰
        return 'standard'
    
    @classmethod
    def get_batch_size(cls, category: str) -> int:
        """è·å–æ‰¹é‡å¤§å°"""
        batch_sizes = {
            'vip': cls.BATCH_SIZE_VIP,
            'standard': cls.BATCH_SIZE_STANDARD,
            'short': cls.BATCH_SIZE_SHORT
        }
        return batch_sizes.get(category, cls.BATCH_SIZE_STANDARD)
    
    @classmethod
    def group_reviews(cls, reviews: list) -> dict:
        """
        å°†è¯„è®ºæŒ‰åˆ†ç±»åˆ†ç»„
        
        Returns:
            {
                'vip': [...],
                'standard': [...],
                'short': [...]
            }
        """
        groups = {
            'vip': [],
            'standard': [],
            'short': []
        }
        
        for review in reviews:
            category = cls.classify(review)
            groups[category].append(review)
        
        return groups


# ============================================================================
# ğŸš¦ å…¨å±€ API é™æµå™¨ï¼ˆé˜²æ­¢ QPS å†²é«˜å¯¼è‡´è´¦å·è¢«å°ï¼‰
# ============================================================================

class APIRateLimiter:
    """
    å…¨å±€ API é™æµå™¨ï¼Œé˜²æ­¢ç¬é—´ RPS å†²é«˜
    
    ç­–ç•¥ï¼š
    - ä½¿ç”¨ Redis æ»‘åŠ¨çª—å£è®¡æ•°
    - qwen-plus-latest: 15,000 RPM = 250 RPS
    - å®‰å…¨ä¸Šé™: 250 * 0.8 = 200 RPSï¼ˆç•™ 20% ä½™é‡ï¼‰
    - æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆå¤šæœåŠ¡å™¨å…±äº« Redis é™æµï¼‰
    - è¶…è¿‡é™åˆ¶æ—¶ï¼Œéšæœºé€€é¿ 0.05-0.2 ç§’
    """
    def __init__(self, redis_client, max_qps=200, window_seconds=1):
        self.redis_client = redis_client
        self.max_qps = max_qps
        self.window_seconds = window_seconds
        self.key_prefix = "api_rate_limit"
    
    def acquire(self, api_name="qwen"):
        """
        è·å– API è°ƒç”¨è®¸å¯
        
        Returns:
            bool: True if allowed, False if rate limited
        """
        key = f"{self.key_prefix}:{api_name}"
        current_time = time.time()
        window_start = current_time - self.window_seconds
        
        # æ¸…ç†è¿‡æœŸè®¡æ•°
        self.redis_client.zremrangebyscore(key, 0, window_start)
        
        # æ£€æŸ¥å½“å‰çª—å£å†…çš„è¯·æ±‚æ•°
        current_count = self.redis_client.zcard(key)
        
        if current_count >= self.max_qps:
            # è¶…è¿‡é™åˆ¶ï¼Œéšæœºé€€é¿
            backoff = random.uniform(0.1, 0.5)
            logger.warning(f"[é™æµ] API QPS è¾¾åˆ° {current_count}/{self.max_qps}ï¼Œé€€é¿ {backoff:.2f}s")
            time.sleep(backoff)
            return False
        
        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self.redis_client.zadd(key, {str(current_time): current_time})
        self.redis_client.expire(key, self.window_seconds * 2)  # 2 å€çª—å£æ—¶é—´è¿‡æœŸ
        
        return True
    
    def wait_and_acquire(self, api_name="qwen", max_retries=10):
        """
        ç­‰å¾…ç›´åˆ°è·å–åˆ° API è°ƒç”¨è®¸å¯
        
        Args:
            api_name: API åç§°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for i in range(max_retries):
            if self.acquire(api_name):
                return True
            time.sleep(random.uniform(0.05, 0.2))  # çŸ­æš‚éšæœºé€€é¿
        
        raise Exception(f"[é™æµ] æ— æ³•è·å– API è®¸å¯ï¼Œå·²é‡è¯• {max_retries} æ¬¡")

# Redis å®¢æˆ·ç«¯ï¼ˆç”¨äºåˆ†å¸ƒå¼é”å’Œé™æµï¼‰
redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

# å…¨å±€é™æµå™¨å®ä¾‹
# qwen-plus-latest: 15,000 RPM = 250 RPSï¼Œå®‰å…¨ä¸Šé™ 200 RPS
import os
MAX_API_RPS = int(os.environ.get('MAX_API_RPS', '200'))
api_limiter = APIRateLimiter(redis_client, max_qps=MAX_API_RPS)

def rate_limited_api(api_name="qwen"):
    """
    API é™æµè£…é¥°å™¨
    
    ç”¨æ³•ï¼š
        @rate_limited_api("qwen")
        def call_qwen_api():
            ...
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç­‰å¾…è·å– API è®¸å¯
            api_limiter.wait_and_acquire(api_name)
            
            # è°ƒç”¨åŸå‡½æ•°
            return func(*args, **kwargs)
        
        return wrapper
    return decorator


# ============================================================================
# ğŸ”¥ æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹æŸ¥è¯¢ PostgreSQLï¼‰
# ============================================================================

class LabelCacheManager:
    """
    æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ç®¡ç†å™¨
    
    ä¼˜åŒ–ç‚¹ï¼š
    - å°†çƒ­é—¨äº§å“çš„æ ‡ç­¾åº“å¸¸é©» Redis
    - é¿å… Worker æ¯æ¬¡æå–ä¸»é¢˜éƒ½æŸ¥è¯¢æ ‡ç­¾è¡¨
    - ç¼“å­˜æœ‰æ•ˆæœŸ 1 å°æ—¶ï¼ˆæ ‡ç­¾åº“å˜åŒ–ä¸é¢‘ç¹ï¼‰
    """
    CACHE_PREFIX = "label_cache"
    CACHE_TTL = 3600  # 1 å°æ—¶
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
    
    def get_label_id_map(self, product_id: str) -> dict:
        """
        ä»ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨
        
        Returns:
            dict: {(theme_type, label_name): label_id} æˆ– Noneï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        """
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            try:
                import json
                data = json.loads(cached_data)
                # é‡å»º tuple key
                return {(k.split("|")[0], k.split("|")[1]): v for k, v in data.items()}
            except Exception as e:
                logger.warning(f"[æ ‡ç­¾ç¼“å­˜] è§£æç¼“å­˜å¤±è´¥: {e}")
                return None
        
        return None
    
    def set_label_id_map(self, product_id: str, label_id_map: dict):
        """
        å°†æ ‡ç­¾æ˜ å°„è¡¨å­˜å…¥ç¼“å­˜
        
        Args:
            product_id: äº§å“ ID
            label_id_map: {(theme_type, label_name): label_id}
        """
        if not label_id_map:
            return
        
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        
        try:
            import json
            # å°† tuple key è½¬æ¢ä¸ºå­—ç¬¦ä¸² key
            data = {f"{k[0]}|{k[1]}": str(v) for k, v in label_id_map.items()}
            self.redis_client.setex(cache_key, self.CACHE_TTL, json.dumps(data))
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾ï¼ˆäº§å“: {product_id}ï¼‰")
        except Exception as e:
            logger.warning(f"[æ ‡ç­¾ç¼“å­˜] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
    
    def invalidate(self, product_id: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆï¼ˆæ ‡ç­¾åº“æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        self.redis_client.delete(cache_key)
        logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²æ¸…é™¤ç¼“å­˜ï¼ˆäº§å“: {product_id}ï¼‰")

# å…¨å±€æ ‡ç­¾ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
label_cache = LabelCacheManager(redis_client)

# Create Celery application
celery_app = Celery(
    "voc_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
)

# Celery configuration
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=1800,  # 30 minutes timeout per task (increased from 600s to handle large batches)
    task_soft_time_limit=1500,  # 25 minutes soft limit (warning before hard kill)
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    worker_send_task_events=True,  # ğŸ”¥ æ”¯æŒ Flower ç›‘æ§
    # ============================================================================
    # ğŸš€ 6 é˜Ÿåˆ— + 5 Worker èŒèƒ½åŒ–æ¶æ„ï¼ˆ4æ ¸16Gï¼Œè§£å†³ç¿»è¯‘é˜»å¡åˆ†æé—®é¢˜ï¼‰
    # ============================================================================
    #
    # ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ï¼šç‰©ç†éš”ç¦»é˜Ÿåˆ— + èŒèƒ½åŒ– Worker åˆ†å·¥
    #
    # 6 ä¸ªç‹¬ç«‹é˜Ÿåˆ—ï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 1. ingestion          - å…¥åº“ï¼ˆç§’å›ï¼‰                                    â”‚
    # â”‚ 2. learning           - å»ºæ¨¡ï¼ˆVIP å¿«è½¦é“ï¼‰                              â”‚
    # â”‚ 3. translation        - ç¿»è¯‘ï¼ˆç‹¬ç«‹é˜Ÿåˆ—ï¼Œä¸é˜»å¡åˆ†æï¼‰                     â”‚
    # â”‚ 4. insight_extraction - æ´å¯Ÿæå–ï¼ˆä¸“å±é˜Ÿåˆ—ï¼‰                            â”‚
    # â”‚ 5. theme_extraction   - ä¸»é¢˜æå–ï¼ˆä¸“å±é˜Ÿåˆ—ï¼‰                            â”‚
    # â”‚ 6. reports            - æŠ¥å‘Šç”Ÿæˆ                                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # 5 ä¸ªèŒèƒ½åŒ– Workerï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ Worker 1 (Base):    ingestion, reports       | Prefork, 4 çº¿ç¨‹         â”‚
    # â”‚   â†’ æ­»å®ˆå…¥åº“ï¼Œä¸æ¥ AI æ´»ï¼Œç¡®ä¿æ’ä»¶ä¸Šä¼ ç§’å›                              â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 2 (VIP):     learning                 | Gevent, 50 åç¨‹         â”‚
    # â”‚   â†’ å»ºæ¨¡å¿«è½¦é“ï¼Œä¸“æ”»ç»´åº¦å­¦ä¹ å’Œ 5W å»ºæ¨¡                                  â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 3 (Trans):   translation              | Gevent, 100 åç¨‹        â”‚
    # â”‚   â†’ ç‹¬ç«‹ç¿»è¯‘ç»„ï¼Œä¸“é—¨æ¶ˆåŒ–æµ·é‡ç¿»è¯‘ï¼Œä¸å½±å“åˆ†æ                            â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 4 (Insight): insight_extraction, learning | Gevent, 100 åç¨‹    â”‚
    # â”‚   â†’ æ´å¯Ÿä¸“å‘˜ï¼Œä¸»æ”»æ´å¯Ÿæå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡                                â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 5 (Theme):   theme_extraction, learning   | Gevent, 100 åç¨‹    â”‚
    # â”‚   â†’ ä¸»é¢˜ä¸“å‘˜ï¼Œä¸»æ”»ä¸»é¢˜æå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡                                â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # æ€»å¹¶å‘ï¼š4 + 50 + 100 + 100 + 100 = 354 å¹¶å‘ï¼
    #
    # ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿ï¼š
    # - ç¿»è¯‘ä¸å†æ˜¯å±éšœï¼šç‹¬ç«‹ Workerï¼Œä¸é˜»å¡åˆ†æ
    # - å»ºæ¨¡æ°¸è¿œä¼˜å…ˆï¼šæ‰€æœ‰ AI Worker éƒ½æ”¯æ´ learning
    # - æ´å¯Ÿ/ä¸»é¢˜å¹¶è¡Œï¼šå„æœ‰ä¸“å±é˜Ÿåˆ—ï¼Œä¸äº’ç›¸ç«äº‰
    #
    task_routes={
        # ============== 1. å¿«è½¦é“ï¼šå…¥åº“ (worker-base) ==============
        # ğŸï¸ çº¯ CPU + ç£ç›˜ï¼Œä¿è¯ API ç§’çº§å“åº”
        "app.worker.task_process_ingestion_queue": {"queue": "ingestion"},
        "app.worker.task_check_pending_translations": {"queue": "ingestion"},
        
        # ============== 2. VIP å¿«è½¦é“ï¼šå­¦ä¹ å»ºæ¨¡ (worker-vip) ==============
        # ğŸŒŸ æ–°äº§å“ç§’çº§å»ºæ¨¡ï¼Œç‹¬ç«‹è¿›ç¨‹ä¸å—å¹²æ‰°
        "app.worker.task_full_auto_analysis": {"queue": "learning"},
        "app.worker.task_scientific_learning_and_analysis": {"queue": "learning"},
        
        # ============== 3. ç‹¬ç«‹ï¼šç¿»è¯‘ (worker-trans) ==============
        # ğŸ”„ ä¸“é—¨æ¶ˆåŒ–æµ·é‡ç¿»è¯‘ï¼Œä¸å½±å“å…¶ä»–åˆ†æä»»åŠ¡
        "app.worker.task_translate_bullet_points": {"queue": "translation"},
        "app.worker.task_process_reviews": {"queue": "translation"},
        "app.worker.task_ingest_translation_only": {"queue": "translation"},
        
        # ============== 4. ä¸“å±ï¼šæ´å¯Ÿæå– (worker-insight) ==============
        # ğŸ” ä¸»æ”»æ´å¯Ÿæå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡
        "app.worker.task_extract_insights": {"queue": "insight_extraction"},
        
        # ============== 5. ä¸“å±ï¼šä¸»é¢˜æå– (worker-theme) ==============
        # ğŸ·ï¸ ä¸»æ”»ä¸»é¢˜æå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡
        "app.worker.task_extract_themes": {"queue": "theme_extraction"},
        
        # ============== 6. ç»„è£…ï¼šæŠ¥å‘Šç”Ÿæˆ (worker-base) ==============
        # ğŸ“Š æœ€åçš„æ•´åˆï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
        "app.worker.task_generate_report": {"queue": "reports"},
    },
    # Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®
    beat_schedule={
        # æ¯ 5 ç§’æ¶ˆè´¹ä¸€æ¬¡å…¥åº“é˜Ÿåˆ—
        "process-ingestion-queue": {
            "task": "app.worker.task_process_ingestion_queue",
            "schedule": 5.0,
        },
        # ğŸ”¥ æ¯ 15 ç§’æ£€æŸ¥å¹¶è§¦å‘å¾…ç¿»è¯‘ä»»åŠ¡ï¼ˆç¡®ä¿ç¿»è¯‘æŒç»­è¿›è¡Œï¼‰
        "check-pending-translations": {
            "task": "app.worker.task_check_pending_translations",
            "schedule": 15.0,
        },
    },
)

# ============================================================================
# ğŸ”§ åŒæ­¥æ•°æ®åº“è¿æ¥ï¼ˆCelery Worker ä¸“ç”¨ï¼‰
# ============================================================================
# Celery ä½¿ç”¨ Gevent åç¨‹ï¼Œéœ€è¦ç‰¹æ®Šçš„è¿æ¥æ± é…ç½®
# 
# ç­–ç•¥è¯´æ˜ï¼š
# - ä½¿ç”¨ NullPoolï¼šæ¯æ¬¡æ“ä½œåˆ›å»ºæ–°è¿æ¥ï¼Œé€‚åˆé«˜å¹¶å‘åç¨‹åœºæ™¯
# - é¿å…è¿æ¥æ± ç“¶é¢ˆï¼šGevent 150 åç¨‹ vs é»˜è®¤ pool_size=5 ä¼šä¸¥é‡é˜»å¡
# - PostgreSQL max_connections=500 è¶³ä»¥æ”¯æ’‘
# ============================================================================
from sqlalchemy.pool import NullPool, QueuePool

SYNC_DATABASE_URL = settings.DATABASE_URL.replace("+asyncpg", "")

# ğŸ”¥ é«˜å¹¶å‘è¿æ¥æ± é…ç½®ï¼ˆæ”¯æŒ 400+ å¹¶å‘ Workerï¼‰
sync_engine = create_engine(
    SYNC_DATABASE_URL,
    echo=settings.DEBUG,
    # ä½¿ç”¨ QueuePool é…åˆå¤§å®¹é‡ï¼Œæ¯” NullPool æ›´é«˜æ•ˆ
    poolclass=QueuePool,
    pool_size=100,        # åŸºç¡€è¿æ¥æ•°
    max_overflow=400,     # æº¢å‡ºè¿æ¥æ•°ï¼ˆæ€»å…±æ”¯æŒ 500 è¿æ¥ï¼‰
    pool_timeout=30,      # ç­‰å¾…è¿æ¥è¶…æ—¶
    pool_pre_ping=True,   # æ£€æµ‹æ–­å¼€çš„è¿æ¥
    pool_recycle=1800,    # 30 åˆ†é’Ÿå›æ”¶è¿æ¥ï¼Œé˜²æ­¢æ•°æ®åº“è¶…æ—¶
)
SyncSession = sessionmaker(bind=sync_engine)


def get_sync_db():
    """Get synchronous database session for worker."""
    return SyncSession()


# ============== Worker å¯åŠ¨æ—¶æ¸…ç†å¡ä½çš„ä»»åŠ¡ ==============

def cleanup_stuck_reviews():
    """
    æ¸…ç†å¡åœ¨ 'processing' çŠ¶æ€çš„è¯„è®ºã€‚
    å½“ Worker é‡å¯æ—¶ï¼Œä¹‹å‰æ­£åœ¨å¤„ç†çš„è¯„è®ºå¯èƒ½ä¼šå¡åœ¨ processing çŠ¶æ€ã€‚
    è¿™ä¸ªå‡½æ•°å°†å®ƒä»¬é‡ç½®ä¸º pendingï¼Œè®©å®ƒä»¬å¯ä»¥è¢«é‡æ–°å¤„ç†ã€‚
    """
    from app.models.review import Review
    
    db = get_sync_db()
    try:
        result = db.execute(
            update(Review)
            .where(Review.translation_status == "processing")
            .values(translation_status="pending")
        )
        db.commit()
        
        if result.rowcount > 0:
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {result.rowcount} æ¡å¡ä½çš„è¯„è®ºé‡ç½®ä¸º pending çŠ¶æ€")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¡ä½çš„è¯„è®º")
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†å¡ä½è¯„è®ºå¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def cleanup_stuck_tasks():
    """
    æ¸…ç†å¡ä½çš„ä»»åŠ¡ï¼ˆå¿ƒè·³è¶…æ—¶ï¼‰ã€‚
    å°† PROCESSING çŠ¶æ€ä½†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡æ ‡è®°ä¸º TIMEOUTã€‚
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone, timedelta
    
    db = get_sync_db()
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ processing çŠ¶æ€çš„ä»»åŠ¡
        result = db.execute(
            select(Task).where(Task.status == TaskStatus.PROCESSING.value)
        )
        tasks = result.scalars().all()
        
        timeout_count = 0
        for task in tasks:
            if task.is_heartbeat_timeout:
                task.status = TaskStatus.TIMEOUT.value
                task.error_message = f"å¿ƒè·³è¶…æ—¶ï¼šæœ€åå¿ƒè·³æ—¶é—´ {task.last_heartbeat}"
                timeout_count += 1
                logger.warning(f"[å¯åŠ¨æ¸…ç†] ä»»åŠ¡ {task.id} ({task.task_type}) å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸º TIMEOUT")
        
        if timeout_count > 0:
            db.commit()
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {timeout_count} ä¸ªè¶…æ—¶ä»»åŠ¡æ ‡è®°ä¸º TIMEOUT")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡")
            
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†è¶…æ—¶ä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


# ============== å¿ƒè·³æ›´æ–°è¾…åŠ©å‡½æ•° ==============

def update_task_heartbeat(db, task_id: str, processed_items: int = None):
    """
    æ›´æ–°ä»»åŠ¡å¿ƒè·³æ—¶é—´ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        processed_items: å¯é€‰ï¼ŒåŒæ—¶æ›´æ–°å·²å¤„ç†æ•°é‡
    """
    from app.models.task import Task
    from datetime import datetime, timezone
    
    try:
        values = {"last_heartbeat": datetime.now(timezone.utc)}
        if processed_items is not None:
            values["processed_items"] = processed_items
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡å¿ƒè·³å¤±è´¥: {e}")
        db.rollback()


def get_or_create_task(db, product_id: str, task_type: str, total_items: int = 0, celery_task_id: str = None):
    """
    è·å–æˆ–åˆ›å»ºä»»åŠ¡è®°å½•ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        product_id: äº§å“ ID
        task_type: ä»»åŠ¡ç±»å‹
        total_items: æ€»é¡¹ç›®æ•°
        celery_task_id: Celery ä»»åŠ¡ ID
        
    Returns:
        Task: ä»»åŠ¡å¯¹è±¡
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone
    
    # æŸ¥æ‰¾ç°æœ‰ä»»åŠ¡
    result = db.execute(
        select(Task).where(
            and_(
                Task.product_id == product_id,
                Task.task_type == task_type
            )
        )
    )
    task = result.scalar_one_or_none()
    
    now = datetime.now(timezone.utc)
    
    if task:
        # æ›´æ–°ç°æœ‰ä»»åŠ¡
        task.status = TaskStatus.PROCESSING.value
        task.total_items = total_items
        task.processed_items = 0
        task.last_heartbeat = now
        task.celery_task_id = celery_task_id
        task.error_message = None
    else:
        # åˆ›å»ºæ–°ä»»åŠ¡
        task = Task(
            product_id=product_id,
            task_type=task_type,
            status=TaskStatus.PROCESSING.value,
            total_items=total_items,
            processed_items=0,
            last_heartbeat=now,
            celery_task_id=celery_task_id
        )
        db.add(task)
    
    db.commit()
    db.refresh(task)
    return task


def complete_task(db, task_id: str, success: bool = True, error_message: str = None):
    """
    å®Œæˆä»»åŠ¡ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        success: æ˜¯å¦æˆåŠŸ
        error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    from app.models.task import Task, TaskStatus
    
    try:
        status = TaskStatus.COMPLETED.value if success else TaskStatus.FAILED.value
        values = {
            "status": status,
            "last_heartbeat": None  # æ¸…é™¤å¿ƒè·³ï¼Œè¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
        }
        if error_message:
            values["error_message"] = error_message
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()


# ä½¿ç”¨ Celery ä¿¡å·åœ¨ Worker å¯åŠ¨æ—¶æ‰§è¡Œæ¸…ç†
from celery.signals import worker_ready

@worker_ready.connect
def on_worker_ready(**kwargs):
    """Worker å¯åŠ¨å®Œæˆåæ‰§è¡Œæ¸…ç†"""
    logger.info("Worker å·²å°±ç»ªï¼Œå¼€å§‹æ£€æŸ¥å¡ä½çš„ä»»åŠ¡...")
    cleanup_stuck_reviews()
    cleanup_stuck_tasks()  # [NEW] æ¸…ç†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡


# ============== ä»»åŠ¡1: äº”ç‚¹ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def task_translate_bullet_points(self, product_id: str):
    """
    Translate product bullet points and title.
    This task should run FIRST, before review translation.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.product import Product
    from app.services.translation import translation_service
    import json
    
    logger.info(f"Starting bullet points translation for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Get product
        result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = result.scalar_one_or_none()
        
        if not product:
            logger.error(f"Product {product_id} not found")
            return {"success": False, "error": "Product not found"}
        
        translated_title = None
        translated_bullets = None
        
        # 1. Translate product title if not already translated
        if product.title and not product.title_translated:
            try:
                translated_title = translation_service.translate_product_title(product.title)
                product.title_translated = translated_title
                logger.info(f"Translated product title: {translated_title[:50]}...")
            except Exception as e:
                logger.error(f"Failed to translate product title: {e}")
        
        # 2. Translate bullet points if not already translated
        if product.bullet_points and not product.bullet_points_translated:
            try:
                # Parse bullet points from JSON, PostgreSQL array, or Python list
                bullet_points = []
                if isinstance(product.bullet_points, list):
                    bullet_points = product.bullet_points
                elif isinstance(product.bullet_points, str):
                    bp_str = product.bullet_points.strip()
                    # å°è¯• JSON æ ¼å¼ [...]
                    if bp_str.startswith('['):
                        bullet_points = json.loads(bp_str)
                    # å¤„ç† PostgreSQL æ•°ç»„æ ¼å¼ {...}
                    elif bp_str.startswith('{') and bp_str.endswith('}'):
                        # ç§»é™¤é¦–å°¾çš„ {} å¹¶æŒ‰é€—å·åˆ†å‰²ï¼ˆè€ƒè™‘å¼•å·å†…çš„é€—å·ï¼‰
                        import re
                        # åŒ¹é…å¼•å·å†…çš„å†…å®¹æˆ–éé€—å·å­—ç¬¦
                        content = bp_str[1:-1]  # ç§»é™¤ { }
                        # ä½¿ç”¨æ­£åˆ™åŒ¹é…å¸¦å¼•å·çš„å­—ç¬¦ä¸²
                        matches = re.findall(r'"([^"]*)"', content)
                        if matches:
                            bullet_points = matches
                        else:
                            # ç®€å•åˆ†å‰²ï¼ˆä¸å¸¦å¼•å·çš„æƒ…å†µï¼‰
                            bullet_points = [s.strip() for s in content.split(',') if s.strip()]
                    else:
                        # å°è¯•ç›´æ¥ JSON è§£æ
                        try:
                            bullet_points = json.loads(bp_str)
                        except:
                            bullet_points = [bp_str] if bp_str else []
                
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    # ç»Ÿä¸€ä¿å­˜ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"Translated {len(translated_bullets)} bullet points")
            except Exception as e:
                logger.error(f"Failed to translate bullet points: {e}")
        
        db.commit()
        
        return {
            "success": True,
            "product_id": product_id,
            "title_translated": translated_title is not None,
            "bullet_points_translated": translated_bullets is not None
        }
        
    except Exception as e:
        logger.error(f"Bullet points translation failed for product {product_id}: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡2: è¯„è®ºç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
def task_process_reviews(self, product_id: str, task_id: str):
    """
    Async task to process and translate reviews.
    
    Workflow:
    1. Get pending reviews from database
    2. For each review:
       a. Call Qwen API for translation
       b. Analyze sentiment
       c. Extract insights (æ·±åº¦è§£è¯»)
       d. Update database
       e. Update task progress
    3. Mark task as completed
    
    Args:
        product_id: UUID of the product
        task_id: UUID of the task to track progress
    """
    from app.models.review import Review
    from app.models.task import Task
    from app.models.insight import ReviewInsight
    from app.services.translation import translation_service
    
    logger.info(f"Starting translation task {task_id} for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Update task status to processing
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(status="processing")
        )
        db.commit()
        
        # Get pending reviews (including processing and failed - to retry stuck/failed translations)
        # ordered by review_date descending (newest first, matching frontend display)
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_(["pending", "processing", "failed"])
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        failed = 0
        
        logger.info(f"Found {total_reviews} pending reviews to translate")
        
        for review in reviews:
            try:
                # Mark as processing
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(translation_status="processing")
                )
                db.commit()
                
                # Validate body_original exists
                if not review.body_original or not review.body_original.strip():
                    logger.warning(f"Review {review.id} has empty body, skipping translation")
                    db.execute(
                        update(Review)
                        .where(Review.id == review.id)
                        .values(translation_status="failed")
                    )
                    db.commit()
                    failed += 1
                    continue
                
                # åªåšç¿»è¯‘ï¼Œä¸æå–æ´å¯Ÿï¼ˆæ´å¯Ÿéœ€è¦ç”¨æˆ·æ‰‹åŠ¨è§¦å‘ï¼‰
                title_translated, body_translated, sentiment, _ = translation_service.translate_review(
                    title=review.title_original,
                    body=review.body_original,
                    extract_insights=False  # å…³é—­è‡ªåŠ¨æ´å¯Ÿæå–
                )
                
                # Validate translation results
                if not body_translated or not body_translated.strip():
                    logger.error(f"Translation returned empty for review {review.id}, body: {review.body_original[:100]}")
                    raise ValueError("Translation returned empty result")
                
                # Update review with translation only
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        title_translated=title_translated if title_translated and title_translated.strip() else None,
                        body_translated=body_translated,
                        sentiment=sentiment.value,
                        translation_status="completed"
                    )
                )
                
                processed += 1
                
                # Update task progress
                db.execute(
                    update(Task)
                    .where(Task.id == task_id)
                    .values(processed_items=processed)
                )
                db.commit()
                
                logger.debug(f"Translated review {review.id}: {review.rating} stars")
                
                # Rate limiting: wait between API calls
                time.sleep(0.2)
                
            except Exception as e:
                logger.error(f"Failed to translate review {review.id}: {e}", exc_info=True)
                failed += 1
                
                # Mark review as failed (don't save empty translations)
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        translation_status="failed",
                        title_translated=None,
                        body_translated=None
                    )
                )
                db.commit()
                
                # Continue with next review
                continue
        
        # Check if there are still pending reviews
        from app.models.review import TranslationStatus
        pending_count_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.PENDING.value
                )
            )
        )
        pending_count = pending_count_result.scalar() or 0
        
        # Update task status - only mark as completed if no pending reviews
        if pending_count == 0:
            final_status = "completed" if failed == 0 else "completed"
        else:
            # Still have pending reviews, keep as processing
            final_status = "processing"
        
        error_msg = f"{failed} reviews failed" if failed > 0 else None
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(
                status=final_status,
                processed_items=processed,
                error_message=error_msg
            )
        )
        db.commit()
        
        logger.info(f"Task {task_id} completed: {processed} translated, {failed} failed")
        
        return {
            "task_id": task_id,
            "product_id": product_id,
            "total": total_reviews,
            "processed": processed,
            "failed": failed
        }
        
    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}")
        
        # Mark task as failed
        try:
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(
                    status="failed",
                    error_message=str(e)
                )
            )
            db.commit()
        except:
            pass
        
        # Retry the task
        raise self.retry(exc=e)
        
    finally:
        db.close()


@celery_app.task
def task_health_check():
    """Simple task to verify worker is running."""
    return {"status": "healthy", "worker": "voc_worker"}


@celery_app.task
def task_retry_failed_reviews(product_id: str):
    """
    Retry translation for failed reviews.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    
    db = get_sync_db()
    
    try:
        # Reset failed reviews to pending
        result = db.execute(
            update(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "failed"
                )
            )
            .values(translation_status="pending")
        )
        db.commit()
        
        logger.info(f"Reset {result.rowcount} failed reviews to pending")
        
        # Trigger processing
        task_process_reviews.delay(product_id, None)
        
    finally:
        db.close()


@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_extract_insights(self, product_id: str):
    """
    Extract insights for already translated reviews (without re-translating).
    
    This task:
    1. Gets all translated reviews that don't have insights yet
    2. **[NEW] Loads product-specific dimensions if available**
    3. Calls AI to extract insights (using dimensions for categorization)
    4. Saves insights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.insight import ReviewInsight
    from app.models.product_dimension import ProductDimension
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[æ´å¯Ÿæå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting insight extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] è·å–äº§å“çš„ç»´åº¦ Schemaï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        dimension_result = db.execute(
            select(ProductDimension)
            .where(ProductDimension.product_id == product_id)
            .order_by(ProductDimension.created_at)
        )
        dimensions = dimension_result.scalars().all()
        
        # è½¬æ¢ä¸º schema æ ¼å¼
        dimension_schema = None
        if dimensions and len(dimensions) > 0:
            dimension_schema = [
                {"name": dim.name, "description": dim.description or ""}
                for dim in dimensions
            ]
            logger.info(f"ä½¿ç”¨ {len(dimension_schema)} ä¸ªäº§å“ç»´åº¦è¿›è¡Œæ´å¯Ÿæå–")
        else:
            logger.info(f"äº§å“æš‚æ— å®šä¹‰ç»´åº¦ï¼Œä½¿ç”¨é€šç”¨æ´å¯Ÿæå–é€»è¾‘")
        
        # [FIX] å…ˆè·å–æ€»è¯„è®ºæ•°ï¼ˆå·²ç¿»è¯‘çš„è¯„è®ºï¼‰
        total_translated_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False
                )
            )
        )
        total_translated = total_translated_result.scalar() or 0
        
        # [FIX] è·å–å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°ï¼ˆprocessed_itemsï¼‰
        already_processed_result = db.execute(
            select(func.count(func.distinct(ReviewInsight.review_id)))
            .join(Review, Review.id == ReviewInsight.review_id)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.is_deleted == False
                )
            )
        )
        already_processed = already_processed_result.scalar() or 0
        
        # [NEW] åˆ›å»º/æ›´æ–° Task è®°å½•ï¼ˆtotal_items = æ€»è¯„è®ºæ•°ï¼Œprocessed_items = å·²å¤„ç†æ•°ï¼‰
        task_record = get_or_create_task(
            db=db,
            product_id=product_id,
            task_type=TaskType.INSIGHTS.value,
            total_items=total_translated,  # æ€»è¯„è®ºæ•°ï¼ˆå›ºå®šå€¼ï¼‰
            celery_task_id=self.request.id
        )
        # è®¾ç½®å·²å¤„ç†æ•°ä¸ºå½“å‰å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°
        task_record.processed_items = already_processed
        db.commit()
        logger.info(f"Task record: total_items={total_translated}, processed_items={already_processed}, remaining={total_translated - already_processed}")
        
        # [FIX] ä½¿ç”¨ NOT EXISTS å­æŸ¥è¯¢æ’é™¤å·²æœ‰æ´å¯Ÿçš„è¯„è®ºï¼Œé¿å…é‡å¤å¤„ç†
        insight_exists_subquery = (
            select(ReviewInsight.id)
            .where(ReviewInsight.review_id == Review.id)
            .exists()
        )
        
        # Get translated reviews that DON'T have insights yet - ordered by review_date to match page display order
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False,
                    ~insight_exists_subquery  # [FIX] Only process reviews without insights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        reviews_to_process = len(reviews)
        processed = 0
        insights_extracted = 0
        
        # ğŸš€ å¹¶è¡Œåç¨‹ä¼˜åŒ–ï¼šä½¿ç”¨ gevent pool å¹¶è¡Œè°ƒç”¨ AI API
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼ŒæœåŠ¡å™¨ B å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å€¼
        import os
        PARALLEL_SIZE = int(os.environ.get('INSIGHT_PARALLEL_SIZE', '60'))  # é»˜è®¤ 60ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
        
        # ğŸ”¥ [OPTIMIZED] BATCH_SIZE = PARALLEL_SIZEï¼Œå……åˆ†åˆ©ç”¨å¹¶è¡Œæ± 
        # ä¹‹å‰ BATCH_SIZE=20 é™åˆ¶äº†çœŸå®å¹¶å‘ï¼Œç°åœ¨ä¸ PARALLEL_SIZE åŒæ­¥
        BATCH_SIZE = PARALLEL_SIZE
        pending_insights = []  # å¾…æäº¤çš„æ´å¯Ÿåˆ—è¡¨
        
        logger.info(f"Found {reviews_to_process} reviews remaining for insight extraction (total={total_translated}, already_done={already_processed})")
        logger.info(f"[å¹¶è¡Œä¼˜åŒ–-æ´å¯Ÿ] ä½¿ç”¨ PARALLEL_SIZE={PARALLEL_SIZE} å¹¶è¡Œå¤„ç†, BATCH_SIZE={BATCH_SIZE} æ‰¹é‡å…¥åº“")
        
        # å®šä¹‰å•æ¡è¯„è®ºçš„å¤„ç†å‡½æ•°
        def process_single_insight(review):
            """å¹¶è¡Œå¤„ç†å•æ¡è¯„è®ºçš„æ´å¯Ÿæå–"""
            try:
                insights = translation_service.extract_insights(
                    original_text=review.body_original or "",
                    translated_text=review.body_translated or "",
                    dimension_schema=dimension_schema
                )
                return {
                    "review_id": review.id,
                    "insights": insights,
                    "success": True
                }
            except Exception as e:
                logger.error(f"Failed to extract insights for review {review.id}: {e}")
                return {
                    "review_id": review.id,
                    "insights": None,
                    "success": False,
                    "error": str(e)
                }
        
        # ä½¿ç”¨ gevent pool å¹¶è¡Œå¤„ç†
        from gevent.pool import Pool
        pool = Pool(PARALLEL_SIZE)
        
        # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        for batch_start in range(0, reviews_to_process, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, reviews_to_process)
            batch_reviews = reviews[batch_start:batch_end]
            
            # ğŸš€ å¹¶è¡Œè°ƒç”¨ AI API
            results = pool.map(process_single_insight, batch_reviews)
            
            # å¤„ç†ç»“æœ
            for result in results:
                if result["success"] and result["insights"]:
                    for insight_data in result["insights"]:
                        insight = ReviewInsight(
                            review_id=result["review_id"],
                            insight_type=insight_data.get('type', 'emotion'),
                            quote=insight_data.get('quote', ''),
                            quote_translated=insight_data.get('quote_translated'),
                            analysis=insight_data.get('analysis', ''),
                            dimension=insight_data.get('dimension')
                        )
                        pending_insights.append(insight)
                    insights_extracted += len(result["insights"])
                else:
                    # æ ‡è®°ä¸ºå·²å¤„ç†ï¼ˆå³ä½¿æ²¡æœ‰æ´å¯Ÿæˆ–å¤±è´¥ï¼‰
                    empty_marker = ReviewInsight(
                        review_id=result["review_id"],
                        insight_type="_empty",
                        quote="",
                        analysis=""
                    )
                    pending_insights.append(empty_marker)
                
                processed += 1
            
            # ğŸ”¥ æ‰¹é‡æäº¤æ•°æ®åº“
            if pending_insights:
                db.add_all(pending_insights)
                db.commit()
                logger.info(f"[å¹¶è¡Œå…¥åº“] å·²æäº¤ {len(pending_insights)} æ¡æ´å¯Ÿï¼ˆè¿›åº¦: {processed}/{reviews_to_process}ï¼‰")
                pending_insights = []
            
            # æ›´æ–° Task è¿›åº¦
            if task_record:
                task_record.processed_items = already_processed + processed
                db.commit()
            
            # [OPTIMIZED] æ‰¹æ¬¡é—´å¾®å°ä¼‘æ¯ï¼Œé˜¿é‡Œäº‘ API é™æµä¸€èˆ¬ 60-100 QPSï¼Œ0.05s è¶³å¤Ÿ
            time.sleep(0.05)
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†æ´å¯Ÿ
        if pending_insights:
            db.add_all(pending_insights)
            db.commit()
            logger.info(f"[å¹¶è¡Œå…¥åº“] æœ€ç»ˆæäº¤ {len(pending_insights)} æ¡æ´å¯Ÿ")
        
        logger.info(f"Insight extraction completed: processed {processed} new reviews (total={total_translated}, now_done={already_processed + processed}), {insights_extracted} insights extracted")
        
        # [FIX] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.processed_items = already_processed + processed  # æœ€ç»ˆå¤„ç†æ•°
            db.commit()
        
        return {
            "product_id": product_id,
            "total_reviews": total_translated,  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å˜é‡å
            "processed": processed,
            "insights_extracted": insights_extracted
        }
        
    except Exception as e:
        logger.error(f"Insight extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡4: ä¸»é¢˜é«˜äº®æå– ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30, time_limit=1800, soft_time_limit=1700)
def task_extract_themes(self, product_id: str):
    """
    Extract 5W theme keywords for already translated reviews.
    
    This task:
    1. **[NEW] Auto-generates 5W context labels if not exists (Definition phase)**
    2. Gets all translated reviews that don't have theme highlights yet
    3. **[NEW] Uses context labels for forced categorization (Execution phase)**
    4. Calls AI to extract 5W themes with evidence and explanation
    5. Saves theme highlights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.product_context_label import ProductContextLabel
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists, func
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ä¸»é¢˜æå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting theme extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] Step 1: æ£€æŸ¥æ˜¯å¦æœ‰ 5W æ ‡ç­¾åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ç”Ÿæˆ
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        context_schema = None
        labels_generated = False
        
        if label_count == 0:
            logger.info(f"äº§å“ {product_id} æš‚æ—  5W æ ‡ç­¾åº“ï¼Œå¼€å§‹è‡ªåŠ¨å­¦ä¹ ...")
            
            # [NEW] å…ˆè·å–äº§å“ä¿¡æ¯ï¼ˆæ ‡é¢˜å’Œäº”ç‚¹ï¼‰
            from app.models.product import Product
            import json as json_lib
            
            product_result = db.execute(
                select(Product).where(Product.id == product_id)
            )
            product = product_result.scalar_one_or_none()
            
            product_title = ""
            bullet_points = []
            
            if product:
                product_title = product.title or ""
                # è§£æäº”ç‚¹ï¼ˆå­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
                if product.bullet_points:
                    try:
                        bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
                    except:
                        bullet_points = []
                logger.info(f"ğŸ“¦ äº§å“ä¿¡æ¯ï¼š{product.asin}ï¼Œæ ‡é¢˜é•¿åº¦={len(product_title)}ï¼Œäº”ç‚¹={len(bullet_points)}æ¡")
            
            # è·å–å·²ç¿»è¯‘çš„è¯„è®ºæ ·æœ¬ï¼ˆè‡³å°‘10æ¡ï¼‰
            sample_result = db.execute(
                select(Review.body_original, Review.body_translated)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == "completed",
                        Review.body_translated.isnot(None),
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(50)
            )
            sample_reviews = sample_result.all()
            
            if len(sample_reviews) >= 30:
                # å‡†å¤‡æ ·æœ¬æ–‡æœ¬
                sample_texts = []
                for row in sample_reviews:
                    text = row.body_translated or row.body_original
                    if text and text.strip():
                        sample_texts.append(text.strip())
                
                if len(sample_texts) >= 30:
                    # [UPDATED] è°ƒç”¨ AI å­¦ä¹ æ ‡ç­¾åº“ï¼ˆä¼ å…¥äº§å“ä¿¡æ¯ï¼‰
                    learned_labels = translation_service.learn_context_labels(
                        reviews_text=sample_texts,
                        product_title=product_title,      # [NEW] äº§å“æ ‡é¢˜
                        bullet_points=bullet_points       # [NEW] äº”ç‚¹å–ç‚¹
                    )
                    
                    if learned_labels:
                        # å­˜å…¥æ•°æ®åº“
                        for context_type in ["who", "where", "when", "why", "what"]:
                            labels = learned_labels.get(context_type, [])
                            for item in labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                        
                        db.commit()
                        labels_generated = True
                        total_labels = sum(len(v) for v in learned_labels.values())
                        logger.info(f"âœ… è‡ªåŠ¨ç”Ÿæˆ 5W æ ‡ç­¾åº“æˆåŠŸï¼Œå…± {total_labels} ä¸ªæ ‡ç­¾")
                    else:
                        logger.warning(f"âš ï¸ AI å­¦ä¹ æ ‡ç­¾åº“å¤±è´¥ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
                else:
                    logger.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘30æ¡ï¼‰ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
            else:
                logger.warning(f"âš ï¸ å·²ç¿»è¯‘è¯„è®ºä¸è¶³ï¼ˆéœ€è¦è‡³å°‘30æ¡ï¼‰ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # Step 2: è·å–æ ‡ç­¾åº“ Schemaï¼ˆå¦‚æœå­˜åœ¨æˆ–åˆšç”Ÿæˆï¼‰
        if label_count > 0 or labels_generated:
            label_result = db.execute(
                select(ProductContextLabel)
                .where(ProductContextLabel.product_id == product_id)
                .order_by(ProductContextLabel.type, ProductContextLabel.created_at)
            )
            labels = label_result.scalars().all()
            
            if labels:
                context_schema = {}
                for label in labels:
                    if label.type not in context_schema:
                        context_schema[label.type] = []
                    context_schema[label.type].append({
                        "name": label.name,
                        "description": label.description or ""
                    })
                logger.info(f"âœ… ä½¿ç”¨ 5W æ ‡ç­¾åº“è¿›è¡Œå¼ºåˆ¶å½’ç±»ï¼Œå…± {len(labels)} ä¸ªæ ‡ç­¾")
        else:
            logger.info(f"â„¹ï¸ æœªä½¿ç”¨æ ‡ç­¾åº“ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # Get translated reviews that don't have theme highlights yet
        # Use a subquery to check for existing theme highlights
        theme_exists_subquery = (
            select(ReviewThemeHighlight.id)
            .where(ReviewThemeHighlight.review_id == Review.id)
            .exists()
        )
        
        # Ordered by review_date to match page display order
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False,
                    ~theme_exists_subquery  # Reviews without theme highlights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        themes_extracted = 0
        
        logger.info(f"Found {total_reviews} translated reviews for theme extraction")
        
        # [NEW] åˆ›å»º/æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œå¯ç”¨å¿ƒè·³
        if total_reviews > 0:
            task_record = get_or_create_task(
                db=db,
                product_id=product_id,
                task_type=TaskType.THEMES.value,
                total_items=total_reviews,
                celery_task_id=self.request.id
            )
            logger.info(f"ä»»åŠ¡è®°å½•å·²åˆ›å»º: {task_record.id}")
        
        # ğŸ”¥ ä¼˜å…ˆä» Redis ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆé¿å…é¢‘ç¹æŸ¥ PostgreSQLï¼‰
        label_id_map = label_cache.get_label_id_map(str(product_id))
        
        if label_id_map:
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âœ… å‘½ä¸­ç¼“å­˜ï¼Œå…± {len(label_id_map)} ä¸ªæ ‡ç­¾")
        elif context_schema:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“æ„å»ºå¹¶ç¼“å­˜
            label_id_map = {}
            for label in labels:
                key = (label.type, label.name)
                label_id_map[key] = label.id
            
            # å­˜å…¥ Redis ç¼“å­˜
            label_cache.set_label_id_map(str(product_id), label_id_map)
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âš¡ å·²æ„å»ºå¹¶ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾")
        else:
            label_id_map = {}
            logger.debug(f"æ— æ ‡ç­¾åº“ï¼Œä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # ğŸš€ å¹¶è¡Œåç¨‹ä¼˜åŒ–ï¼šä½¿ç”¨ gevent pool å¹¶è¡Œè°ƒç”¨ AI API
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼ŒæœåŠ¡å™¨ B å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å€¼
        import os
        PARALLEL_SIZE = int(os.environ.get('THEME_PARALLEL_SIZE', '80'))  # é»˜è®¤ 80ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒæ•´
        
        # ğŸ”¥ [OPTIMIZED] BATCH_SIZE = PARALLEL_SIZEï¼Œå……åˆ†åˆ©ç”¨å¹¶è¡Œæ± 
        # ä¹‹å‰ BATCH_SIZE=20 é™åˆ¶äº†çœŸå®å¹¶å‘ï¼Œç°åœ¨ä¸ PARALLEL_SIZE åŒæ­¥
        BATCH_SIZE = PARALLEL_SIZE
        pending_themes = []  # å¾…æäº¤çš„ä¸»é¢˜åˆ—è¡¨
        
        logger.info(f"[å¹¶è¡Œä¼˜åŒ–-ä¸»é¢˜] ä½¿ç”¨ PARALLEL_SIZE={PARALLEL_SIZE} å¹¶è¡Œå¤„ç†, BATCH_SIZE={BATCH_SIZE} æ‰¹é‡å…¥åº“")
        
        # å®šä¹‰å•æ¡è¯„è®ºçš„å¤„ç†å‡½æ•°ï¼ˆé—­åŒ…ï¼Œæ•è· context_schemaï¼‰
        def process_single_theme(review):
            """å¹¶è¡Œå¤„ç†å•æ¡è¯„è®ºçš„ä¸»é¢˜æå–"""
            try:
                themes = translation_service.extract_themes(
                    original_text=review.body_original or "",
                    translated_text=review.body_translated or "",
                    context_schema=context_schema
                )
                return {
                    "review_id": review.id,
                    "themes": themes,
                    "success": True
                }
            except Exception as e:
                logger.error(f"Failed to extract themes for review {review.id}: {e}")
                return {
                    "review_id": review.id,
                    "themes": None,
                    "success": False,
                    "error": str(e)
                }
        
        # ä½¿ç”¨ gevent pool å¹¶è¡Œå¤„ç†
        from gevent.pool import Pool
        pool = Pool(PARALLEL_SIZE)
        
        # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        for batch_start in range(0, total_reviews, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, total_reviews)
            batch_reviews = reviews[batch_start:batch_end]
            
            # ğŸš€ å¹¶è¡Œè°ƒç”¨ AI API
            results = pool.map(process_single_theme, batch_reviews)
            
            # å¤„ç†ç»“æœ
            batch_themes_count = 0
            for result in results:
                if result["success"] and result["themes"]:
                    for theme_type, items in result["themes"].items():
                        if not items or len(items) == 0:
                            continue
                        
                        for item in items:
                            label_name = item.get("content", "").strip()
                            quote = item.get("quote") or item.get("content_original") or None
                            quote_translated = item.get("quote_translated") or item.get("content_translated") or None
                            explanation = item.get("explanation") or None
                            
                            if not label_name:
                                continue
                            
                            context_label_id = label_id_map.get((theme_type, label_name))
                            
                            theme_highlight = ReviewThemeHighlight(
                                review_id=result["review_id"],
                                theme_type=theme_type,
                                label_name=label_name,
                                quote=quote,
                                quote_translated=quote_translated,
                                explanation=explanation,
                                context_label_id=context_label_id,
                                items=[item]
                            )
                            pending_themes.append(theme_highlight)
                            batch_themes_count += 1
                else:
                    # æ ‡è®°ä¸ºå·²å¤„ç†
                    empty_marker = ReviewThemeHighlight(
                        review_id=result["review_id"],
                        theme_type="_empty",
                        label_name=None,
                        items=None
                    )
                    pending_themes.append(empty_marker)
                
                processed += 1
            
            themes_extracted += batch_themes_count
            
            # ğŸ”¥ æ‰¹é‡æäº¤æ•°æ®åº“
            if pending_themes:
                db.add_all(pending_themes)
                db.commit()
                logger.info(f"[å¹¶è¡Œå…¥åº“] å·²æäº¤ {len(pending_themes)} æ¡ä¸»é¢˜ï¼ˆè¿›åº¦: {processed}/{total_reviews}ï¼‰")
                pending_themes = []
            
            # æ›´æ–° Task è¿›åº¦
            if task_record:
                update_task_heartbeat(db, str(task_record.id), processed_items=processed)
            
            # [OPTIMIZED] æ‰¹æ¬¡é—´å¾®å°ä¼‘æ¯ï¼Œé˜¿é‡Œäº‘ API é™æµä¸€èˆ¬ 60-100 QPSï¼Œ0.05s è¶³å¤Ÿ
            time.sleep(0.05)
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†ä¸»é¢˜
        if pending_themes:
            db.add_all(pending_themes)
            db.commit()
            logger.info(f"[å¹¶è¡Œå…¥åº“] æœ€ç»ˆæäº¤ {len(pending_themes)} æ¡ä¸»é¢˜")
        
        logger.info(f"Theme extraction completed: {processed}/{total_reviews} reviews processed, {themes_extracted} theme entries created")
        
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.total_items = total_reviews
            task_record.processed_items = processed
            db.commit()
        
        return {
            "product_id": product_id,
            "total_reviews": total_reviews,
            "processed": processed,
            "themes_extracted": themes_extracted
        }
        
    except Exception as e:
        logger.error(f"Theme extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡5: æµå¼è½»é‡ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_ingest_translation_only(self, product_id: str):
    """
    æµå¼è½»é‡ç¿»è¯‘ä»»åŠ¡ (Stream Translation Only)
    
    æ•°æ®å…¥åº“åç«‹å³è¿è¡Œï¼Œåªè´Ÿè´£ï¼š
    1. Title/BulletPoints ç¿»è¯‘
    2. Review Text ç¿»è¯‘
    
    ä¸è´Ÿè´£ï¼š
    - ç»´åº¦æå–
    - æ´å¯Ÿåˆ†æ
    - ä¸»é¢˜æå–
    
    è®¾è®¡ç†å¿µï¼šè®©ç”¨æˆ·åœ¨å‰å°"è¾¹é‡‡è¾¹çœ‹"ç¿»è¯‘ç»“æœ
    
    ğŸ”’ å¹¶å‘ç­–ç•¥ï¼šä½¿ç”¨ PostgreSQL è¡Œçº§é”ï¼ˆSELECT FOR UPDATE SKIP LOCKEDï¼‰
       - å¤šä¸ªä»»åŠ¡å¯ä»¥å¹¶å‘å¤„ç†åŒä¸€äº§å“çš„ä¸åŒè¯„è®º
       - è‡ªåŠ¨é¿å…é‡å¤å¤„ç†åŒä¸€æ¡è¯„è®º
       - æ— éœ€æ‰‹åŠ¨ç®¡ç†åˆ†å¸ƒå¼é”
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.services.translation import translation_service
    import json
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆæ›´å¤§çš„å»¶è¿Ÿï¼Œé¿å…ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ç¿»è¯‘ä»»åŠ¡] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"[æµå¼ç¿»è¯‘] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    
    try:
        # 1. è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # 2. ç¿»è¯‘äº§å“æ ‡é¢˜ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.title and not product.title_translated:
            try:
                product.title_translated = translation_service.translate_product_title(product.title)
                logger.info(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å®Œæˆ: {product.title_translated[:30]}...")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
        
        # 3. ç¿»è¯‘äº”ç‚¹æè¿°ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.bullet_points and not product.bullet_points_translated:
            try:
                # Parse bullet points from JSON, PostgreSQL array, or Python list
                bullet_points = []
                if isinstance(product.bullet_points, list):
                    bullet_points = product.bullet_points
                elif isinstance(product.bullet_points, str):
                    bp_str = product.bullet_points.strip()
                    # å°è¯• JSON æ ¼å¼ [...]
                    if bp_str.startswith('['):
                        bullet_points = json.loads(bp_str)
                    # å¤„ç† PostgreSQL æ•°ç»„æ ¼å¼ {...}
                    elif bp_str.startswith('{') and bp_str.endswith('}'):
                        import re
                        content = bp_str[1:-1]  # ç§»é™¤ { }
                        matches = re.findall(r'"([^"]*)"', content)
                        if matches:
                            bullet_points = matches
                        else:
                            bullet_points = [s.strip() for s in content.split(',') if s.strip()]
                    else:
                        try:
                            bullet_points = json.loads(bp_str)
                        except:
                            bullet_points = [bp_str] if bp_str else []
                    
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    # ç»Ÿä¸€ä¿å­˜ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å®Œæˆ: {len(translated_bullets)} æ¡")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å¤±è´¥: {e}")
        
        db.commit()
        
        # =========================================================================
        # 4. ğŸ¯ æ™ºèƒ½æ‰¹é‡ç¿»è¯‘ï¼ˆå·®å¼‚åŒ–å¤„ç†ï¼šVIPå•ç‹¬/æ ‡å‡†5æ¡/çŸ­è¯„20æ¡ï¼‰
        # =========================================================================
        # 
        # ç­–ç•¥ï¼š
        # - VIP è¯„è®ºï¼ˆ>200å­—æˆ–æç«¯æ˜Ÿçº§>100å­—ï¼‰ï¼šå•ç‹¬ç¿»è¯‘ï¼Œä¿è¯è´¨é‡
        # - æ ‡å‡†è¯„è®ºï¼ˆ50-200å­—ï¼‰ï¼š5 æ¡ä¸€æ‰¹
        # - çŸ­è¯„è®ºï¼ˆâ‰¤50å­—ï¼‰ï¼š20 æ¡ä¸€æ‰¹ï¼Œæœ€å¤§åŒ–æ•ˆç‡
        #
        # ä¼˜åŠ¿ï¼š
        # - è´¨é‡ä¿è¯ï¼šé‡è¦è¯„è®ºä¸é™ä½ç¿»è¯‘è´¨é‡
        # - æ•ˆç‡æœ€å¤§åŒ–ï¼šçŸ­è¯„è®º QPS æ¶ˆè€—é™ä½ 20 å€
        # - çµæ´»å¹³è¡¡ï¼šä¸­ç­‰è¯„è®ºå…¼é¡¾è´¨é‡å’Œæ•ˆç‡
        #
        translated_count = 0
        failed_count = 0
        
        # ç»Ÿè®¡ä¸åŒç±»åˆ«çš„å¤„ç†æƒ…å†µ
        category_stats = {
            'vip': {'total': 0, 'success': 0},
            'standard': {'total': 0, 'success': 0},
            'short': {'total': 0, 'success': 0}
        }
        
        # æ¯æ¬¡è·å–æ›´å¤šè¯„è®ºï¼ŒæŒ‰åˆ†ç±»å¤„ç†
        MAX_FETCH_SIZE = 100  # æ¯æ¬¡æœ€å¤šè·å– 100 æ¡å¾…ç¿»è¯‘è¯„è®º
        
        while True:
            # ğŸ”’ è·å–å¾…ç¿»è¯‘è¯„è®ºï¼ˆä½¿ç”¨ PostgreSQL è¡Œçº§é”ï¼‰
            pending_result = db.execute(
                select(Review)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status.in_([
                            TranslationStatus.PENDING.value,
                            TranslationStatus.FAILED.value
                        ]),
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(MAX_FETCH_SIZE)
                .with_for_update(skip_locked=True)
            )
            pending_reviews = pending_result.scalars().all()
            
            if not pending_reviews:
                logger.info(f"[æ™ºèƒ½ç¿»è¯‘] æ²¡æœ‰æ›´å¤šå¾…ç¿»è¯‘çš„è¯„è®º")
                break
            
            # ğŸ¯ æŒ‰é•¿åº¦å’Œè´¨é‡åˆ†ç±»
            grouped_reviews = ReviewClassifier.group_reviews(pending_reviews)
            
            logger.info(
                f"[æ™ºèƒ½ç¿»è¯‘] ğŸ“Š è¯„è®ºåˆ†ç±»: "
                f"VIP={len(grouped_reviews['vip'])} | "
                f"æ ‡å‡†={len(grouped_reviews['standard'])} | "
                f"çŸ­è¯„={len(grouped_reviews['short'])}"
            )
            
            # å¤„ç†é¡ºåºï¼šçŸ­è¯„ â†’ æ ‡å‡† â†’ VIPï¼ˆä¼˜å…ˆå¿«é€Ÿå¤„ç†å¤§é‡çŸ­è¯„ï¼‰
            for category in ['short', 'standard', 'vip']:
                reviews = grouped_reviews[category]
                if not reviews:
                    continue
                
                batch_size = ReviewClassifier.get_batch_size(category)
                category_stats[category]['total'] += len(reviews)
                
                logger.info(f"[æ™ºèƒ½ç¿»è¯‘] ğŸš€ å¤„ç† {category} ç±»è¯„è®º: {len(reviews)} æ¡ï¼Œæ‰¹é‡å¤§å°={batch_size}")
                
                # æŒ‰æ‰¹é‡å¤§å°åˆ†æ‰¹å¤„ç†
                for i in range(0, len(reviews), batch_size):
                    batch = reviews[i:i+batch_size]
                    
                    # æ ‡è®°ä¸ºå¤„ç†ä¸­
                    for review in batch:
                        review.translation_status = TranslationStatus.PROCESSING.value
                    db.commit()
                    
                    # æ„å»ºæ‰¹é‡ç¿»è¯‘è¯·æ±‚
                    batch_input = []
                    for review in batch:
                        text = review.body_original or ""
                        if text.strip():
                            batch_input.append({
                                "id": str(review.id),
                                "text": text
                            })
                    
                    # ğŸ”¥ æ‰¹é‡ç¿»è¯‘ï¼ˆVIP=1æ¡ï¼Œæ ‡å‡†=5æ¡ï¼ŒçŸ­è¯„=20æ¡ï¼‰
                    try:
                        if batch_size == 1:
                            # VIP è¯„è®ºï¼šå•ç‹¬ç¿»è¯‘
                            review = batch[0]
                            translated = translation_service.translate_text(review.body_original)
                            batch_results = {str(review.id): translated}
                        else:
                            # æ ‡å‡†/çŸ­è¯„ï¼šæ‰¹é‡ç¿»è¯‘
                            batch_results = translation_service.translate_batch_with_fallback(batch_input)
                        
                        logger.info(f"[æ™ºèƒ½ç¿»è¯‘] {category} æ‰¹æ¬¡ç¿»è¯‘å®Œæˆ: {len(batch_results)}/{len(batch)} æ¡")
                    except Exception as e:
                        logger.error(f"[æ™ºèƒ½ç¿»è¯‘] {category} æ‰¹æ¬¡ç¿»è¯‘å¤±è´¥: {e}")
                        batch_results = {}
                    
                    # æ‰¹é‡æ›´æ–°æ•°æ®åº“
                    for review in batch:
                        review_id_str = str(review.id)
                        
                        if review_id_str in batch_results and batch_results[review_id_str]:
                            # ç¿»è¯‘æˆåŠŸ
                            review.body_translated = batch_results[review_id_str]
                            
                            # æ ‡é¢˜å•ç‹¬ç¿»è¯‘
                            if review.title_original and not review.title_translated:
                                try:
                                    review.title_translated = translation_service.translate_text(review.title_original)
                                except:
                                    pass
                            
                            # æƒ…æ„Ÿåˆ†æ
                            try:
                                sentiment = translation_service.analyze_sentiment(review.body_translated)
                                review.sentiment = sentiment.value
                            except:
                                review.sentiment = "neutral"
                            
                            review.translation_status = TranslationStatus.COMPLETED.value
                            translated_count += 1
                            category_stats[category]['success'] += 1
                        else:
                            # ç¿»è¯‘å¤±è´¥
                            review.translation_status = TranslationStatus.FAILED.value
                            failed_count += 1
                    
                    # æäº¤æœ¬æ‰¹æ›´æ–°
                    db.commit()
                    
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å… QPS å†²é«˜
                    time.sleep(0.3 if batch_size == 1 else 0.5)
            
            # å¦‚æœè·å–çš„è¯„è®ºå°‘äº MAX_FETCH_SIZEï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šäº†
            if len(pending_reviews) < MAX_FETCH_SIZE:
                break
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(
            f"[æ™ºèƒ½ç¿»è¯‘] âœ… å®Œæˆ: æ€»è®¡ {translated_count} æ¡æˆåŠŸ, {failed_count} æ¡å¤±è´¥\n"
            f"  ğŸ“Š VIP è¯„è®º: {category_stats['vip']['success']}/{category_stats['vip']['total']} æ¡\n"
            f"  ğŸ“Š æ ‡å‡†è¯„è®º: {category_stats['standard']['success']}/{category_stats['standard']['total']} æ¡\n"
            f"  ğŸ“Š çŸ­è¯„è®º: {category_stats['short']['success']}/{category_stats['short']['total']} æ¡"
        )
        
        logger.info(f"[æµå¼ç¿»è¯‘] å®Œæˆ: ç¿»è¯‘ {translated_count} æ¡, å¤±è´¥ {failed_count} æ¡")
        
        return {
            "success": True,
            "product_id": product_id,
            "translated_count": translated_count,
            "failed_count": failed_count
        }
        
    except Exception as e:
        logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()
        # ğŸ”’ PostgreSQL è¡Œçº§é”ä¼šåœ¨äº‹åŠ¡ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾


# ============== [NEW] ä»»åŠ¡6: ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡« ==============

@celery_app.task(bind=True, max_retries=1, default_retry_delay=60)
def task_scientific_learning_and_analysis(self, product_id: str):
    """
    ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡«ä»»åŠ¡ (Scientific Learning & Backfill)
    
    ç”¨æˆ·ç‚¹å‡»"å¼€å§‹åˆ†æ"æˆ–é‡‡é›†å®Œæˆåè§¦å‘ã€‚
    åˆ©ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œ Schema å­¦ä¹ ï¼Œç„¶åå›å¡«æ‰€æœ‰æ•°æ®ã€‚
    
    æµç¨‹ï¼š
    1. ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰å¾…ç¿»è¯‘ï¼‰
    2. è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ï¼ˆç»´åº¦ + 5Wæ ‡ç­¾ï¼‰
    3. å…¨é‡æ´å¯Ÿå›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–æ´å¯Ÿï¼‰
    4. å…¨é‡ä¸»é¢˜å›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–5Wä¸»é¢˜ï¼‰
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.product_dimension import ProductDimension
    from app.models.product_context_label import ProductContextLabel
    from app.services.translation import translation_service
    import json as json_lib
    import asyncio
    
    logger.info(f"[ç§‘å­¦å­¦ä¹ ] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    
    try:
        # === Step 0: è·å–äº§å“ä¿¡æ¯ ===
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # === Step 1: ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 1: ç§‘å­¦é‡‡æ ·ä¸­...")
        
        # éœ€è¦åŒæ­¥æ–¹å¼æ‰§è¡Œå¼‚æ­¥æ–¹æ³•
        from app.services.review_service import ReviewService
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from app.core.config import settings
        from app.models.review import Review
        
        # ä½¿ç”¨åŒæ­¥æŸ¥è¯¢è·å–ç§‘å­¦é‡‡æ ·
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        if len(raw_samples) < 10:
            logger.warning(f"[ç§‘å­¦å­¦ä¹ ] æ ·æœ¬ä¸è¶³ï¼ˆ{len(raw_samples)} æ¡ï¼‰ï¼Œéœ€è¦è‡³å°‘ 10 æ¡è‹±æ–‡è¯„è®º")
            return {"success": False, "error": f"æ ·æœ¬ä¸è¶³: {len(raw_samples)} æ¡ï¼Œéœ€è¦è‡³å°‘ 10 æ¡"}
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] é‡‡æ ·å®Œæˆ: {len(raw_samples)} æ¡é«˜è´¨é‡è‹±æ–‡è¯„è®º")
        
        # === Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹  ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ä¸­...")
        
        # 2.1 å­¦ä¹ ç»´åº¦ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        dim_count_result = db.execute(
            select(func.count(ProductDimension.id))
            .where(ProductDimension.product_id == product_id)
        )
        dim_count = dim_count_result.scalar() or 0
        
        dimensions_learned = 0
        if dim_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ äº§å“ç»´åº¦ä¸­...")
            dims = translation_service.learn_dimensions_from_raw(
                raw_reviews=raw_samples,
                product_title=product_title,
                bullet_points="\n".join(bullet_points) if bullet_points else ""
            )
            
            if dims:
                for dim in dims:
                    dimension = ProductDimension(
                        product_id=product_id,
                        name=dim["name"],
                        description=dim.get("description", ""),
                        is_ai_generated=True
                    )
                    db.add(dimension)
                db.commit()
                dimensions_learned = len(dims)
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] ç»´åº¦å­¦ä¹ å®Œæˆ: {dimensions_learned} ä¸ª")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {dim_count} ä¸ªç»´åº¦ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # 2.2 å­¦ä¹ 5Wæ ‡ç­¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        labels_learned = 0
        if label_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
            labels = translation_service.learn_context_labels_from_raw(
                raw_reviews=raw_samples,
                product_title=product_title,
                bullet_points=bullet_points
            )
            
            if labels:
                for context_type in ["who", "where", "when", "why", "what"]:
                    type_labels = labels.get(context_type, [])
                    for item in type_labels:
                        if isinstance(item, dict) and item.get("name"):
                            label = ProductContextLabel(
                                product_id=product_id,
                                type=context_type,
                                name=item["name"].strip(),
                                description=item.get("description", "").strip() or None,
                                count=0,
                                is_ai_generated=True
                            )
                            db.add(label)
                            labels_learned += 1
                db.commit()
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_learned} ä¸ª")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {label_count} ä¸ª5Wæ ‡ç­¾ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # === Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡«...")
        task_extract_insights.delay(product_id)
        
        # === Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡«...")
        task_extract_themes.delay(product_id)
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] å®Œæˆ: ç»´åº¦ +{dimensions_learned}, æ ‡ç­¾ +{labels_learned}")
        
        return {
            "success": True,
            "product_id": product_id,
            "samples_used": len(raw_samples),
            "dimensions_learned": dimensions_learned,
            "labels_learned": labels_learned,
            "backfill_triggered": True
        }
        
    except Exception as e:
        logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡7: å…¨è‡ªåŠ¨åˆ†æï¼ˆé‡‡é›†å®Œæˆåè§¦å‘ï¼‰==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=120)
def task_full_auto_analysis(self, product_id: str, task_id: str):
    """
    ğŸš€ å…¨è‡ªåŠ¨åˆ†æä»»åŠ¡ (Full Auto Analysis Pipeline) - æµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆ
    
    é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿ã€‚
    
    â­ æ ¸å¿ƒä¼˜åŒ–ï¼šç¿»è¯‘åœ¨ ingest æ—¶å°±å·²å¼€å§‹ï¼ˆæµå¼ä¸Šä¼ è¾¹å­˜è¾¹è¯‘ï¼‰
    
    æµå¼å¹¶è¡Œæµç¨‹ï¼š
    
    [æ•°æ®é‡‡é›†é˜¶æ®µ - åœ¨æ­¤ä»»åŠ¡è§¦å‘ä¹‹å‰]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘ï¼ˆtask_ingest_translation_onlyï¼‰
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘
    ...ï¼ˆæŒç»­è¿›è¡Œä¸­ï¼‰
    
    [é‡‡é›†å®Œæˆåè§¦å‘æ­¤ä»»åŠ¡]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Step 1: å­¦ä¹ ç»´åº¦+5Wæ ‡ç­¾ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰ç¿»è¯‘ï¼‰
                              â†“
    Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–ï¼ˆç¿»è¯‘æ­¤æ—¶å·²åœ¨è¿›è¡Œä¸­ï¼ï¼‰
                              â†“
    Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆ
            â”œâ”€ ç¿»è¯‘ï¼ˆå·²åœ¨è¿›è¡Œï¼Œä¼šå…ˆå®Œæˆï¼‰
            â”œâ”€ æ´å¯Ÿæå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
            â””â”€ ä¸»é¢˜æå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
                              â†“
    Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
    
    æ—¶é—´ä¼˜åŒ–ï¼š
    - ç¿»è¯‘åœ¨é‡‡é›†æ—¶å°±å¼€å§‹ â†’ ä¸ç­‰å¾…
    - å­¦ä¹ åŸºäºè‹±æ–‡åŸæ–‡ â†’ ä¸ä¾èµ–ç¿»è¯‘
    - ä¸‰ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ â†’ å¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´
    - é¢„è®¡èŠ‚çœ 50%+ çš„æ€»æ—¶é—´
    
    Args:
        product_id: äº§å“ UUID
        task_id: AUTO_ANALYSIS ä»»åŠ¡ UUIDï¼ˆç”¨äºæ›´æ–°è¿›åº¦ï¼‰
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.models.task import Task, TaskStatus, TaskType
    from app.models.insight import ReviewInsight
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.report import ProductReport, ReportType, ReportStatus
    from app.services.translation import translation_service
    from datetime import datetime, timezone
    import json as json_lib
    
    # ğŸš¦ VIP å¿«è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆé¿å… Worker é‡å¯æ—¶ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.1, 0.5)
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸŒŸ VIP å¿«è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}sï¼ˆé˜²æ­¢ QPS å†²é«˜ï¼‰")
    time.sleep(startup_delay)
    
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸš€ å¼€å§‹å¤„ç†äº§å“ {product_id}ï¼Œä»»åŠ¡ {task_id}")
    
    db = get_sync_db()
    
    def update_task_progress(step: int, status: str = TaskStatus.PROCESSING.value, error: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            task_update = {
                "processed_items": step,
                "status": status,
                "last_heartbeat": datetime.now(timezone.utc)
            }
            if error:
                task_update["error_message"] = error
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(**task_update)
            )
            db.commit()
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
    
    try:
        # è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} ä¸å­˜åœ¨")
            update_task_progress(0, TaskStatus.FAILED.value, "äº§å“ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # ==========================================
        # Step 1: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ä¾èµ–ç¿»è¯‘ï¼ï¼‰
        # ==========================================
        update_task_progress(1, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 1/3: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰...")
        
        # ç›´æ¥è°ƒç”¨ç§‘å­¦å­¦ä¹ ä»»åŠ¡çš„é€»è¾‘ï¼ˆåŒæ­¥æ‰§è¡Œï¼‰
        from app.models.product_dimension import ProductDimension
        from app.models.product_context_label import ProductContextLabel
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        if len(raw_samples) >= 10:
            # å­¦ä¹ ç»´åº¦
            dim_count_result = db.execute(
                select(func.count(ProductDimension.id))
                .where(ProductDimension.product_id == product_id)
            )
            dim_count = dim_count_result.scalar() or 0
            
            if dim_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ äº§å“ç»´åº¦ä¸­...")
                try:
                    dims = translation_service.learn_dimensions_from_raw(
                        raw_reviews=raw_samples,
                        product_title=product_title,
                        bullet_points="\n".join(bullet_points) if bullet_points else ""
                    )
                    if dims:
                        for dim in dims:
                            dimension = ProductDimension(
                                product_id=product_id,
                                name=dim["name"],
                                description=dim.get("description", ""),
                                is_ai_generated=True
                            )
                            db.add(dimension)
                        db.commit()
                        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ å®Œæˆ: {len(dims)} ä¸ª")
                except Exception as e:
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ å¤±è´¥: {e}")
            
            # å­¦ä¹ 5Wæ ‡ç­¾
            label_count_result = db.execute(
                select(func.count(ProductContextLabel.id))
                .where(ProductContextLabel.product_id == product_id)
            )
            label_count = label_count_result.scalar() or 0
            
            if label_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
                try:
                    labels = translation_service.learn_context_labels_from_raw(
                        raw_reviews=raw_samples,
                        product_title=product_title,
                        bullet_points=bullet_points
                    )
                    if labels:
                        labels_saved = 0
                        for context_type in ["who", "where", "when", "why", "what"]:
                            type_labels = labels.get(context_type, [])
                            for item in type_labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                                    labels_saved += 1
                        db.commit()
                        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_saved} ä¸ª")
                except Exception as e:
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å¤±è´¥: {e}")
        else:
            logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] æ ·æœ¬ä¸è¶³ï¼ˆ{len(raw_samples)} æ¡ï¼‰ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # ==========================================
        # Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–
        # æ³¨æ„ï¼šç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²ç»å¯åŠ¨äº†ï¼ä¸éœ€è¦åœ¨è¿™é‡Œè§¦å‘
        # ==========================================
        update_task_progress(2, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 2/4: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–...")
        
        # æ£€æŸ¥å½“å‰ç¿»è¯‘è¿›åº¦ï¼ˆç¿»è¯‘åœ¨ ingest æ—¶å°±å·²ç»å¼€å§‹äº†ï¼‰
        pending_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_([
                        TranslationStatus.PENDING.value,
                        TranslationStatus.PROCESSING.value
                    ]),
                    Review.is_deleted == False
                )
            )
        )
        pending_translation = pending_result.scalar() or 0
        
        translated_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.COMPLETED.value,
                    Review.is_deleted == False
                )
            )
        )
        translated_count = translated_result.scalar() or 0
        
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å½“å‰ç¿»è¯‘çŠ¶æ€: å·²ç¿»è¯‘ {translated_count} æ¡, å¾…ç¿»è¯‘ {pending_translation} æ¡")
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ’¡ ç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²å¯åŠ¨ï¼Œç°åœ¨è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–")
        
        # è§¦å‘æ´å¯Ÿå’Œä¸»é¢˜æå–ï¼ˆå®ƒä»¬ä¼šå¤„ç†å·²ç¿»è¯‘çš„è¯„è®ºï¼Œè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
        task_extract_insights.delay(product_id)
        task_extract_themes.delay(product_id)
        
        # ==========================================
        # Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘ + æ´å¯Ÿ + ä¸»é¢˜ï¼‰
        # ==========================================
        update_task_progress(3, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 3/4: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘+æ´å¯Ÿ+ä¸»é¢˜ï¼‰...")
        
        # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰ 30 åˆ†é’Ÿï¼Œä» 15 åˆ†é’Ÿæå‡ï¼‰
        max_wait_seconds = 1800  # ğŸ”¥ ä» 900 ç§’ï¼ˆ15 åˆ†é’Ÿï¼‰æå‡åˆ° 1800 ç§’ï¼ˆ30 åˆ†é’Ÿï¼‰
        wait_interval = 15
        waited = 0
        last_log_time = 0
        
        while waited < max_wait_seconds:
            time.sleep(wait_interval)
            waited += wait_interval
            
            # æ£€æŸ¥ç¿»è¯‘è¿›åº¦
            pending_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status.in_([
                            TranslationStatus.PENDING.value,
                            TranslationStatus.PROCESSING.value
                        ]),
                        Review.is_deleted == False
                    )
                )
            )
            pending_translation = pending_result.scalar() or 0
            
            # æ£€æŸ¥å·²ç¿»è¯‘çš„è¯„è®ºæ•°
            translated_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False
                    )
                )
            )
            translated_count = translated_result.scalar() or 0
            
            # æ£€æŸ¥æ´å¯Ÿæå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–æ´å¯Ÿçš„è¯„è®ºï¼‰
            no_insight_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewInsight.review_id).where(ReviewInsight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_insights = no_insight_result.scalar() or 0
            
            # æ£€æŸ¥ä¸»é¢˜æå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–ä¸»é¢˜çš„è¯„è®ºï¼‰
            no_theme_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewThemeHighlight.review_id).where(ReviewThemeHighlight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_themes = no_theme_result.scalar() or 0
            
            # æ¯30ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            if waited - last_log_time >= 30:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å¹¶è¡Œè¿›åº¦ - å¾…ç¿»è¯‘:{pending_translation} | å¾…æ´å¯Ÿ:{pending_insights} | å¾…ä¸»é¢˜:{pending_themes}")
                last_log_time = waited
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%å®Œæˆåº¦ï¼Œå¯ä»¥æå‰è§¦å‘æŠ¥å‘Šç”Ÿæˆ
            insights_completion = (translated_count - pending_insights) / translated_count if translated_count > 0 else 0
            themes_completion = (translated_count - pending_themes) / translated_count if translated_count > 0 else 0
            
            # ğŸš€ ä¼˜åŒ–ï¼š90%å®Œæˆåº¦å³å¯è§¦å‘æŠ¥å‘Šç”Ÿæˆ
            if insights_completion >= 0.90 and themes_completion >= 0.90:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… è¾¾åˆ°90%å®Œæˆåº¦ï¼Œè§¦å‘æŠ¥å‘Šç”Ÿæˆï¼æ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}")
                break
            
            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆï¼ˆ100%ï¼‰
            if pending_translation == 0 and pending_insights == 0 and pending_themes == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… å¹¶è¡Œå¤„ç†å…¨éƒ¨å®Œæˆï¼å·²ç¿»è¯‘:{translated_count}æ¡")
                break
            
            # [OPTIMIZED] æ¯ 120 ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œåªåœ¨è¿›åº¦åœæ»æ—¶é‡æ–°è§¦å‘
            # é¿å…é¢‘ç¹è§¦å‘å¯¼è‡´ä»»åŠ¡å †ç§¯ï¼Œå½±å“å…¶ä»–ç”¨æˆ·
            if waited % 120 == 0 and waited > 0:
                # ç¿»è¯‘ä»»åŠ¡ï¼šåªåœ¨æœ‰å¤§é‡å¾…ç¿»è¯‘æ—¶è§¦å‘
                if pending_translation > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ”„ é‡æ–°è§¦å‘ç¿»è¯‘ä»»åŠ¡ï¼ˆè¿˜æœ‰{pending_translation}æ¡å¾…å¤„ç†ï¼‰")
                    task_ingest_translation_only.delay(product_id)
                # æ´å¯Ÿ/ä¸»é¢˜ï¼šåªè§¦å‘ä¸€ä¸ªï¼Œé¿å…å ç”¨å¤ªå¤šèµ„æº
                if pending_insights > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘æ´å¯Ÿæå–ï¼ˆè¿˜æœ‰{pending_insights}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_insights.delay(product_id)
                elif pending_themes > 10:  # ç”¨ elif é¿å…åŒæ—¶è§¦å‘
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘ä¸»é¢˜æå–ï¼ˆè¿˜æœ‰{pending_themes}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_themes.delay(product_id)
            
            # æ›´æ–°å¿ƒè·³
            update_task_progress(3, TaskStatus.PROCESSING.value)
        
        if waited >= max_wait_seconds:
            # ğŸ”¥ ä¼˜åŒ–ï¼šæ”¾å®½å®Œæˆåº¦è¦æ±‚ï¼Œä» 95% é™åˆ° 80%
            # ç†ç”±ï¼š80% å·²è¶³å¤Ÿç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šï¼Œå‰©ä½™ä»»åŠ¡å¯å¼‚æ­¥ç»§ç»­
            insights_completion = (translated_count - pending_insights) / translated_count if translated_count > 0 else 0
            themes_completion = (translated_count - pending_themes) / translated_count if translated_count > 0 else 0
            
            if insights_completion < 0.80 or themes_completion < 0.80:
                logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] âš ï¸ ç­‰å¾…è¶…æ—¶ä¸”å®Œæˆåº¦ <80%ï¼ˆæ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}ï¼‰")
                update_task_progress(3, TaskStatus.FAILED.value, f"å¤„ç†è¶…æ—¶ï¼Œæ´å¯Ÿå®Œæˆåº¦:{insights_completion:.0%}ï¼Œä¸»é¢˜å®Œæˆåº¦:{themes_completion:.0%}")
                return {
                    "success": False,
                    "product_id": product_id,
                    "task_id": task_id,
                    "error": f"å¹¶è¡Œå¤„ç†è¶…æ—¶ä¸”å®Œæˆåº¦ä¸è¶³80%ï¼Œè¯·ç¨åé‡è¯•ã€‚æ´å¯Ÿ:{insights_completion:.0%}ï¼Œä¸»é¢˜:{themes_completion:.0%}"
                }
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] å¹¶è¡Œå¤„ç†ç­‰å¾…è¶…æ—¶ï¼Œä½†å®Œæˆåº¦è¾¾åˆ°80%ä»¥ä¸Šï¼Œç»§ç»­ç”ŸæˆæŠ¥å‘Šï¼ˆæ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}ï¼‰")
        
        # ==========================================
        # Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
        # ==========================================
        update_task_progress(4, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 4/4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨æŠ¥å‘Šç”Ÿæˆ
            # ç”±äº SummaryService æ˜¯å¼‚æ­¥çš„ï¼Œéœ€è¦ä½¿ç”¨ asyncio
            import asyncio
            from app.services.summary_service import SummaryService
            
            async def generate_report_async():
                # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥ï¼šengine å’Œ async_session_maker
                from app.db.session import async_session_maker
                
                async with async_session_maker() as async_db:
                    summary_service = SummaryService(async_db)
                    result = await summary_service.generate_report(
                        product_id=product_id,
                        report_type="comprehensive",  # ç»¼åˆæˆ˜ç•¥ç‰ˆ
                        min_reviews=10,
                        save_to_db=True,
                        force_regenerate=False,  # [NEW] ä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œæ£€æŸ¥å»é‡
                        require_full_completion=False  # [ä¼˜åŒ–] å…è®¸90%å®Œæˆåº¦ç”ŸæˆæŠ¥å‘Š
                    )
                    await async_db.commit()  # ç¡®ä¿æäº¤
                    return result
            
            # è¿è¡Œå¼‚æ­¥å‡½æ•°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                report_result = loop.run_until_complete(generate_report_async())
            finally:
                loop.close()
            
            if report_result.get("success"):
                report_id = report_result.get("report_id")
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼ŒæŠ¥å‘ŠID: {report_id}")
                
                # æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œä¿å­˜æŠ¥å‘Š ID
                try:
                    db.execute(
                        update(Task)
                        .where(Task.id == task_id)
                        .values(error_message=f"report_id:{report_id}")  # ä¸´æ—¶å­˜å‚¨æŠ¥å‘ŠID
                    )
                    db.commit()
                except Exception as save_err:
                    logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ä¿å­˜æŠ¥å‘ŠIDå¤±è´¥: {save_err}")
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_result.get('error')}")
                
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            # ä¸å› æŠ¥å‘Šç”Ÿæˆå¤±è´¥è€Œä¸­æ–­æ•´ä¸ªä»»åŠ¡
        
        # ==========================================
        # å®Œæˆ
        # ==========================================
        update_task_progress(4, TaskStatus.COMPLETED.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… äº§å“ {product_id} å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ï¼ˆæµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆï¼‰")
        
        return {
            "success": True,
            "product_id": product_id,
            "task_id": task_id,
            "message": "å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰"
        }
        
    except Exception as e:
        logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        update_task_progress(0, TaskStatus.FAILED.value, str(e))
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡8: å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ ==============

@celery_app.task(bind=True, max_retries=0)
def task_check_pending_translations(self):
    """
    ğŸ”„ å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ (Periodic Translation Check)
    
    æ¯ 15 ç§’ç”± Celery Beat è§¦å‘ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…ç¿»è¯‘çš„äº§å“ã€‚
    å¦‚æœæœ‰ï¼Œä¸ºæ¯ä¸ªäº§å“è§¦å‘ 3 ä¸ªå¹¶è¡Œç¿»è¯‘ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤š Worker å¹¶å‘ã€‚
    
    è®¾è®¡ç†å¿µï¼š
    - ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨è¡Œçº§é”ï¼ˆSKIP LOCKEDï¼‰ï¼Œå¤šä»»åŠ¡å¯ä»¥å®‰å…¨å¹¶å‘
    - è§¦å‘å¤šä¸ªä»»åŠ¡è®© 6 ä¸ª Worker çº¿ç¨‹éƒ½æœ‰æ´»å¹²
    - é¿å…ç¿»è¯‘å› è¡Œçº§é”ç«äº‰è€Œæå‰ç»“æŸ
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    
    db = get_sync_db()
    
    try:
        # æŸ¥æ‰¾æœ‰å¾…ç¿»è¯‘è¯„è®ºçš„äº§å“ï¼ˆæœ€å¤šå¤„ç† 5 ä¸ªäº§å“ï¼‰
        products_with_pending = db.execute(
            select(Product.id, func.count(Review.id).label("pending_count"))
            .join(Review, Review.product_id == Product.id)
            .where(
                and_(
                    Review.translation_status.in_([
                        TranslationStatus.PENDING.value,
                        TranslationStatus.FAILED.value
                    ]),
                    Review.is_deleted == False
                )
            )
            .group_by(Product.id)
            .having(func.count(Review.id) > 0)
            .order_by(func.count(Review.id).desc())
            .limit(5)
        )
        
        pending_products = products_with_pending.all()
        
        if not pending_products:
            return {"triggered": 0, "message": "No pending translations"}
        
        triggered = 0
        for product_id, pending_count in pending_products:
            # ğŸ”¥ ä¸ºæ¯ä¸ªäº§å“è§¦å‘å¤šä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆå……åˆ†åˆ©ç”¨å¹¶å‘ï¼‰
            # æ ¹æ®å¾…ç¿»è¯‘æ•°é‡å†³å®šè§¦å‘å‡ ä¸ªä»»åŠ¡
            num_tasks = min(3, max(1, pending_count // 20))  # æ¯ 20 æ¡è§¦å‘ 1 ä¸ªä»»åŠ¡ï¼Œæœ€å¤š 3 ä¸ª
            
            for _ in range(num_tasks):
                task_ingest_translation_only.delay(str(product_id))
                triggered += 1
            
            logger.info(f"[ç¿»è¯‘è°ƒåº¦] äº§å“ {product_id} å¾…ç¿»è¯‘ {pending_count} æ¡ï¼Œè§¦å‘ {num_tasks} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        return {
            "triggered": triggered,
            "products": len(pending_products),
            "message": f"Triggered {triggered} translation tasks for {len(pending_products)} products"
        }
        
    except Exception as e:
        logger.error(f"[ç¿»è¯‘è°ƒåº¦] æ£€æŸ¥å¤±è´¥: {e}")
        return {"triggered": 0, "error": str(e)}
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡9: é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=10)
def task_process_ingestion_queue(self):
    """
    ğŸš€ é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ä»»åŠ¡ (Ingestion Queue Consumer)
    
    ä» Redis é˜Ÿåˆ—æ‰¹é‡æ¶ˆè´¹è¯„è®ºæ•°æ®ï¼Œå…¥åº“åˆ° PostgreSQLã€‚
    
    è®¾è®¡ç‰¹ç‚¹ï¼š
    1. é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–ï¼šAPI å±‚åªå†™ Redisï¼Œæœ¬ä»»åŠ¡æ‰¹é‡å…¥åº“
    2. ä¸‰å±‚å»é‡ï¼šRedis Set â†’ å†…å­˜ Set â†’ DB ON CONFLICT
    3. æŒ‰ ASIN åˆ†ç»„å¤„ç†ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
    4. å…¥åº“æˆåŠŸåè§¦å‘ç¿»è¯‘ä»»åŠ¡
    
    è°ƒåº¦æ–¹å¼ï¼š
    - Celery Beat æ¯ 5 ç§’è§¦å‘ä¸€æ¬¡
    - æ¯æ¬¡æœ€å¤šå¤„ç† 100 æ¡é˜Ÿåˆ—æ•°æ®
    
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    from app.core.redis import ReviewIngestionQueueSync, get_sync_redis
    from app.services.ingestion_service import IngestionService
    
    logger.debug("[Ingestion] å¼€å§‹æ¶ˆè´¹é˜Ÿåˆ—...")
    
    redis_cli = get_sync_redis()
    queue = ReviewIngestionQueueSync(redis_cli)
    
    # Step 1: ä»é˜Ÿåˆ—æ‰¹é‡å–å‡ºæ•°æ®
    items = queue.pop_batch(count=100)
    
    if not items:
        logger.debug("[Ingestion] é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡")
        return {"processed": 0, "items": 0}
    
    logger.info(f"[Ingestion] ä»é˜Ÿåˆ—å–å‡º {len(items)} æ¡æ•°æ®")
    
    db = get_sync_db()
    
    try:
        # Step 2: è°ƒç”¨å…¥åº“æœåŠ¡å¤„ç†
        service = IngestionService(db, redis_cli)
        results = service.process_queue_items(items)
        
        # Step 3: ç»Ÿè®¡ç»“æœ
        total_inserted = sum(r.get("inserted", 0) for r in results.values())
        total_skipped = sum(r.get("skipped", 0) for r in results.values())
        
        logger.info(
            f"[Ingestion] å¤„ç†å®Œæˆ: {len(results)} ä¸ªäº§å“, "
            f"æ–°å¢ {total_inserted} æ¡, è·³è¿‡ {total_skipped} æ¡"
        )
        
        # Step 4: ä¸ºæœ‰æ–°æ•°æ®çš„äº§å“è§¦å‘ç¿»è¯‘
        for asin, result in results.items():
            if result.get("inserted", 0) > 0:
                # è·å– product_id
                from app.models.product import Product
                product_result = db.execute(
                    select(Product).where(Product.asin == asin)
                )
                product = product_result.scalar_one_or_none()
                
                if product:
                    # è§¦å‘æµå¼ç¿»è¯‘
                    task_ingest_translation_only.delay(str(product.id))
                    logger.info(f"[Ingestion] äº§å“ {asin} å·²è§¦å‘ç¿»è¯‘ä»»åŠ¡")
        
        return {
            "processed": len(items),
            "products": len(results),
            "inserted": total_inserted,
            "skipped": total_skipped,
            "details": results
        }
        
    except Exception as e:
        logger.error(f"[Ingestion] å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] è¾…åŠ©å‡½æ•°ï¼šåŒæ­¥å·²æœ‰ review_id åˆ° Redis ==============

@celery_app.task
def task_sync_product_reviews_to_redis(asin: str):
    """
    å°†äº§å“çš„å·²æœ‰ review_id åŒæ­¥åˆ° Redis
    
    ç”¨äºï¼š
    1. Redis é‡å¯åæ¢å¤å»é‡æ•°æ®
    2. æ‰‹åŠ¨è§¦å‘åŒæ­¥
    
    Args:
        asin: äº§å“ ASIN
    """
    from app.services.ingestion_service import IngestionService
    
    db = get_sync_db()
    
    try:
        service = IngestionService(db)
        service.sync_redis_from_db(asin)
        logger.info(f"[Sync] äº§å“ {asin} çš„ review_id å·²åŒæ­¥åˆ° Redis")
        return {"success": True, "asin": asin}
    except Exception as e:
        logger.error(f"[Sync] åŒæ­¥å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()
