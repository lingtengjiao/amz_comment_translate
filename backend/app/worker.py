"""
Celery Worker Configuration and Tasks

This module handles asynchronous processing of reviews:
1. Translation via Qwen API
2. Sentiment analysis
3. Database updates
"""
import logging
import time
import random
from typing import Optional
from functools import wraps

from celery import Celery
from sqlalchemy import create_engine, select, update, and_, func
from sqlalchemy.orm import sessionmaker
import redis
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# ğŸš¦ å…¨å±€ API é™æµå™¨ï¼ˆé˜²æ­¢ QPS å†²é«˜å¯¼è‡´è´¦å·è¢«å°ï¼‰
# ============================================================================

class APIRateLimiter:
    """
    å…¨å±€ API é™æµå™¨ï¼Œé˜²æ­¢ç¬é—´ QPS å†²é«˜
    
    ç­–ç•¥ï¼š
    - ä½¿ç”¨ Redis æ»‘åŠ¨çª—å£è®¡æ•°
    - æœ€å¤§ QPS = 25ï¼ˆåƒé—® API é™åˆ¶ 20-30 QPSï¼‰
    - è¶…è¿‡é™åˆ¶æ—¶ï¼Œéšæœºé€€é¿ 0.1-0.5 ç§’
    """
    def __init__(self, redis_client, max_qps=25, window_seconds=1):
        self.redis_client = redis_client
        self.max_qps = max_qps
        self.window_seconds = window_seconds
        self.key_prefix = "api_rate_limit"
    
    def acquire(self, api_name="qwen"):
        """
        è·å– API è°ƒç”¨è®¸å¯
        
        Returns:
            bool: True if allowed, False if rate limited
        """
        key = f"{self.key_prefix}:{api_name}"
        current_time = time.time()
        window_start = current_time - self.window_seconds
        
        # æ¸…ç†è¿‡æœŸè®¡æ•°
        self.redis_client.zremrangebyscore(key, 0, window_start)
        
        # æ£€æŸ¥å½“å‰çª—å£å†…çš„è¯·æ±‚æ•°
        current_count = self.redis_client.zcard(key)
        
        if current_count >= self.max_qps:
            # è¶…è¿‡é™åˆ¶ï¼Œéšæœºé€€é¿
            backoff = random.uniform(0.1, 0.5)
            logger.warning(f"[é™æµ] API QPS è¾¾åˆ° {current_count}/{self.max_qps}ï¼Œé€€é¿ {backoff:.2f}s")
            time.sleep(backoff)
            return False
        
        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self.redis_client.zadd(key, {str(current_time): current_time})
        self.redis_client.expire(key, self.window_seconds * 2)  # 2 å€çª—å£æ—¶é—´è¿‡æœŸ
        
        return True
    
    def wait_and_acquire(self, api_name="qwen", max_retries=10):
        """
        ç­‰å¾…ç›´åˆ°è·å–åˆ° API è°ƒç”¨è®¸å¯
        
        Args:
            api_name: API åç§°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for i in range(max_retries):
            if self.acquire(api_name):
                return True
            time.sleep(random.uniform(0.05, 0.2))  # çŸ­æš‚éšæœºé€€é¿
        
        raise Exception(f"[é™æµ] æ— æ³•è·å– API è®¸å¯ï¼Œå·²é‡è¯• {max_retries} æ¬¡")

# Redis å®¢æˆ·ç«¯ï¼ˆç”¨äºåˆ†å¸ƒå¼é”å’Œé™æµï¼‰
redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

# å…¨å±€é™æµå™¨å®ä¾‹
api_limiter = APIRateLimiter(redis_client, max_qps=25)

def rate_limited_api(api_name="qwen"):
    """
    API é™æµè£…é¥°å™¨
    
    ç”¨æ³•ï¼š
        @rate_limited_api("qwen")
        def call_qwen_api():
            ...
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç­‰å¾…è·å– API è®¸å¯
            api_limiter.wait_and_acquire(api_name)
            
            # è°ƒç”¨åŸå‡½æ•°
            return func(*args, **kwargs)
        
        return wrapper
    return decorator


# ============================================================================
# ğŸ”¥ æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹æŸ¥è¯¢ PostgreSQLï¼‰
# ============================================================================

class LabelCacheManager:
    """
    æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ç®¡ç†å™¨
    
    ä¼˜åŒ–ç‚¹ï¼š
    - å°†çƒ­é—¨äº§å“çš„æ ‡ç­¾åº“å¸¸é©» Redis
    - é¿å… Worker æ¯æ¬¡æå–ä¸»é¢˜éƒ½æŸ¥è¯¢æ ‡ç­¾è¡¨
    - ç¼“å­˜æœ‰æ•ˆæœŸ 1 å°æ—¶ï¼ˆæ ‡ç­¾åº“å˜åŒ–ä¸é¢‘ç¹ï¼‰
    """
    CACHE_PREFIX = "label_cache"
    CACHE_TTL = 3600  # 1 å°æ—¶
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
    
    def get_label_id_map(self, product_id: str) -> dict:
        """
        ä»ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨
        
        Returns:
            dict: {(theme_type, label_name): label_id} æˆ– Noneï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        """
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            try:
                import json
                data = json.loads(cached_data)
                # é‡å»º tuple key
                return {(k.split("|")[0], k.split("|")[1]): v for k, v in data.items()}
            except Exception as e:
                logger.warning(f"[æ ‡ç­¾ç¼“å­˜] è§£æç¼“å­˜å¤±è´¥: {e}")
                return None
        
        return None
    
    def set_label_id_map(self, product_id: str, label_id_map: dict):
        """
        å°†æ ‡ç­¾æ˜ å°„è¡¨å­˜å…¥ç¼“å­˜
        
        Args:
            product_id: äº§å“ ID
            label_id_map: {(theme_type, label_name): label_id}
        """
        if not label_id_map:
            return
        
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        
        try:
            import json
            # å°† tuple key è½¬æ¢ä¸ºå­—ç¬¦ä¸² key
            data = {f"{k[0]}|{k[1]}": str(v) for k, v in label_id_map.items()}
            self.redis_client.setex(cache_key, self.CACHE_TTL, json.dumps(data))
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾ï¼ˆäº§å“: {product_id}ï¼‰")
        except Exception as e:
            logger.warning(f"[æ ‡ç­¾ç¼“å­˜] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
    
    def invalidate(self, product_id: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆï¼ˆæ ‡ç­¾åº“æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        self.redis_client.delete(cache_key)
        logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²æ¸…é™¤ç¼“å­˜ï¼ˆäº§å“: {product_id}ï¼‰")

# å…¨å±€æ ‡ç­¾ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
label_cache = LabelCacheManager(redis_client)

# Create Celery application
celery_app = Celery(
    "voc_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
)

# Celery configuration
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=1800,  # 30 minutes timeout per task (increased from 600s to handle large batches)
    task_soft_time_limit=1500,  # 25 minutes soft limit (warning before hard kill)
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    # ============================================================================
    # ğŸš€ 5 é˜Ÿåˆ— + 4 Worker é«˜ååæ¶æ„ï¼ˆ4æ ¸16Gï¼Œæ”¯æŒ 400 å¹¶å‘ APIï¼‰
    # ============================================================================
    #
    # è®¾è®¡ç†å¿µï¼š
    # - å¿«è½¦é“ï¼šå…¥åº“ + æŠ¥å‘Šï¼ˆç§’çº§å“åº”ï¼‰
    # - VIP å¿«è½¦é“ï¼šå­¦ä¹ å»ºæ¨¡ï¼ˆæ–°äº§å“ç§’çº§å¯åŠ¨ï¼‰
    # - æ…¢è½¦é“ï¼šç¿»è¯‘ + åˆ†æï¼ˆè¶…é«˜å¹¶å‘ AI è°ƒç”¨ï¼‰
    #
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ Worker 1: åŸºç¡€å“åº”å‘˜ (Prefork, 4çº¿ç¨‹)                                   â”‚
    # â”‚   Queue: ingestion, reports                                            â”‚
    # â”‚   ç‰¹ç‚¹ï¼šçº¯ CPU + ç£ç›˜ï¼Œä¿è¯ API æ°¸è¿œä¸å¡                                 â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 2: VIP å»ºæ¨¡å‘˜ (Gevent, 100åç¨‹)                                  â”‚
    # â”‚   Queue: learning                                                       â”‚
    # â”‚   ç‰¹ç‚¹ï¼šæ–°äº§å“ç§’çº§å»ºæ¨¡ï¼Œç‹¬ç«‹å¿«è½¦é“                                        â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 3 & 4: AI ååä¸»åŠ› (Gevent, å„150åç¨‹)                           â”‚
    # â”‚   Queue: learning, translation, analysis                               â”‚
    # â”‚   ç‰¹ç‚¹ï¼š300 å¹¶å‘ APIï¼Œç¿»è¯‘/æ´å¯Ÿ/ä¸»é¢˜ä¸€èµ·å¤„ç†                             â”‚
    # â”‚   learning é˜Ÿåˆ—ä¹Ÿç›‘å¬ï¼Œä½œä¸º VIP Worker çš„å¤‡ä»½                            â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # æ€»å¹¶å‘ï¼š4 + 100 + 300 = 404 å¹¶å‘ï¼
    #
    task_routes={
        # ============== å¿«è½¦é“ï¼šå…¥åº“ + æŠ¥å‘Š (worker-base, Prefork) ==============
        # ğŸï¸ çº¯ CPU + ç£ç›˜ï¼Œä¿è¯ API ç§’çº§å“åº”
        "app.worker.task_process_ingestion_queue": {"queue": "ingestion"},
        "app.worker.task_check_pending_translations": {"queue": "ingestion"},
        "app.worker.task_generate_report": {"queue": "reports"},
        
        # ============== VIP å¿«è½¦é“ï¼šå­¦ä¹ å»ºæ¨¡ (worker-learning, Gevent) ==============
        # ğŸŒŸ æ–°äº§å“ç§’çº§å»ºæ¨¡ï¼Œç‹¬ç«‹è¿›ç¨‹ä¸å—å¹²æ‰°
        "app.worker.task_full_auto_analysis": {"queue": "learning"},
        "app.worker.task_scientific_learning_and_analysis": {"queue": "learning"},
        
        # ============== æ…¢è½¦é“ï¼šç¿»è¯‘ + åˆ†æ (worker-heavy Ã— 2, Gevent) ==============
        # ğŸ¢ ç¿»è¯‘é˜Ÿåˆ—
        "app.worker.task_translate_bullet_points": {"queue": "translation"},
        "app.worker.task_process_reviews": {"queue": "translation"},
        "app.worker.task_ingest_translation_only": {"queue": "translation"},
        
        # ğŸ¢ åˆ†æé˜Ÿåˆ—ï¼ˆæ´å¯Ÿ + ä¸»é¢˜åˆå¹¶ï¼‰
        "app.worker.task_extract_insights": {"queue": "analysis"},
        "app.worker.task_extract_themes": {"queue": "analysis"},
    },
    # Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®
    beat_schedule={
        # æ¯ 5 ç§’æ¶ˆè´¹ä¸€æ¬¡å…¥åº“é˜Ÿåˆ—
        "process-ingestion-queue": {
            "task": "app.worker.task_process_ingestion_queue",
            "schedule": 5.0,
        },
        # ğŸ”¥ æ¯ 15 ç§’æ£€æŸ¥å¹¶è§¦å‘å¾…ç¿»è¯‘ä»»åŠ¡ï¼ˆç¡®ä¿ç¿»è¯‘æŒç»­è¿›è¡Œï¼‰
        "check-pending-translations": {
            "task": "app.worker.task_check_pending_translations",
            "schedule": 15.0,
        },
    },
)

# ============================================================================
# ğŸ”§ åŒæ­¥æ•°æ®åº“è¿æ¥ï¼ˆCelery Worker ä¸“ç”¨ï¼‰
# ============================================================================
# Celery ä½¿ç”¨ Gevent åç¨‹ï¼Œéœ€è¦ç‰¹æ®Šçš„è¿æ¥æ± é…ç½®
# 
# ç­–ç•¥è¯´æ˜ï¼š
# - ä½¿ç”¨ NullPoolï¼šæ¯æ¬¡æ“ä½œåˆ›å»ºæ–°è¿æ¥ï¼Œé€‚åˆé«˜å¹¶å‘åç¨‹åœºæ™¯
# - é¿å…è¿æ¥æ± ç“¶é¢ˆï¼šGevent 150 åç¨‹ vs é»˜è®¤ pool_size=5 ä¼šä¸¥é‡é˜»å¡
# - PostgreSQL max_connections=500 è¶³ä»¥æ”¯æ’‘
# ============================================================================
from sqlalchemy.pool import NullPool, QueuePool

SYNC_DATABASE_URL = settings.DATABASE_URL.replace("+asyncpg", "")

# ğŸ”¥ é«˜å¹¶å‘è¿æ¥æ± é…ç½®ï¼ˆæ”¯æŒ 400+ å¹¶å‘ Workerï¼‰
sync_engine = create_engine(
    SYNC_DATABASE_URL,
    echo=settings.DEBUG,
    # ä½¿ç”¨ QueuePool é…åˆå¤§å®¹é‡ï¼Œæ¯” NullPool æ›´é«˜æ•ˆ
    poolclass=QueuePool,
    pool_size=100,        # åŸºç¡€è¿æ¥æ•°
    max_overflow=400,     # æº¢å‡ºè¿æ¥æ•°ï¼ˆæ€»å…±æ”¯æŒ 500 è¿æ¥ï¼‰
    pool_timeout=30,      # ç­‰å¾…è¿æ¥è¶…æ—¶
    pool_pre_ping=True,   # æ£€æµ‹æ–­å¼€çš„è¿æ¥
    pool_recycle=1800,    # 30 åˆ†é’Ÿå›æ”¶è¿æ¥ï¼Œé˜²æ­¢æ•°æ®åº“è¶…æ—¶
)
SyncSession = sessionmaker(bind=sync_engine)


def get_sync_db():
    """Get synchronous database session for worker."""
    return SyncSession()


# ============== Worker å¯åŠ¨æ—¶æ¸…ç†å¡ä½çš„ä»»åŠ¡ ==============

def cleanup_stuck_reviews():
    """
    æ¸…ç†å¡åœ¨ 'processing' çŠ¶æ€çš„è¯„è®ºã€‚
    å½“ Worker é‡å¯æ—¶ï¼Œä¹‹å‰æ­£åœ¨å¤„ç†çš„è¯„è®ºå¯èƒ½ä¼šå¡åœ¨ processing çŠ¶æ€ã€‚
    è¿™ä¸ªå‡½æ•°å°†å®ƒä»¬é‡ç½®ä¸º pendingï¼Œè®©å®ƒä»¬å¯ä»¥è¢«é‡æ–°å¤„ç†ã€‚
    """
    from app.models.review import Review
    
    db = get_sync_db()
    try:
        result = db.execute(
            update(Review)
            .where(Review.translation_status == "processing")
            .values(translation_status="pending")
        )
        db.commit()
        
        if result.rowcount > 0:
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {result.rowcount} æ¡å¡ä½çš„è¯„è®ºé‡ç½®ä¸º pending çŠ¶æ€")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¡ä½çš„è¯„è®º")
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†å¡ä½è¯„è®ºå¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def cleanup_stuck_tasks():
    """
    æ¸…ç†å¡ä½çš„ä»»åŠ¡ï¼ˆå¿ƒè·³è¶…æ—¶ï¼‰ã€‚
    å°† PROCESSING çŠ¶æ€ä½†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡æ ‡è®°ä¸º TIMEOUTã€‚
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone, timedelta
    
    db = get_sync_db()
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ processing çŠ¶æ€çš„ä»»åŠ¡
        result = db.execute(
            select(Task).where(Task.status == TaskStatus.PROCESSING.value)
        )
        tasks = result.scalars().all()
        
        timeout_count = 0
        for task in tasks:
            if task.is_heartbeat_timeout:
                task.status = TaskStatus.TIMEOUT.value
                task.error_message = f"å¿ƒè·³è¶…æ—¶ï¼šæœ€åå¿ƒè·³æ—¶é—´ {task.last_heartbeat}"
                timeout_count += 1
                logger.warning(f"[å¯åŠ¨æ¸…ç†] ä»»åŠ¡ {task.id} ({task.task_type}) å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸º TIMEOUT")
        
        if timeout_count > 0:
            db.commit()
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {timeout_count} ä¸ªè¶…æ—¶ä»»åŠ¡æ ‡è®°ä¸º TIMEOUT")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡")
            
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†è¶…æ—¶ä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


# ============== å¿ƒè·³æ›´æ–°è¾…åŠ©å‡½æ•° ==============

def update_task_heartbeat(db, task_id: str, processed_items: int = None):
    """
    æ›´æ–°ä»»åŠ¡å¿ƒè·³æ—¶é—´ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        processed_items: å¯é€‰ï¼ŒåŒæ—¶æ›´æ–°å·²å¤„ç†æ•°é‡
    """
    from app.models.task import Task
    from datetime import datetime, timezone
    
    try:
        values = {"last_heartbeat": datetime.now(timezone.utc)}
        if processed_items is not None:
            values["processed_items"] = processed_items
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡å¿ƒè·³å¤±è´¥: {e}")
        db.rollback()


def get_or_create_task(db, product_id: str, task_type: str, total_items: int = 0, celery_task_id: str = None):
    """
    è·å–æˆ–åˆ›å»ºä»»åŠ¡è®°å½•ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        product_id: äº§å“ ID
        task_type: ä»»åŠ¡ç±»å‹
        total_items: æ€»é¡¹ç›®æ•°
        celery_task_id: Celery ä»»åŠ¡ ID
        
    Returns:
        Task: ä»»åŠ¡å¯¹è±¡
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone
    
    # æŸ¥æ‰¾ç°æœ‰ä»»åŠ¡
    result = db.execute(
        select(Task).where(
            and_(
                Task.product_id == product_id,
                Task.task_type == task_type
            )
        )
    )
    task = result.scalar_one_or_none()
    
    now = datetime.now(timezone.utc)
    
    if task:
        # æ›´æ–°ç°æœ‰ä»»åŠ¡
        task.status = TaskStatus.PROCESSING.value
        task.total_items = total_items
        task.processed_items = 0
        task.last_heartbeat = now
        task.celery_task_id = celery_task_id
        task.error_message = None
    else:
        # åˆ›å»ºæ–°ä»»åŠ¡
        task = Task(
            product_id=product_id,
            task_type=task_type,
            status=TaskStatus.PROCESSING.value,
            total_items=total_items,
            processed_items=0,
            last_heartbeat=now,
            celery_task_id=celery_task_id
        )
        db.add(task)
    
    db.commit()
    db.refresh(task)
    return task


def complete_task(db, task_id: str, success: bool = True, error_message: str = None):
    """
    å®Œæˆä»»åŠ¡ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        success: æ˜¯å¦æˆåŠŸ
        error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    from app.models.task import Task, TaskStatus
    
    try:
        status = TaskStatus.COMPLETED.value if success else TaskStatus.FAILED.value
        values = {
            "status": status,
            "last_heartbeat": None  # æ¸…é™¤å¿ƒè·³ï¼Œè¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
        }
        if error_message:
            values["error_message"] = error_message
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()


# ä½¿ç”¨ Celery ä¿¡å·åœ¨ Worker å¯åŠ¨æ—¶æ‰§è¡Œæ¸…ç†
from celery.signals import worker_ready

@worker_ready.connect
def on_worker_ready(**kwargs):
    """Worker å¯åŠ¨å®Œæˆåæ‰§è¡Œæ¸…ç†"""
    logger.info("Worker å·²å°±ç»ªï¼Œå¼€å§‹æ£€æŸ¥å¡ä½çš„ä»»åŠ¡...")
    cleanup_stuck_reviews()
    cleanup_stuck_tasks()  # [NEW] æ¸…ç†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡


# ============== ä»»åŠ¡1: äº”ç‚¹ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def task_translate_bullet_points(self, product_id: str):
    """
    Translate product bullet points and title.
    This task should run FIRST, before review translation.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.product import Product
    from app.services.translation import translation_service
    import json
    
    logger.info(f"Starting bullet points translation for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Get product
        result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = result.scalar_one_or_none()
        
        if not product:
            logger.error(f"Product {product_id} not found")
            return {"success": False, "error": "Product not found"}
        
        translated_title = None
        translated_bullets = None
        
        # 1. Translate product title if not already translated
        if product.title and not product.title_translated:
            try:
                translated_title = translation_service.translate_product_title(product.title)
                product.title_translated = translated_title
                logger.info(f"Translated product title: {translated_title[:50]}...")
            except Exception as e:
                logger.error(f"Failed to translate product title: {e}")
        
        # 2. Translate bullet points if not already translated
        if product.bullet_points and not product.bullet_points_translated:
            try:
                # Parse bullet points from JSON
                bullet_points = json.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
                
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"Translated {len(translated_bullets)} bullet points")
            except Exception as e:
                logger.error(f"Failed to translate bullet points: {e}")
        
        db.commit()
        
        return {
            "success": True,
            "product_id": product_id,
            "title_translated": translated_title is not None,
            "bullet_points_translated": translated_bullets is not None
        }
        
    except Exception as e:
        logger.error(f"Bullet points translation failed for product {product_id}: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡2: è¯„è®ºç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
def task_process_reviews(self, product_id: str, task_id: str):
    """
    Async task to process and translate reviews.
    
    Workflow:
    1. Get pending reviews from database
    2. For each review:
       a. Call Qwen API for translation
       b. Analyze sentiment
       c. Extract insights (æ·±åº¦è§£è¯»)
       d. Update database
       e. Update task progress
    3. Mark task as completed
    
    Args:
        product_id: UUID of the product
        task_id: UUID of the task to track progress
    """
    from app.models.review import Review
    from app.models.task import Task
    from app.models.insight import ReviewInsight
    from app.services.translation import translation_service
    
    logger.info(f"Starting translation task {task_id} for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Update task status to processing
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(status="processing")
        )
        db.commit()
        
        # Get pending reviews (including processing and failed - to retry stuck/failed translations)
        # ordered by review_date descending (newest first, matching frontend display)
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_(["pending", "processing", "failed"])
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        failed = 0
        
        logger.info(f"Found {total_reviews} pending reviews to translate")
        
        for review in reviews:
            try:
                # Mark as processing
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(translation_status="processing")
                )
                db.commit()
                
                # Validate body_original exists
                if not review.body_original or not review.body_original.strip():
                    logger.warning(f"Review {review.id} has empty body, skipping translation")
                    db.execute(
                        update(Review)
                        .where(Review.id == review.id)
                        .values(translation_status="failed")
                    )
                    db.commit()
                    failed += 1
                    continue
                
                # åªåšç¿»è¯‘ï¼Œä¸æå–æ´å¯Ÿï¼ˆæ´å¯Ÿéœ€è¦ç”¨æˆ·æ‰‹åŠ¨è§¦å‘ï¼‰
                title_translated, body_translated, sentiment, _ = translation_service.translate_review(
                    title=review.title_original,
                    body=review.body_original,
                    extract_insights=False  # å…³é—­è‡ªåŠ¨æ´å¯Ÿæå–
                )
                
                # Validate translation results
                if not body_translated or not body_translated.strip():
                    logger.error(f"Translation returned empty for review {review.id}, body: {review.body_original[:100]}")
                    raise ValueError("Translation returned empty result")
                
                # Update review with translation only
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        title_translated=title_translated if title_translated and title_translated.strip() else None,
                        body_translated=body_translated,
                        sentiment=sentiment.value,
                        translation_status="completed"
                    )
                )
                
                processed += 1
                
                # Update task progress
                db.execute(
                    update(Task)
                    .where(Task.id == task_id)
                    .values(processed_items=processed)
                )
                db.commit()
                
                logger.debug(f"Translated review {review.id}: {review.rating} stars")
                
                # Rate limiting: wait between API calls
                time.sleep(0.2)
                
            except Exception as e:
                logger.error(f"Failed to translate review {review.id}: {e}", exc_info=True)
                failed += 1
                
                # Mark review as failed (don't save empty translations)
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        translation_status="failed",
                        title_translated=None,
                        body_translated=None
                    )
                )
                db.commit()
                
                # Continue with next review
                continue
        
        # Check if there are still pending reviews
        from app.models.review import TranslationStatus
        pending_count_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.PENDING.value
                )
            )
        )
        pending_count = pending_count_result.scalar() or 0
        
        # Update task status - only mark as completed if no pending reviews
        if pending_count == 0:
            final_status = "completed" if failed == 0 else "completed"
        else:
            # Still have pending reviews, keep as processing
            final_status = "processing"
        
        error_msg = f"{failed} reviews failed" if failed > 0 else None
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(
                status=final_status,
                processed_items=processed,
                error_message=error_msg
            )
        )
        db.commit()
        
        logger.info(f"Task {task_id} completed: {processed} translated, {failed} failed")
        
        return {
            "task_id": task_id,
            "product_id": product_id,
            "total": total_reviews,
            "processed": processed,
            "failed": failed
        }
        
    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}")
        
        # Mark task as failed
        try:
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(
                    status="failed",
                    error_message=str(e)
                )
            )
            db.commit()
        except:
            pass
        
        # Retry the task
        raise self.retry(exc=e)
        
    finally:
        db.close()


@celery_app.task
def task_health_check():
    """Simple task to verify worker is running."""
    return {"status": "healthy", "worker": "voc_worker"}


@celery_app.task
def task_retry_failed_reviews(product_id: str):
    """
    Retry translation for failed reviews.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    
    db = get_sync_db()
    
    try:
        # Reset failed reviews to pending
        result = db.execute(
            update(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "failed"
                )
            )
            .values(translation_status="pending")
        )
        db.commit()
        
        logger.info(f"Reset {result.rowcount} failed reviews to pending")
        
        # Trigger processing
        task_process_reviews.delay(product_id, None)
        
    finally:
        db.close()


@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_extract_insights(self, product_id: str):
    """
    Extract insights for already translated reviews (without re-translating).
    
    This task:
    1. Gets all translated reviews that don't have insights yet
    2. **[NEW] Loads product-specific dimensions if available**
    3. Calls AI to extract insights (using dimensions for categorization)
    4. Saves insights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.insight import ReviewInsight
    from app.models.product_dimension import ProductDimension
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[æ´å¯Ÿæå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting insight extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] è·å–äº§å“çš„ç»´åº¦ Schemaï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        dimension_result = db.execute(
            select(ProductDimension)
            .where(ProductDimension.product_id == product_id)
            .order_by(ProductDimension.created_at)
        )
        dimensions = dimension_result.scalars().all()
        
        # è½¬æ¢ä¸º schema æ ¼å¼
        dimension_schema = None
        if dimensions and len(dimensions) > 0:
            dimension_schema = [
                {"name": dim.name, "description": dim.description or ""}
                for dim in dimensions
            ]
            logger.info(f"ä½¿ç”¨ {len(dimension_schema)} ä¸ªäº§å“ç»´åº¦è¿›è¡Œæ´å¯Ÿæå–")
        else:
            logger.info(f"äº§å“æš‚æ— å®šä¹‰ç»´åº¦ï¼Œä½¿ç”¨é€šç”¨æ´å¯Ÿæå–é€»è¾‘")
        
        # [FIX] å…ˆè·å–æ€»è¯„è®ºæ•°ï¼ˆå·²ç¿»è¯‘çš„è¯„è®ºï¼‰
        total_translated_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False
                )
            )
        )
        total_translated = total_translated_result.scalar() or 0
        
        # [FIX] è·å–å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°ï¼ˆprocessed_itemsï¼‰
        already_processed_result = db.execute(
            select(func.count(func.distinct(ReviewInsight.review_id)))
            .join(Review, Review.id == ReviewInsight.review_id)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.is_deleted == False
                )
            )
        )
        already_processed = already_processed_result.scalar() or 0
        
        # [NEW] åˆ›å»º/æ›´æ–° Task è®°å½•ï¼ˆtotal_items = æ€»è¯„è®ºæ•°ï¼Œprocessed_items = å·²å¤„ç†æ•°ï¼‰
        task_record = get_or_create_task(
            db=db,
            product_id=product_id,
            task_type=TaskType.INSIGHTS.value,
            total_items=total_translated,  # æ€»è¯„è®ºæ•°ï¼ˆå›ºå®šå€¼ï¼‰
            celery_task_id=self.request.id
        )
        # è®¾ç½®å·²å¤„ç†æ•°ä¸ºå½“å‰å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°
        task_record.processed_items = already_processed
        db.commit()
        logger.info(f"Task record: total_items={total_translated}, processed_items={already_processed}, remaining={total_translated - already_processed}")
        
        # [FIX] ä½¿ç”¨ NOT EXISTS å­æŸ¥è¯¢æ’é™¤å·²æœ‰æ´å¯Ÿçš„è¯„è®ºï¼Œé¿å…é‡å¤å¤„ç†
        insight_exists_subquery = (
            select(ReviewInsight.id)
            .where(ReviewInsight.review_id == Review.id)
            .exists()
        )
        
        # Get translated reviews that DON'T have insights yet - ordered by review_date to match page display order
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False,
                    ~insight_exists_subquery  # [FIX] Only process reviews without insights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        reviews_to_process = len(reviews)
        processed = 0
        insights_extracted = 0
        
        # ğŸ”¥ æ‰¹é‡å…¥åº“ä¼˜åŒ–ï¼ˆBulk Insertï¼‰ï¼šå‡å°‘ç£ç›˜ IO
        BATCH_SIZE = 20  # æ¯ 20 æ¡è¯„è®ºæ‰¹é‡æäº¤ä¸€æ¬¡
        pending_insights = []  # å¾…æäº¤çš„æ´å¯Ÿåˆ—è¡¨
        
        logger.info(f"Found {reviews_to_process} reviews remaining for insight extraction (total={total_translated}, already_done={already_processed})")
        logger.info(f"[æ‰¹é‡ä¼˜åŒ–] ä½¿ç”¨ BATCH_SIZE={BATCH_SIZE} å‡å°‘ç£ç›˜ IO")
        
        for review in reviews:
            try:
                # å¯¹æ¯æ¡è¯„è®ºéƒ½æ‰§è¡Œæ´å¯Ÿæå–ï¼ˆå³ä½¿å†…å®¹å¾ˆçŸ­ï¼Œç»“æœå¯èƒ½ä¸ºç©ºï¼‰
                # [UPDATED] ä¼ å…¥ç»´åº¦ schemaï¼Œè®© AI æŒ‰å®šä¹‰çš„ç»´åº¦åˆ†ç±»
                insights = translation_service.extract_insights(
                    original_text=review.body_original or "",
                    translated_text=review.body_translated or "",
                    dimension_schema=dimension_schema  # [NEW] æ³¨å…¥ç»´åº¦
                )
                
                # [FIX] ç”±äºç°åœ¨åªå¤„ç†æ²¡æœ‰æ´å¯Ÿçš„è¯„è®ºï¼Œä¸éœ€è¦åˆ é™¤æ—§æ•°æ®
                # Insert new insights (if any)
                if insights:
                    for insight_data in insights:
                        insight = ReviewInsight(
                            review_id=review.id,
                            insight_type=insight_data.get('type', 'emotion'),
                            quote=insight_data.get('quote', ''),
                            quote_translated=insight_data.get('quote_translated'),
                            analysis=insight_data.get('analysis', ''),
                            dimension=insight_data.get('dimension')
                        )
                        pending_insights.append(insight)
                    
                    insights_extracted += len(insights)
                    logger.debug(f"Extracted {len(insights)} insights for review {review.id}")
                else:
                    # å³ä½¿æ²¡æœ‰æ´å¯Ÿï¼Œä¹Ÿæ’å…¥ä¸€ä¸ªæ ‡è®°è®°å½•ï¼Œè¡¨ç¤ºå·²å¤„ç†
                    # è¿™æ ·ç»Ÿè®¡ä¼šæ˜¾ç¤º 100%ï¼Œä¸”ä¸‹æ¬¡ä¸ä¼šé‡å¤å¤„ç†
                    empty_marker = ReviewInsight(
                        review_id=review.id,
                        insight_type="_empty",  # ç‰¹æ®Šæ ‡è®°ï¼Œè¡¨ç¤ºå†…å®¹å¤ªçŸ­æ— æ´å¯Ÿ
                        quote="",
                        analysis=""
                    )
                    pending_insights.append(empty_marker)
                    logger.debug(f"No insights found for review {review.id} (content too short), marked as processed")
                
                processed += 1
                
                # ğŸ”¥ æ‰¹é‡æäº¤ï¼šæ¯ BATCH_SIZE æ¡è¯„è®ºæäº¤ä¸€æ¬¡ï¼Œå‡å°‘ç£ç›˜åå¤æŠ˜ç£¨
                if processed % BATCH_SIZE == 0:
                    if pending_insights:
                        db.add_all(pending_insights)
                        db.commit()
                        logger.info(f"[æ‰¹é‡å…¥åº“] å·²æäº¤ {len(pending_insights)} æ¡æ´å¯Ÿï¼ˆè¿›åº¦: {processed}/{reviews_to_process}ï¼‰")
                        pending_insights = []
                    
                    # æ›´æ–° Task è¿›åº¦
                    if task_record:
                        task_record.processed_items = already_processed + processed
                        db.commit()
                
                # Rate limitingï¼ˆé™æµå™¨åœ¨ API å±‚å·²å¤„ç†ï¼Œè¿™é‡ŒåªåšåŸºæœ¬å»¶è¿Ÿï¼‰
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Failed to extract insights for review {review.id}: {e}")
                # æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œå•æ¡å¤±è´¥ä¸å›æ»šæ•´ä¸ªæ‰¹æ¬¡
                continue
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†æ´å¯Ÿ
        if pending_insights:
            db.add_all(pending_insights)
            db.commit()
            logger.info(f"[æ‰¹é‡å…¥åº“] æœ€ç»ˆæäº¤ {len(pending_insights)} æ¡æ´å¯Ÿ")
        
        logger.info(f"Insight extraction completed: processed {processed} new reviews (total={total_translated}, now_done={already_processed + processed}), {insights_extracted} insights extracted")
        
        # [FIX] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.processed_items = already_processed + processed  # æœ€ç»ˆå¤„ç†æ•°
            db.commit()
        
        return {
            "product_id": product_id,
            "total_reviews": total_translated,  # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å˜é‡å
            "processed": processed,
            "insights_extracted": insights_extracted
        }
        
    except Exception as e:
        logger.error(f"Insight extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡4: ä¸»é¢˜é«˜äº®æå– ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30, time_limit=1800, soft_time_limit=1700)
def task_extract_themes(self, product_id: str):
    """
    Extract 5W theme keywords for already translated reviews.
    
    This task:
    1. **[NEW] Auto-generates 5W context labels if not exists (Definition phase)**
    2. Gets all translated reviews that don't have theme highlights yet
    3. **[NEW] Uses context labels for forced categorization (Execution phase)**
    4. Calls AI to extract 5W themes with evidence and explanation
    5. Saves theme highlights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.product_context_label import ProductContextLabel
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists, func
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ä¸»é¢˜æå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting theme extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] Step 1: æ£€æŸ¥æ˜¯å¦æœ‰ 5W æ ‡ç­¾åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ç”Ÿæˆ
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        context_schema = None
        labels_generated = False
        
        if label_count == 0:
            logger.info(f"äº§å“ {product_id} æš‚æ—  5W æ ‡ç­¾åº“ï¼Œå¼€å§‹è‡ªåŠ¨å­¦ä¹ ...")
            
            # [NEW] å…ˆè·å–äº§å“ä¿¡æ¯ï¼ˆæ ‡é¢˜å’Œäº”ç‚¹ï¼‰
            from app.models.product import Product
            import json as json_lib
            
            product_result = db.execute(
                select(Product).where(Product.id == product_id)
            )
            product = product_result.scalar_one_or_none()
            
            product_title = ""
            bullet_points = []
            
            if product:
                product_title = product.title or ""
                # è§£æäº”ç‚¹ï¼ˆå­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
                if product.bullet_points:
                    try:
                        bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
                    except:
                        bullet_points = []
                logger.info(f"ğŸ“¦ äº§å“ä¿¡æ¯ï¼š{product.asin}ï¼Œæ ‡é¢˜é•¿åº¦={len(product_title)}ï¼Œäº”ç‚¹={len(bullet_points)}æ¡")
            
            # è·å–å·²ç¿»è¯‘çš„è¯„è®ºæ ·æœ¬ï¼ˆè‡³å°‘10æ¡ï¼‰
            sample_result = db.execute(
                select(Review.body_original, Review.body_translated)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == "completed",
                        Review.body_translated.isnot(None),
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(50)
            )
            sample_reviews = sample_result.all()
            
            if len(sample_reviews) >= 30:
                # å‡†å¤‡æ ·æœ¬æ–‡æœ¬
                sample_texts = []
                for row in sample_reviews:
                    text = row.body_translated or row.body_original
                    if text and text.strip():
                        sample_texts.append(text.strip())
                
                if len(sample_texts) >= 30:
                    # [UPDATED] è°ƒç”¨ AI å­¦ä¹ æ ‡ç­¾åº“ï¼ˆä¼ å…¥äº§å“ä¿¡æ¯ï¼‰
                    learned_labels = translation_service.learn_context_labels(
                        reviews_text=sample_texts,
                        product_title=product_title,      # [NEW] äº§å“æ ‡é¢˜
                        bullet_points=bullet_points       # [NEW] äº”ç‚¹å–ç‚¹
                    )
                    
                    if learned_labels:
                        # å­˜å…¥æ•°æ®åº“
                        for context_type in ["who", "where", "when", "why", "what"]:
                            labels = learned_labels.get(context_type, [])
                            for item in labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                        
                        db.commit()
                        labels_generated = True
                        total_labels = sum(len(v) for v in learned_labels.values())
                        logger.info(f"âœ… è‡ªåŠ¨ç”Ÿæˆ 5W æ ‡ç­¾åº“æˆåŠŸï¼Œå…± {total_labels} ä¸ªæ ‡ç­¾")
                    else:
                        logger.warning(f"âš ï¸ AI å­¦ä¹ æ ‡ç­¾åº“å¤±è´¥ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
                else:
                    logger.warning(f"âš ï¸ æœ‰æ•ˆæ ·æœ¬ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘30æ¡ï¼‰ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
            else:
                logger.warning(f"âš ï¸ å·²ç¿»è¯‘è¯„è®ºä¸è¶³ï¼ˆéœ€è¦è‡³å°‘30æ¡ï¼‰ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # Step 2: è·å–æ ‡ç­¾åº“ Schemaï¼ˆå¦‚æœå­˜åœ¨æˆ–åˆšç”Ÿæˆï¼‰
        if label_count > 0 or labels_generated:
            label_result = db.execute(
                select(ProductContextLabel)
                .where(ProductContextLabel.product_id == product_id)
                .order_by(ProductContextLabel.type, ProductContextLabel.created_at)
            )
            labels = label_result.scalars().all()
            
            if labels:
                context_schema = {}
                for label in labels:
                    if label.type not in context_schema:
                        context_schema[label.type] = []
                    context_schema[label.type].append({
                        "name": label.name,
                        "description": label.description or ""
                    })
                logger.info(f"âœ… ä½¿ç”¨ 5W æ ‡ç­¾åº“è¿›è¡Œå¼ºåˆ¶å½’ç±»ï¼Œå…± {len(labels)} ä¸ªæ ‡ç­¾")
        else:
            logger.info(f"â„¹ï¸ æœªä½¿ç”¨æ ‡ç­¾åº“ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # Get translated reviews that don't have theme highlights yet
        # Use a subquery to check for existing theme highlights
        theme_exists_subquery = (
            select(ReviewThemeHighlight.id)
            .where(ReviewThemeHighlight.review_id == Review.id)
            .exists()
        )
        
        # Ordered by review_date to match page display order
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "completed",
                    Review.body_translated.isnot(None),
                    Review.is_deleted == False,
                    ~theme_exists_subquery  # Reviews without theme highlights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        themes_extracted = 0
        
        logger.info(f"Found {total_reviews} translated reviews for theme extraction")
        
        # [NEW] åˆ›å»º/æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œå¯ç”¨å¿ƒè·³
        if total_reviews > 0:
            task_record = get_or_create_task(
                db=db,
                product_id=product_id,
                task_type=TaskType.THEMES.value,
                total_items=total_reviews,
                celery_task_id=self.request.id
            )
            logger.info(f"ä»»åŠ¡è®°å½•å·²åˆ›å»º: {task_record.id}")
        
        # ğŸ”¥ ä¼˜å…ˆä» Redis ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆé¿å…é¢‘ç¹æŸ¥ PostgreSQLï¼‰
        label_id_map = label_cache.get_label_id_map(str(product_id))
        
        if label_id_map:
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âœ… å‘½ä¸­ç¼“å­˜ï¼Œå…± {len(label_id_map)} ä¸ªæ ‡ç­¾")
        elif context_schema:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“æ„å»ºå¹¶ç¼“å­˜
            label_id_map = {}
            for label in labels:
                key = (label.type, label.name)
                label_id_map[key] = label.id
            
            # å­˜å…¥ Redis ç¼“å­˜
            label_cache.set_label_id_map(str(product_id), label_id_map)
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âš¡ å·²æ„å»ºå¹¶ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾")
        else:
            label_id_map = {}
            logger.debug(f"æ— æ ‡ç­¾åº“ï¼Œä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # ğŸ”¥ æ‰¹é‡å…¥åº“ä¼˜åŒ–ï¼ˆBulk Insertï¼‰ï¼šå‡å°‘ç£ç›˜ IO
        BATCH_SIZE = 20  # æ¯ 20 æ¡è¯„è®ºæ‰¹é‡æäº¤ä¸€æ¬¡
        pending_themes = []  # å¾…æäº¤çš„ä¸»é¢˜åˆ—è¡¨
        logger.info(f"[æ‰¹é‡ä¼˜åŒ–] ä½¿ç”¨ BATCH_SIZE={BATCH_SIZE} å‡å°‘ç£ç›˜ IO")
        
        for review in reviews:
            try:
                # å¯¹æ¯æ¡è¯„è®ºéƒ½æ‰§è¡Œä¸»é¢˜æå–ï¼ˆå³ä½¿å†…å®¹å¾ˆçŸ­ï¼Œç»“æœå¯èƒ½ä¸ºç©ºï¼‰
                # ğŸ”¥ æ‰¹é‡æ¨¡å¼ï¼šä¸å†æ¯æ¡åˆ é™¤æ—§æ•°æ®ï¼ˆå› ä¸ºåªå¤„ç†æ²¡æœ‰ä¸»é¢˜çš„è¯„è®ºï¼‰
                
                # [UPDATED] Extract themes with context schema (forced categorization)
                themes = translation_service.extract_themes(
                    original_text=review.body_original or "",
                    translated_text=review.body_translated or "",
                    context_schema=context_schema  # [NEW] ä½¿ç”¨æ ‡ç­¾åº“è¿›è¡Œå¼ºåˆ¶å½’ç±»
                )
                
                # [UPDATED] Insert theme highlights - ä¸€æ¡è®°å½• = ä¸€ä¸ªæ ‡ç­¾
                if themes:
                    for theme_type, items in themes.items():
                        if not items or len(items) == 0:
                            continue
                        
                        for item in items:
                            # è·å–æ ‡ç­¾ä¿¡æ¯ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼štag/quote æˆ– content/content_originalï¼‰
                            label_name = item.get("content", "").strip()
                            # åŸæ–‡è¯æ®ï¼ˆå…¼å®¹ quote å’Œ content_originalï¼‰
                            quote = item.get("quote") or item.get("content_original") or None
                            # ä¸­æ–‡ç¿»è¯‘è¯æ®ï¼ˆå…¼å®¹ quote_translated å’Œ content_translatedï¼‰
                            quote_translated = item.get("quote_translated") or item.get("content_translated") or None
                            explanation = item.get("explanation") or None
                            
                            if not label_name:
                                continue
                            
                            # [NEW] æŸ¥æ‰¾å¯¹åº”çš„ context_label_id
                            context_label_id = label_id_map.get((theme_type, label_name))
                            
                            # åˆ›å»ºä¸€æ¡è®°å½•å¯¹åº”ä¸€ä¸ªæ ‡ç­¾
                            theme_highlight = ReviewThemeHighlight(
                                review_id=review.id,
                                theme_type=theme_type,
                                label_name=label_name,               # æ ‡ç­¾åç§°
                                quote=quote,                         # åŸæ–‡è¯æ®
                                quote_translated=quote_translated,   # [NEW] ä¸­æ–‡ç¿»è¯‘è¯æ®
                                explanation=explanation,             # å½’ç±»ç†ç”±
                                context_label_id=context_label_id,   # å…³è”æ ‡ç­¾åº“ID
                                items=[item]                         # ä¿ç•™ items ç”¨äºå‘åå…¼å®¹
                            )
                            pending_themes.append(theme_highlight)
                            themes_extracted += 1
                    
                    logger.debug(f"Extracted {themes_extracted} theme labels for review {review.id}")
                else:
                    # å³ä½¿æ²¡æœ‰ä¸»é¢˜ï¼Œä¹Ÿæ’å…¥ä¸€ä¸ªæ ‡è®°è®°å½•ï¼Œè¡¨ç¤ºå·²å¤„ç†
                    empty_marker = ReviewThemeHighlight(
                        review_id=review.id,
                        theme_type="_empty",
                        label_name=None,
                        items=None
                    )
                    pending_themes.append(empty_marker)
                    logger.debug(f"No themes found for review {review.id}, marked as processed")
                
                processed += 1
                
                # ğŸ”¥ æ‰¹é‡æäº¤ï¼šæ¯ BATCH_SIZE æ¡è¯„è®ºæäº¤ä¸€æ¬¡
                if processed % BATCH_SIZE == 0:
                    if pending_themes:
                        db.add_all(pending_themes)
                        db.commit()
                        logger.info(f"[æ‰¹é‡å…¥åº“] å·²æäº¤ {len(pending_themes)} æ¡ä¸»é¢˜ï¼ˆè¿›åº¦: {processed}/{total_reviews}ï¼‰")
                        pending_themes = []
                    
                    # æ›´æ–° Task è¿›åº¦
                    if task_record:
                        update_task_heartbeat(db, str(task_record.id), processed_items=processed)
                
                # Rate limitingï¼ˆé™æµå™¨åœ¨ API å±‚å·²å¤„ç†ï¼‰
                time.sleep(0.1)
                
            except Exception as e:
                logger.error(f"Failed to extract themes for review {review.id}: {e}")
                # æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œå•æ¡å¤±è´¥ä¸å›æ»šæ•´ä¸ªæ‰¹æ¬¡
                continue
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†ä¸»é¢˜
        if pending_themes:
            db.add_all(pending_themes)
            db.commit()
            logger.info(f"[æ‰¹é‡å…¥åº“] æœ€ç»ˆæäº¤ {len(pending_themes)} æ¡ä¸»é¢˜")
        
        logger.info(f"Theme extraction completed: {processed}/{total_reviews} reviews processed, {themes_extracted} theme entries created")
        
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.total_items = total_reviews
            task_record.processed_items = processed
            db.commit()
        
        return {
            "product_id": product_id,
            "total_reviews": total_reviews,
            "processed": processed,
            "themes_extracted": themes_extracted
        }
        
    except Exception as e:
        logger.error(f"Theme extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡5: æµå¼è½»é‡ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_ingest_translation_only(self, product_id: str):
    """
    æµå¼è½»é‡ç¿»è¯‘ä»»åŠ¡ (Stream Translation Only)
    
    æ•°æ®å…¥åº“åç«‹å³è¿è¡Œï¼Œåªè´Ÿè´£ï¼š
    1. Title/BulletPoints ç¿»è¯‘
    2. Review Text ç¿»è¯‘
    
    ä¸è´Ÿè´£ï¼š
    - ç»´åº¦æå–
    - æ´å¯Ÿåˆ†æ
    - ä¸»é¢˜æå–
    
    è®¾è®¡ç†å¿µï¼šè®©ç”¨æˆ·åœ¨å‰å°"è¾¹é‡‡è¾¹çœ‹"ç¿»è¯‘ç»“æœ
    
    ğŸ”’ å¹¶å‘ç­–ç•¥ï¼šä½¿ç”¨ PostgreSQL è¡Œçº§é”ï¼ˆSELECT FOR UPDATE SKIP LOCKEDï¼‰
       - å¤šä¸ªä»»åŠ¡å¯ä»¥å¹¶å‘å¤„ç†åŒä¸€äº§å“çš„ä¸åŒè¯„è®º
       - è‡ªåŠ¨é¿å…é‡å¤å¤„ç†åŒä¸€æ¡è¯„è®º
       - æ— éœ€æ‰‹åŠ¨ç®¡ç†åˆ†å¸ƒå¼é”
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.services.translation import translation_service
    import json
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆæ›´å¤§çš„å»¶è¿Ÿï¼Œé¿å…ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ç¿»è¯‘ä»»åŠ¡] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"[æµå¼ç¿»è¯‘] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    
    try:
        # 1. è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # 2. ç¿»è¯‘äº§å“æ ‡é¢˜ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.title and not product.title_translated:
            try:
                product.title_translated = translation_service.translate_product_title(product.title)
                logger.info(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å®Œæˆ: {product.title_translated[:30]}...")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
        
        # 3. ç¿»è¯‘äº”ç‚¹æè¿°ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.bullet_points and not product.bullet_points_translated:
            try:
                bullet_points = json.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å®Œæˆ: {len(translated_bullets)} æ¡")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å¤±è´¥: {e}")
        
        db.commit()
        
        # 4. ğŸ”„ å¾ªç¯ç¿»è¯‘æ‰€æœ‰å¾…å¤„ç†çš„è¯„è®ºï¼ˆåªç¿»è¯‘ï¼Œä¸æå–æ´å¯Ÿï¼‰
        # ä½¿ç”¨å¾ªç¯å¤„ç†æ‰€æœ‰å¾…ç¿»è¯‘è¯„è®ºï¼Œè€Œä¸æ˜¯åªå¤„ç† 100 æ¡
        translated_count = 0
        failed_count = 0
        batch_size = 20  # ğŸ”¥ æ¯æ‰¹å¤„ç† 20 æ¡ï¼ˆåŒ¹é…æµå¼æ’å…¥çš„é¢‘ç‡ï¼‰
        
        while True:
            # ğŸ”’ ä½¿ç”¨ PostgreSQL è¡Œçº§é”é¿å…é‡å¤å¤„ç†
            # FOR UPDATE SKIP LOCKED: è·³è¿‡å·²è¢«å…¶ä»–ä»»åŠ¡é”å®šçš„è¡Œ
            # è¿™æ ·å¤šä¸ªä»»åŠ¡å¯ä»¥å¹¶å‘å¤„ç†ä¸åŒçš„è¯„è®º
            pending_result = db.execute(
                select(Review)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status.in_([
                            TranslationStatus.PENDING.value,
                            TranslationStatus.FAILED.value
                        ]),
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(batch_size)
                .with_for_update(skip_locked=True)  # ğŸ”¥ å…³é”®ï¼šè·³è¿‡å·²é”å®šçš„è¡Œ
            )
            pending_reviews = pending_result.scalars().all()
            
            if not pending_reviews:
                logger.info(f"[æµå¼ç¿»è¯‘] æ²¡æœ‰æ›´å¤šå¾…ç¿»è¯‘çš„è¯„è®º")
                break
            
            logger.info(f"[æµå¼ç¿»è¯‘] å¤„ç†æ‰¹æ¬¡: {len(pending_reviews)} æ¡è¯„è®º")
            
            for review in pending_reviews:
                try:
                    # æ ‡è®°ä¸ºå¤„ç†ä¸­
                    review.translation_status = TranslationStatus.PROCESSING.value
                    db.commit()
                    
                    # åªåšç¿»è¯‘ï¼Œä¸æå–æ´å¯Ÿ
                    title_translated, body_translated, sentiment, _ = translation_service.translate_review(
                        title=review.title_original,
                        body=review.body_original,
                        extract_insights=False  # å…³é—­æ´å¯Ÿæå–
                    )
                    
                    if body_translated and body_translated.strip():
                        review.title_translated = title_translated
                        review.body_translated = body_translated
                        review.sentiment = sentiment.value
                        review.translation_status = TranslationStatus.COMPLETED.value
                        translated_count += 1
                    else:
                        review.translation_status = TranslationStatus.FAILED.value
                        failed_count += 1
                    
                    db.commit()
                    
                    # æ§åˆ¶é€Ÿç‡
                    time.sleep(0.1)
                    
                except Exception as e:
                    logger.warning(f"[æµå¼ç¿»è¯‘] è¯„è®º {review.id} ç¿»è¯‘å¤±è´¥: {e}")
                    review.translation_status = TranslationStatus.FAILED.value
                    db.commit()
                    failed_count += 1
            
            # å¦‚æœè¿™æ‰¹å¤„ç†çš„æ•°é‡å°äº batch_sizeï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šäº†
            if len(pending_reviews) < batch_size:
                break
        
        logger.info(f"[æµå¼ç¿»è¯‘] å®Œæˆ: ç¿»è¯‘ {translated_count} æ¡, å¤±è´¥ {failed_count} æ¡")
        
        return {
            "success": True,
            "product_id": product_id,
            "translated_count": translated_count,
            "failed_count": failed_count
        }
        
    except Exception as e:
        logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()
        # ğŸ”’ PostgreSQL è¡Œçº§é”ä¼šåœ¨äº‹åŠ¡ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾


# ============== [NEW] ä»»åŠ¡6: ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡« ==============

@celery_app.task(bind=True, max_retries=1, default_retry_delay=60)
def task_scientific_learning_and_analysis(self, product_id: str):
    """
    ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡«ä»»åŠ¡ (Scientific Learning & Backfill)
    
    ç”¨æˆ·ç‚¹å‡»"å¼€å§‹åˆ†æ"æˆ–é‡‡é›†å®Œæˆåè§¦å‘ã€‚
    åˆ©ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œ Schema å­¦ä¹ ï¼Œç„¶åå›å¡«æ‰€æœ‰æ•°æ®ã€‚
    
    æµç¨‹ï¼š
    1. ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰å¾…ç¿»è¯‘ï¼‰
    2. è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ï¼ˆç»´åº¦ + 5Wæ ‡ç­¾ï¼‰
    3. å…¨é‡æ´å¯Ÿå›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–æ´å¯Ÿï¼‰
    4. å…¨é‡ä¸»é¢˜å›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–5Wä¸»é¢˜ï¼‰
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.product_dimension import ProductDimension
    from app.models.product_context_label import ProductContextLabel
    from app.services.translation import translation_service
    import json as json_lib
    import asyncio
    
    logger.info(f"[ç§‘å­¦å­¦ä¹ ] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    
    try:
        # === Step 0: è·å–äº§å“ä¿¡æ¯ ===
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # === Step 1: ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 1: ç§‘å­¦é‡‡æ ·ä¸­...")
        
        # éœ€è¦åŒæ­¥æ–¹å¼æ‰§è¡Œå¼‚æ­¥æ–¹æ³•
        from app.services.review_service import ReviewService
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from app.core.config import settings
        from app.models.review import Review
        
        # ä½¿ç”¨åŒæ­¥æŸ¥è¯¢è·å–ç§‘å­¦é‡‡æ ·
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        if len(raw_samples) < 10:
            logger.warning(f"[ç§‘å­¦å­¦ä¹ ] æ ·æœ¬ä¸è¶³ï¼ˆ{len(raw_samples)} æ¡ï¼‰ï¼Œéœ€è¦è‡³å°‘ 10 æ¡è‹±æ–‡è¯„è®º")
            return {"success": False, "error": f"æ ·æœ¬ä¸è¶³: {len(raw_samples)} æ¡ï¼Œéœ€è¦è‡³å°‘ 10 æ¡"}
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] é‡‡æ ·å®Œæˆ: {len(raw_samples)} æ¡é«˜è´¨é‡è‹±æ–‡è¯„è®º")
        
        # === Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹  ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ä¸­...")
        
        # 2.1 å­¦ä¹ ç»´åº¦ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        dim_count_result = db.execute(
            select(func.count(ProductDimension.id))
            .where(ProductDimension.product_id == product_id)
        )
        dim_count = dim_count_result.scalar() or 0
        
        dimensions_learned = 0
        if dim_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ äº§å“ç»´åº¦ä¸­...")
            dims = translation_service.learn_dimensions_from_raw(
                raw_reviews=raw_samples,
                product_title=product_title,
                bullet_points="\n".join(bullet_points) if bullet_points else ""
            )
            
            if dims:
                for dim in dims:
                    dimension = ProductDimension(
                        product_id=product_id,
                        name=dim["name"],
                        description=dim.get("description", ""),
                        is_ai_generated=True
                    )
                    db.add(dimension)
                db.commit()
                dimensions_learned = len(dims)
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] ç»´åº¦å­¦ä¹ å®Œæˆ: {dimensions_learned} ä¸ª")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {dim_count} ä¸ªç»´åº¦ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # 2.2 å­¦ä¹ 5Wæ ‡ç­¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        labels_learned = 0
        if label_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
            labels = translation_service.learn_context_labels_from_raw(
                raw_reviews=raw_samples,
                product_title=product_title,
                bullet_points=bullet_points
            )
            
            if labels:
                for context_type in ["who", "where", "when", "why", "what"]:
                    type_labels = labels.get(context_type, [])
                    for item in type_labels:
                        if isinstance(item, dict) and item.get("name"):
                            label = ProductContextLabel(
                                product_id=product_id,
                                type=context_type,
                                name=item["name"].strip(),
                                description=item.get("description", "").strip() or None,
                                count=0,
                                is_ai_generated=True
                            )
                            db.add(label)
                            labels_learned += 1
                db.commit()
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_learned} ä¸ª")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {label_count} ä¸ª5Wæ ‡ç­¾ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # === Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡«...")
        task_extract_insights.delay(product_id)
        
        # === Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡«...")
        task_extract_themes.delay(product_id)
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] å®Œæˆ: ç»´åº¦ +{dimensions_learned}, æ ‡ç­¾ +{labels_learned}")
        
        return {
            "success": True,
            "product_id": product_id,
            "samples_used": len(raw_samples),
            "dimensions_learned": dimensions_learned,
            "labels_learned": labels_learned,
            "backfill_triggered": True
        }
        
    except Exception as e:
        logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡7: å…¨è‡ªåŠ¨åˆ†æï¼ˆé‡‡é›†å®Œæˆåè§¦å‘ï¼‰==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=120)
def task_full_auto_analysis(self, product_id: str, task_id: str):
    """
    ğŸš€ å…¨è‡ªåŠ¨åˆ†æä»»åŠ¡ (Full Auto Analysis Pipeline) - æµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆ
    
    é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿ã€‚
    
    â­ æ ¸å¿ƒä¼˜åŒ–ï¼šç¿»è¯‘åœ¨ ingest æ—¶å°±å·²å¼€å§‹ï¼ˆæµå¼ä¸Šä¼ è¾¹å­˜è¾¹è¯‘ï¼‰
    
    æµå¼å¹¶è¡Œæµç¨‹ï¼š
    
    [æ•°æ®é‡‡é›†é˜¶æ®µ - åœ¨æ­¤ä»»åŠ¡è§¦å‘ä¹‹å‰]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘ï¼ˆtask_ingest_translation_onlyï¼‰
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘
    ...ï¼ˆæŒç»­è¿›è¡Œä¸­ï¼‰
    
    [é‡‡é›†å®Œæˆåè§¦å‘æ­¤ä»»åŠ¡]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Step 1: å­¦ä¹ ç»´åº¦+5Wæ ‡ç­¾ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰ç¿»è¯‘ï¼‰
                              â†“
    Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–ï¼ˆç¿»è¯‘æ­¤æ—¶å·²åœ¨è¿›è¡Œä¸­ï¼ï¼‰
                              â†“
    Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆ
            â”œâ”€ ç¿»è¯‘ï¼ˆå·²åœ¨è¿›è¡Œï¼Œä¼šå…ˆå®Œæˆï¼‰
            â”œâ”€ æ´å¯Ÿæå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
            â””â”€ ä¸»é¢˜æå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
                              â†“
    Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
    
    æ—¶é—´ä¼˜åŒ–ï¼š
    - ç¿»è¯‘åœ¨é‡‡é›†æ—¶å°±å¼€å§‹ â†’ ä¸ç­‰å¾…
    - å­¦ä¹ åŸºäºè‹±æ–‡åŸæ–‡ â†’ ä¸ä¾èµ–ç¿»è¯‘
    - ä¸‰ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ â†’ å¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´
    - é¢„è®¡èŠ‚çœ 50%+ çš„æ€»æ—¶é—´
    
    Args:
        product_id: äº§å“ UUID
        task_id: AUTO_ANALYSIS ä»»åŠ¡ UUIDï¼ˆç”¨äºæ›´æ–°è¿›åº¦ï¼‰
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.models.task import Task, TaskStatus, TaskType
    from app.models.insight import ReviewInsight
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.report import ProductReport, ReportType, ReportStatus
    from app.services.translation import translation_service
    from datetime import datetime, timezone
    import json as json_lib
    
    # ğŸš¦ VIP å¿«è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆé¿å… Worker é‡å¯æ—¶ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.1, 0.5)
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸŒŸ VIP å¿«è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}sï¼ˆé˜²æ­¢ QPS å†²é«˜ï¼‰")
    time.sleep(startup_delay)
    
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸš€ å¼€å§‹å¤„ç†äº§å“ {product_id}ï¼Œä»»åŠ¡ {task_id}")
    
    db = get_sync_db()
    
    def update_task_progress(step: int, status: str = TaskStatus.PROCESSING.value, error: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            task_update = {
                "processed_items": step,
                "status": status,
                "last_heartbeat": datetime.now(timezone.utc)
            }
            if error:
                task_update["error_message"] = error
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(**task_update)
            )
            db.commit()
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
    
    try:
        # è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} ä¸å­˜åœ¨")
            update_task_progress(0, TaskStatus.FAILED.value, "äº§å“ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # ==========================================
        # Step 1: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ä¾èµ–ç¿»è¯‘ï¼ï¼‰
        # ==========================================
        update_task_progress(1, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 1/3: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰...")
        
        # ç›´æ¥è°ƒç”¨ç§‘å­¦å­¦ä¹ ä»»åŠ¡çš„é€»è¾‘ï¼ˆåŒæ­¥æ‰§è¡Œï¼‰
        from app.models.product_dimension import ProductDimension
        from app.models.product_context_label import ProductContextLabel
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        if len(raw_samples) >= 10:
            # å­¦ä¹ ç»´åº¦
            dim_count_result = db.execute(
                select(func.count(ProductDimension.id))
                .where(ProductDimension.product_id == product_id)
            )
            dim_count = dim_count_result.scalar() or 0
            
            if dim_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ äº§å“ç»´åº¦ä¸­...")
                try:
                    dims = translation_service.learn_dimensions_from_raw(
                        raw_reviews=raw_samples,
                        product_title=product_title,
                        bullet_points="\n".join(bullet_points) if bullet_points else ""
                    )
                    if dims:
                        for dim in dims:
                            dimension = ProductDimension(
                                product_id=product_id,
                                name=dim["name"],
                                description=dim.get("description", ""),
                                is_ai_generated=True
                            )
                            db.add(dimension)
                        db.commit()
                        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ å®Œæˆ: {len(dims)} ä¸ª")
                except Exception as e:
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ å¤±è´¥: {e}")
            
            # å­¦ä¹ 5Wæ ‡ç­¾
            label_count_result = db.execute(
                select(func.count(ProductContextLabel.id))
                .where(ProductContextLabel.product_id == product_id)
            )
            label_count = label_count_result.scalar() or 0
            
            if label_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
                try:
                    labels = translation_service.learn_context_labels_from_raw(
                        raw_reviews=raw_samples,
                        product_title=product_title,
                        bullet_points=bullet_points
                    )
                    if labels:
                        labels_saved = 0
                        for context_type in ["who", "where", "when", "why", "what"]:
                            type_labels = labels.get(context_type, [])
                            for item in type_labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                                    labels_saved += 1
                        db.commit()
                        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_saved} ä¸ª")
                except Exception as e:
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å¤±è´¥: {e}")
        else:
            logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] æ ·æœ¬ä¸è¶³ï¼ˆ{len(raw_samples)} æ¡ï¼‰ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # ==========================================
        # Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–
        # æ³¨æ„ï¼šç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²ç»å¯åŠ¨äº†ï¼ä¸éœ€è¦åœ¨è¿™é‡Œè§¦å‘
        # ==========================================
        update_task_progress(2, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 2/4: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–...")
        
        # æ£€æŸ¥å½“å‰ç¿»è¯‘è¿›åº¦ï¼ˆç¿»è¯‘åœ¨ ingest æ—¶å°±å·²ç»å¼€å§‹äº†ï¼‰
        pending_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_([
                        TranslationStatus.PENDING.value,
                        TranslationStatus.PROCESSING.value
                    ]),
                    Review.is_deleted == False
                )
            )
        )
        pending_translation = pending_result.scalar() or 0
        
        translated_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.COMPLETED.value,
                    Review.is_deleted == False
                )
            )
        )
        translated_count = translated_result.scalar() or 0
        
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å½“å‰ç¿»è¯‘çŠ¶æ€: å·²ç¿»è¯‘ {translated_count} æ¡, å¾…ç¿»è¯‘ {pending_translation} æ¡")
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ’¡ ç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²å¯åŠ¨ï¼Œç°åœ¨è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–")
        
        # è§¦å‘æ´å¯Ÿå’Œä¸»é¢˜æå–ï¼ˆå®ƒä»¬ä¼šå¤„ç†å·²ç¿»è¯‘çš„è¯„è®ºï¼Œè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
        task_extract_insights.delay(product_id)
        task_extract_themes.delay(product_id)
        
        # ==========================================
        # Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘ + æ´å¯Ÿ + ä¸»é¢˜ï¼‰
        # ==========================================
        update_task_progress(3, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 3/4: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘+æ´å¯Ÿ+ä¸»é¢˜ï¼‰...")
        
        # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰ 30 åˆ†é’Ÿï¼Œä» 15 åˆ†é’Ÿæå‡ï¼‰
        max_wait_seconds = 1800  # ğŸ”¥ ä» 900 ç§’ï¼ˆ15 åˆ†é’Ÿï¼‰æå‡åˆ° 1800 ç§’ï¼ˆ30 åˆ†é’Ÿï¼‰
        wait_interval = 15
        waited = 0
        last_log_time = 0
        
        while waited < max_wait_seconds:
            time.sleep(wait_interval)
            waited += wait_interval
            
            # æ£€æŸ¥ç¿»è¯‘è¿›åº¦
            pending_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status.in_([
                            TranslationStatus.PENDING.value,
                            TranslationStatus.PROCESSING.value
                        ]),
                        Review.is_deleted == False
                    )
                )
            )
            pending_translation = pending_result.scalar() or 0
            
            # æ£€æŸ¥å·²ç¿»è¯‘çš„è¯„è®ºæ•°
            translated_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False
                    )
                )
            )
            translated_count = translated_result.scalar() or 0
            
            # æ£€æŸ¥æ´å¯Ÿæå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–æ´å¯Ÿçš„è¯„è®ºï¼‰
            no_insight_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewInsight.review_id).where(ReviewInsight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_insights = no_insight_result.scalar() or 0
            
            # æ£€æŸ¥ä¸»é¢˜æå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–ä¸»é¢˜çš„è¯„è®ºï¼‰
            no_theme_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewThemeHighlight.review_id).where(ReviewThemeHighlight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_themes = no_theme_result.scalar() or 0
            
            # æ¯30ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            if waited - last_log_time >= 30:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å¹¶è¡Œè¿›åº¦ - å¾…ç¿»è¯‘:{pending_translation} | å¾…æ´å¯Ÿ:{pending_insights} | å¾…ä¸»é¢˜:{pending_themes}")
                last_log_time = waited
            
            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
            if pending_translation == 0 and pending_insights == 0 and pending_themes == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… å¹¶è¡Œå¤„ç†å…¨éƒ¨å®Œæˆï¼å·²ç¿»è¯‘:{translated_count}æ¡")
                break
            
            # [OPTIMIZED] æ¯ 120 ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œåªåœ¨è¿›åº¦åœæ»æ—¶é‡æ–°è§¦å‘
            # é¿å…é¢‘ç¹è§¦å‘å¯¼è‡´ä»»åŠ¡å †ç§¯ï¼Œå½±å“å…¶ä»–ç”¨æˆ·
            if waited % 120 == 0 and waited > 0:
                # ç¿»è¯‘ä»»åŠ¡ï¼šåªåœ¨æœ‰å¤§é‡å¾…ç¿»è¯‘æ—¶è§¦å‘
                if pending_translation > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ”„ é‡æ–°è§¦å‘ç¿»è¯‘ä»»åŠ¡ï¼ˆè¿˜æœ‰{pending_translation}æ¡å¾…å¤„ç†ï¼‰")
                    task_ingest_translation_only.delay(product_id)
                # æ´å¯Ÿ/ä¸»é¢˜ï¼šåªè§¦å‘ä¸€ä¸ªï¼Œé¿å…å ç”¨å¤ªå¤šèµ„æº
                if pending_insights > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘æ´å¯Ÿæå–ï¼ˆè¿˜æœ‰{pending_insights}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_insights.delay(product_id)
                elif pending_themes > 10:  # ç”¨ elif é¿å…åŒæ—¶è§¦å‘
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘ä¸»é¢˜æå–ï¼ˆè¿˜æœ‰{pending_themes}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_themes.delay(product_id)
            
            # æ›´æ–°å¿ƒè·³
            update_task_progress(3, TaskStatus.PROCESSING.value)
        
        if waited >= max_wait_seconds:
            # ğŸ”¥ ä¼˜åŒ–ï¼šæ”¾å®½å®Œæˆåº¦è¦æ±‚ï¼Œä» 95% é™åˆ° 85%
            # ç†ç”±ï¼š85% å·²è¶³å¤Ÿç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šï¼Œå‰©ä½™ä»»åŠ¡å¯å¼‚æ­¥ç»§ç»­
            if pending_insights > translated_count * 0.15 or pending_themes > translated_count * 0.15:
                logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] âš ï¸ ç­‰å¾…è¶…æ—¶ä¸”å®Œæˆåº¦ <85%ï¼ˆæ´å¯Ÿå¾…å¤„ç†:{pending_insights}, ä¸»é¢˜å¾…å¤„ç†:{pending_themes}ï¼‰")
                update_task_progress(3, TaskStatus.FAILED.value, f"å¤„ç†è¶…æ—¶ï¼Œæ´å¯Ÿå¾…å¤„ç†:{pending_insights}ï¼Œä¸»é¢˜å¾…å¤„ç†:{pending_themes}")
                return {
                    "success": False,
                    "product_id": product_id,
                    "task_id": task_id,
                    "error": f"å¹¶è¡Œå¤„ç†è¶…æ—¶ä¸”å®Œæˆåº¦ä¸è¶³85%ï¼Œè¯·ç¨åé‡è¯•ã€‚æ´å¯Ÿå¾…å¤„ç†:{pending_insights}ï¼Œä¸»é¢˜å¾…å¤„ç†:{pending_themes}"
                }
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] å¹¶è¡Œå¤„ç†ç­‰å¾…è¶…æ—¶ï¼Œä½†å®Œæˆåº¦è¾¾åˆ°85%ä»¥ä¸Šï¼Œç»§ç»­ç”ŸæˆæŠ¥å‘Šï¼ˆæ´å¯Ÿ:{translated_count - pending_insights}/{translated_count}, ä¸»é¢˜:{translated_count - pending_themes}/{translated_count}ï¼‰")
        
        # ==========================================
        # Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
        # ==========================================
        update_task_progress(4, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 4/4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨æŠ¥å‘Šç”Ÿæˆ
            # ç”±äº SummaryService æ˜¯å¼‚æ­¥çš„ï¼Œéœ€è¦ä½¿ç”¨ asyncio
            import asyncio
            from app.services.summary_service import SummaryService
            
            async def generate_report_async():
                # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥ï¼šengine å’Œ async_session_maker
                from app.db.session import async_session_maker
                
                async with async_session_maker() as async_db:
                    summary_service = SummaryService(async_db)
                    result = await summary_service.generate_report(
                        product_id=product_id,
                        report_type="comprehensive",  # ç»¼åˆæˆ˜ç•¥ç‰ˆ
                        min_reviews=10,
                        save_to_db=True,
                        force_regenerate=False,  # [NEW] ä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œæ£€æŸ¥å»é‡
                        require_full_completion=True  # [NEW] è¦æ±‚æ´å¯Ÿå’Œä¸»é¢˜100%å®Œæˆ
                    )
                    await async_db.commit()  # ç¡®ä¿æäº¤
                    return result
            
            # è¿è¡Œå¼‚æ­¥å‡½æ•°
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                report_result = loop.run_until_complete(generate_report_async())
            finally:
                loop.close()
            
            if report_result.get("success"):
                report_id = report_result.get("report_id")
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼ŒæŠ¥å‘ŠID: {report_id}")
                
                # æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œä¿å­˜æŠ¥å‘Š ID
                try:
                    db.execute(
                        update(Task)
                        .where(Task.id == task_id)
                        .values(error_message=f"report_id:{report_id}")  # ä¸´æ—¶å­˜å‚¨æŠ¥å‘ŠID
                    )
                    db.commit()
                except Exception as save_err:
                    logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ä¿å­˜æŠ¥å‘ŠIDå¤±è´¥: {save_err}")
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_result.get('error')}")
                
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            # ä¸å› æŠ¥å‘Šç”Ÿæˆå¤±è´¥è€Œä¸­æ–­æ•´ä¸ªä»»åŠ¡
        
        # ==========================================
        # å®Œæˆ
        # ==========================================
        update_task_progress(4, TaskStatus.COMPLETED.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… äº§å“ {product_id} å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ï¼ˆæµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆï¼‰")
        
        return {
            "success": True,
            "product_id": product_id,
            "task_id": task_id,
            "message": "å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰"
        }
        
    except Exception as e:
        logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        update_task_progress(0, TaskStatus.FAILED.value, str(e))
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡8: å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ ==============

@celery_app.task(bind=True, max_retries=0)
def task_check_pending_translations(self):
    """
    ğŸ”„ å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ (Periodic Translation Check)
    
    æ¯ 15 ç§’ç”± Celery Beat è§¦å‘ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…ç¿»è¯‘çš„äº§å“ã€‚
    å¦‚æœæœ‰ï¼Œä¸ºæ¯ä¸ªäº§å“è§¦å‘ 3 ä¸ªå¹¶è¡Œç¿»è¯‘ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤š Worker å¹¶å‘ã€‚
    
    è®¾è®¡ç†å¿µï¼š
    - ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨è¡Œçº§é”ï¼ˆSKIP LOCKEDï¼‰ï¼Œå¤šä»»åŠ¡å¯ä»¥å®‰å…¨å¹¶å‘
    - è§¦å‘å¤šä¸ªä»»åŠ¡è®© 6 ä¸ª Worker çº¿ç¨‹éƒ½æœ‰æ´»å¹²
    - é¿å…ç¿»è¯‘å› è¡Œçº§é”ç«äº‰è€Œæå‰ç»“æŸ
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    
    db = get_sync_db()
    
    try:
        # æŸ¥æ‰¾æœ‰å¾…ç¿»è¯‘è¯„è®ºçš„äº§å“ï¼ˆæœ€å¤šå¤„ç† 5 ä¸ªäº§å“ï¼‰
        products_with_pending = db.execute(
            select(Product.id, func.count(Review.id).label("pending_count"))
            .join(Review, Review.product_id == Product.id)
            .where(
                and_(
                    Review.translation_status.in_([
                        TranslationStatus.PENDING.value,
                        TranslationStatus.FAILED.value
                    ]),
                    Review.is_deleted == False
                )
            )
            .group_by(Product.id)
            .having(func.count(Review.id) > 0)
            .order_by(func.count(Review.id).desc())
            .limit(5)
        )
        
        pending_products = products_with_pending.all()
        
        if not pending_products:
            return {"triggered": 0, "message": "No pending translations"}
        
        triggered = 0
        for product_id, pending_count in pending_products:
            # ğŸ”¥ ä¸ºæ¯ä¸ªäº§å“è§¦å‘å¤šä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆå……åˆ†åˆ©ç”¨å¹¶å‘ï¼‰
            # æ ¹æ®å¾…ç¿»è¯‘æ•°é‡å†³å®šè§¦å‘å‡ ä¸ªä»»åŠ¡
            num_tasks = min(3, max(1, pending_count // 20))  # æ¯ 20 æ¡è§¦å‘ 1 ä¸ªä»»åŠ¡ï¼Œæœ€å¤š 3 ä¸ª
            
            for _ in range(num_tasks):
                task_ingest_translation_only.delay(str(product_id))
                triggered += 1
            
            logger.info(f"[ç¿»è¯‘è°ƒåº¦] äº§å“ {product_id} å¾…ç¿»è¯‘ {pending_count} æ¡ï¼Œè§¦å‘ {num_tasks} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        return {
            "triggered": triggered,
            "products": len(pending_products),
            "message": f"Triggered {triggered} translation tasks for {len(pending_products)} products"
        }
        
    except Exception as e:
        logger.error(f"[ç¿»è¯‘è°ƒåº¦] æ£€æŸ¥å¤±è´¥: {e}")
        return {"triggered": 0, "error": str(e)}
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡9: é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=10)
def task_process_ingestion_queue(self):
    """
    ğŸš€ é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ä»»åŠ¡ (Ingestion Queue Consumer)
    
    ä» Redis é˜Ÿåˆ—æ‰¹é‡æ¶ˆè´¹è¯„è®ºæ•°æ®ï¼Œå…¥åº“åˆ° PostgreSQLã€‚
    
    è®¾è®¡ç‰¹ç‚¹ï¼š
    1. é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–ï¼šAPI å±‚åªå†™ Redisï¼Œæœ¬ä»»åŠ¡æ‰¹é‡å…¥åº“
    2. ä¸‰å±‚å»é‡ï¼šRedis Set â†’ å†…å­˜ Set â†’ DB ON CONFLICT
    3. æŒ‰ ASIN åˆ†ç»„å¤„ç†ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
    4. å…¥åº“æˆåŠŸåè§¦å‘ç¿»è¯‘ä»»åŠ¡
    
    è°ƒåº¦æ–¹å¼ï¼š
    - Celery Beat æ¯ 5 ç§’è§¦å‘ä¸€æ¬¡
    - æ¯æ¬¡æœ€å¤šå¤„ç† 100 æ¡é˜Ÿåˆ—æ•°æ®
    
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    from app.core.redis import ReviewIngestionQueueSync, get_sync_redis
    from app.services.ingestion_service import IngestionService
    
    logger.debug("[Ingestion] å¼€å§‹æ¶ˆè´¹é˜Ÿåˆ—...")
    
    redis_cli = get_sync_redis()
    queue = ReviewIngestionQueueSync(redis_cli)
    
    # Step 1: ä»é˜Ÿåˆ—æ‰¹é‡å–å‡ºæ•°æ®
    items = queue.pop_batch(count=100)
    
    if not items:
        logger.debug("[Ingestion] é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡")
        return {"processed": 0, "items": 0}
    
    logger.info(f"[Ingestion] ä»é˜Ÿåˆ—å–å‡º {len(items)} æ¡æ•°æ®")
    
    db = get_sync_db()
    
    try:
        # Step 2: è°ƒç”¨å…¥åº“æœåŠ¡å¤„ç†
        service = IngestionService(db, redis_cli)
        results = service.process_queue_items(items)
        
        # Step 3: ç»Ÿè®¡ç»“æœ
        total_inserted = sum(r.get("inserted", 0) for r in results.values())
        total_skipped = sum(r.get("skipped", 0) for r in results.values())
        
        logger.info(
            f"[Ingestion] å¤„ç†å®Œæˆ: {len(results)} ä¸ªäº§å“, "
            f"æ–°å¢ {total_inserted} æ¡, è·³è¿‡ {total_skipped} æ¡"
        )
        
        # Step 4: ä¸ºæœ‰æ–°æ•°æ®çš„äº§å“è§¦å‘ç¿»è¯‘
        for asin, result in results.items():
            if result.get("inserted", 0) > 0:
                # è·å– product_id
                from app.models.product import Product
                product_result = db.execute(
                    select(Product).where(Product.asin == asin)
                )
                product = product_result.scalar_one_or_none()
                
                if product:
                    # è§¦å‘æµå¼ç¿»è¯‘
                    task_ingest_translation_only.delay(str(product.id))
                    logger.info(f"[Ingestion] äº§å“ {asin} å·²è§¦å‘ç¿»è¯‘ä»»åŠ¡")
        
        return {
            "processed": len(items),
            "products": len(results),
            "inserted": total_inserted,
            "skipped": total_skipped,
            "details": results
        }
        
    except Exception as e:
        logger.error(f"[Ingestion] å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] è¾…åŠ©å‡½æ•°ï¼šåŒæ­¥å·²æœ‰ review_id åˆ° Redis ==============

@celery_app.task
def task_sync_product_reviews_to_redis(asin: str):
    """
    å°†äº§å“çš„å·²æœ‰ review_id åŒæ­¥åˆ° Redis
    
    ç”¨äºï¼š
    1. Redis é‡å¯åæ¢å¤å»é‡æ•°æ®
    2. æ‰‹åŠ¨è§¦å‘åŒæ­¥
    
    Args:
        asin: äº§å“ ASIN
    """
    from app.services.ingestion_service import IngestionService
    
    db = get_sync_db()
    
    try:
        service = IngestionService(db)
        service.sync_redis_from_db(asin)
        logger.info(f"[Sync] äº§å“ {asin} çš„ review_id å·²åŒæ­¥åˆ° Redis")
        return {"success": True, "asin": asin}
    except Exception as e:
        logger.error(f"[Sync] åŒæ­¥å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()
