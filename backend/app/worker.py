"""
Celery Worker Configuration and Tasks

This module handles asynchronous processing of reviews:
1. Translation via Qwen API
2. Sentiment analysis
3. Database updates
"""
import logging
import time
import random
from typing import Optional
from functools import wraps
from uuid import UUID

from celery import Celery
from sqlalchemy import create_engine, select, update, and_, func
from sqlalchemy.orm import sessionmaker
import redis
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import settings

logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ¯ æ™ºèƒ½è¯„è®ºåˆ†ç±»å™¨ï¼ˆæ ¹æ®é•¿åº¦å’Œè´¨é‡å·®å¼‚åŒ–å¤„ç†ï¼‰
# ============================================================================

class ReviewClassifier:
    """
    è¯„è®ºåˆ†ç±»å™¨ï¼šæ ¹æ®è¯„è®ºé•¿åº¦å’Œè´¨é‡åˆ†ç±»ï¼Œé‡‡ç”¨å·®å¼‚åŒ–ç¿»è¯‘ç­–ç•¥
    
    åˆ†ç±»æ ‡å‡†ï¼š
    - VIP è¯„è®ºï¼šé•¿è¯„è®ºï¼ˆ> 200å­—ï¼‰æˆ–æç«¯æ˜Ÿçº§çš„è¯¦ç»†è¯„è®ºï¼ˆ1/5æ˜Ÿ ä¸” > 100å­—ï¼‰
      â†’ å•ç‹¬ç¿»è¯‘ï¼Œä¿è¯è´¨é‡
    
    - æ ‡å‡†è¯„è®ºï¼šä¸­ç­‰é•¿åº¦ï¼ˆ50-200å­—ï¼‰
      â†’ 5æ¡ä¸€æ‰¹ç¿»è¯‘ï¼Œå¹³è¡¡è´¨é‡å’Œæ•ˆç‡
    
    - çŸ­è¯„è®ºï¼šç®€çŸ­è¡¨è¾¾ï¼ˆâ‰¤ 50å­—ï¼‰
      â†’ 20æ¡ä¸€æ‰¹ç¿»è¯‘ï¼Œæœ€å¤§åŒ–æ•ˆç‡
    
    ä¼˜åŠ¿ï¼š
    - è´¨é‡ä¿è¯ï¼šé‡è¦è¯„è®ºå•ç‹¬ç¿»è¯‘ï¼Œä¸é™ä½è´¨é‡
    - æ•ˆç‡æœ€å¤§åŒ–ï¼šçŸ­è¯„è®ºå¤§æ‰¹é‡å¤„ç†ï¼ŒQPS æ¶ˆè€—é™ä½ 20 å€
    - çµæ´»å¹³è¡¡ï¼šä¸­ç­‰è¯„è®ºé€‚åº¦æ‰¹é‡ï¼Œå…¼é¡¾è´¨é‡å’Œæ•ˆç‡
    """
    
    # å¯é…ç½®çš„åˆ†ç±»é˜ˆå€¼
    VIP_LENGTH_THRESHOLD = 200        # VIP è¯„è®ºæœ€ä½å­—æ•°
    VIP_EXTREME_RATING_LENGTH = 100   # æç«¯æ˜Ÿçº§è¯„è®ºçš„å­—æ•°é˜ˆå€¼
    EXTREME_RATINGS = [1, 5]          # æç«¯æ˜Ÿçº§ï¼ˆå·®è¯„/å¥½è¯„ï¼‰
    
    STANDARD_MIN_LENGTH = 50          # æ ‡å‡†è¯„è®ºæœ€ä½å­—æ•°
    STANDARD_MAX_LENGTH = 200         # æ ‡å‡†è¯„è®ºæœ€é«˜å­—æ•°
    
    SHORT_MAX_LENGTH = 50             # çŸ­è¯„è®ºæœ€é«˜å­—æ•°
    
    # æ‰¹é‡å¤§å°é…ç½®
    BATCH_SIZE_VIP = 1       # VIP è¯„è®ºï¼šå•ç‹¬ç¿»è¯‘
    BATCH_SIZE_STANDARD = 5  # æ ‡å‡†è¯„è®ºï¼š5 æ¡ä¸€æ‰¹
    BATCH_SIZE_SHORT = 20    # çŸ­è¯„è®ºï¼š20 æ¡ä¸€æ‰¹
    
    @classmethod
    def classify(cls, review) -> str:
        """
        è¯„è®ºåˆ†ç±»
        
        Args:
            review: Review å¯¹è±¡
        
        Returns:
            'vip': é«˜è´¨é‡é•¿è¯„è®º
            'standard': ä¸­ç­‰è¯„è®º
            'short': çŸ­è¯„è®º
        """
        text = review.body_original or ""
        text_length = len(text.strip())
        rating = review.rating
        
        # VIP è¯„è®ºï¼šé•¿è¯„è®ºæˆ–æç«¯æ˜Ÿçº§çš„è¯¦ç»†è¯„è®º
        if text_length > cls.VIP_LENGTH_THRESHOLD:
            return 'vip'
        
        if text_length > cls.VIP_EXTREME_RATING_LENGTH and rating in cls.EXTREME_RATINGS:
            return 'vip'
        
        # çŸ­è¯„è®º
        if text_length <= cls.SHORT_MAX_LENGTH:
            return 'short'
        
        # æ ‡å‡†è¯„è®ºï¼ˆé»˜è®¤ï¼‰
        return 'standard'
    
    @classmethod
    def get_batch_size(cls, category: str) -> int:
        """è·å–æ‰¹é‡å¤§å°"""
        batch_sizes = {
            'vip': cls.BATCH_SIZE_VIP,
            'standard': cls.BATCH_SIZE_STANDARD,
            'short': cls.BATCH_SIZE_SHORT
        }
        return batch_sizes.get(category, cls.BATCH_SIZE_STANDARD)
    
    @classmethod
    def group_reviews(cls, reviews: list) -> dict:
        """
        å°†è¯„è®ºæŒ‰åˆ†ç±»åˆ†ç»„
        
        Returns:
            {
                'vip': [...],
                'standard': [...],
                'short': [...]
            }
        """
        groups = {
            'vip': [],
            'standard': [],
            'short': []
        }
        
        for review in reviews:
            category = cls.classify(review)
            groups[category].append(review)
        
        return groups


# ============================================================================
# ğŸš¦ å…¨å±€ API é™æµå™¨ï¼ˆé˜²æ­¢ QPS å†²é«˜å¯¼è‡´è´¦å·è¢«å°ï¼‰
# ============================================================================

class APIRateLimiter:
    """
    å…¨å±€ API é™æµå™¨ï¼Œé˜²æ­¢ç¬é—´ RPS å†²é«˜
    
    ç­–ç•¥ï¼š
    - ä½¿ç”¨ Redis æ»‘åŠ¨çª—å£è®¡æ•°
    - qwen-plus-latest: 40,000 RPM = 666 RPS
    - å®‰å…¨ä¸Šé™: 666 * 0.75 = 500 RPSï¼ˆç•™ 25% ä½™é‡ï¼‰
    - æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆå¤šæœåŠ¡å™¨å…±äº« Redis é™æµï¼‰
    - è¶…è¿‡é™åˆ¶æ—¶ï¼Œéšæœºé€€é¿ 0.05-0.2 ç§’
    """
    def __init__(self, redis_client, max_qps=200, window_seconds=1):
        self.redis_client = redis_client
        self.max_qps = max_qps
        self.window_seconds = window_seconds
        self.key_prefix = "api_rate_limit"
    
    def acquire(self, api_name="qwen"):
        """
        è·å– API è°ƒç”¨è®¸å¯
        
        Returns:
            bool: True if allowed, False if rate limited
        """
        key = f"{self.key_prefix}:{api_name}"
        current_time = time.time()
        window_start = current_time - self.window_seconds
        
        # æ¸…ç†è¿‡æœŸè®¡æ•°
        self.redis_client.zremrangebyscore(key, 0, window_start)
        
        # æ£€æŸ¥å½“å‰çª—å£å†…çš„è¯·æ±‚æ•°
        current_count = self.redis_client.zcard(key)
        
        if current_count >= self.max_qps:
            # è¶…è¿‡é™åˆ¶ï¼Œéšæœºé€€é¿
            backoff = random.uniform(0.1, 0.5)
            logger.warning(f"[é™æµ] API QPS è¾¾åˆ° {current_count}/{self.max_qps}ï¼Œé€€é¿ {backoff:.2f}s")
            time.sleep(backoff)
            return False
        
        # è®°å½•æœ¬æ¬¡è¯·æ±‚
        self.redis_client.zadd(key, {str(current_time): current_time})
        self.redis_client.expire(key, self.window_seconds * 2)  # 2 å€çª—å£æ—¶é—´è¿‡æœŸ
        
        return True
    
    def wait_and_acquire(self, api_name="qwen", max_retries=10):
        """
        ç­‰å¾…ç›´åˆ°è·å–åˆ° API è°ƒç”¨è®¸å¯
        
        Args:
            api_name: API åç§°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        for i in range(max_retries):
            if self.acquire(api_name):
                return True
            time.sleep(random.uniform(0.05, 0.2))  # çŸ­æš‚éšæœºé€€é¿
        
        raise Exception(f"[é™æµ] æ— æ³•è·å– API è®¸å¯ï¼Œå·²é‡è¯• {max_retries} æ¬¡")

# Redis å®¢æˆ·ç«¯ï¼ˆç”¨äºåˆ†å¸ƒå¼é”å’Œé™æµï¼‰
redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

# å…¨å±€é™æµå™¨å®ä¾‹
# qwen-plus-latest: 40,000 RPM = 666 RPSï¼Œå®‰å…¨ä¸Šé™ 500 RPSï¼ˆç•™ 25% ä½™é‡ï¼‰
import os
MAX_API_RPS = int(os.environ.get('MAX_API_RPS', '500'))
api_limiter = APIRateLimiter(redis_client, max_qps=MAX_API_RPS)

def rate_limited_api(api_name="qwen"):
    """
    API é™æµè£…é¥°å™¨
    
    ç”¨æ³•ï¼š
        @rate_limited_api("qwen")
        def call_qwen_api():
            ...
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç­‰å¾…è·å– API è®¸å¯
            api_limiter.wait_and_acquire(api_name)
            
            # è°ƒç”¨åŸå‡½æ•°
            return func(*args, **kwargs)
        
        return wrapper
    return decorator


# ============================================================================
# ğŸ”¥ æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹æŸ¥è¯¢ PostgreSQLï¼‰
# ============================================================================

class LabelCacheManager:
    """
    æ ‡ç­¾æ˜ å°„ Redis ç¼“å­˜ç®¡ç†å™¨
    
    ä¼˜åŒ–ç‚¹ï¼š
    - å°†çƒ­é—¨äº§å“çš„æ ‡ç­¾åº“å¸¸é©» Redis
    - é¿å… Worker æ¯æ¬¡æå–ä¸»é¢˜éƒ½æŸ¥è¯¢æ ‡ç­¾è¡¨
    - ç¼“å­˜æœ‰æ•ˆæœŸ 1 å°æ—¶ï¼ˆæ ‡ç­¾åº“å˜åŒ–ä¸é¢‘ç¹ï¼‰
    """
    CACHE_PREFIX = "label_cache"
    CACHE_TTL = 3600  # 1 å°æ—¶
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
    
    def get_label_id_map(self, product_id: str) -> dict:
        """
        ä»ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨
        
        Returns:
            dict: {(theme_type, label_name): label_id} æˆ– Noneï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        """
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        cached_data = self.redis_client.get(cache_key)
        
        if cached_data:
            try:
                import json
                data = json.loads(cached_data)
                # é‡å»º tuple key
                return {(k.split("|")[0], k.split("|")[1]): v for k, v in data.items()}
            except Exception as e:
                logger.warning(f"[æ ‡ç­¾ç¼“å­˜] è§£æç¼“å­˜å¤±è´¥: {e}")
                return None
        
        return None
    
    def set_label_id_map(self, product_id: str, label_id_map: dict):
        """
        å°†æ ‡ç­¾æ˜ å°„è¡¨å­˜å…¥ç¼“å­˜
        
        Args:
            product_id: äº§å“ ID
            label_id_map: {(theme_type, label_name): label_id}
        """
        if not label_id_map:
            return
        
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        
        try:
            import json
            # å°† tuple key è½¬æ¢ä¸ºå­—ç¬¦ä¸² key
            data = {f"{k[0]}|{k[1]}": str(v) for k, v in label_id_map.items()}
            self.redis_client.setex(cache_key, self.CACHE_TTL, json.dumps(data))
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾ï¼ˆäº§å“: {product_id}ï¼‰")
        except Exception as e:
            logger.warning(f"[æ ‡ç­¾ç¼“å­˜] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
    
    def invalidate(self, product_id: str):
        """ä½¿ç¼“å­˜å¤±æ•ˆï¼ˆæ ‡ç­¾åº“æ›´æ–°æ—¶è°ƒç”¨ï¼‰"""
        cache_key = f"{self.CACHE_PREFIX}:{product_id}"
        self.redis_client.delete(cache_key)
        logger.info(f"[æ ‡ç­¾ç¼“å­˜] å·²æ¸…é™¤ç¼“å­˜ï¼ˆäº§å“: {product_id}ï¼‰")

# å…¨å±€æ ‡ç­¾ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
label_cache = LabelCacheManager(redis_client)

# Create Celery application
celery_app = Celery(
    "voc_worker",
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
)

# Celery configuration
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True,
    task_time_limit=1800,  # 30 minutes timeout per task (increased from 600s to handle large batches)
    task_soft_time_limit=1500,  # 25 minutes soft limit (warning before hard kill)
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    worker_send_task_events=True,  # ğŸ”¥ æ”¯æŒ Flower ç›‘æ§
    # ============================================================================
    # ğŸš€ 6 é˜Ÿåˆ— + 5 Worker èŒèƒ½åŒ–æ¶æ„ï¼ˆ4æ ¸16Gï¼Œè§£å†³ç¿»è¯‘é˜»å¡åˆ†æé—®é¢˜ï¼‰
    # ============================================================================
    #
    # ğŸ¯ æ ¸å¿ƒä¼˜åŒ–ï¼šç‰©ç†éš”ç¦»é˜Ÿåˆ— + èŒèƒ½åŒ– Worker åˆ†å·¥
    #
    # 6 ä¸ªç‹¬ç«‹é˜Ÿåˆ—ï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ 1. ingestion          - å…¥åº“ï¼ˆç§’å›ï¼‰                                    â”‚
    # â”‚ 2. learning           - å»ºæ¨¡ï¼ˆVIP å¿«è½¦é“ï¼‰                              â”‚
    # â”‚ 3. translation        - ç¿»è¯‘ï¼ˆç‹¬ç«‹é˜Ÿåˆ—ï¼Œä¸é˜»å¡åˆ†æï¼‰                     â”‚
    # â”‚ 4. insight_extraction - æ´å¯Ÿæå–ï¼ˆä¸“å±é˜Ÿåˆ—ï¼‰                            â”‚
    # â”‚ 5. theme_extraction   - ä¸»é¢˜æå–ï¼ˆä¸“å±é˜Ÿåˆ—ï¼‰                            â”‚
    # â”‚ 6. reports            - æŠ¥å‘Šç”Ÿæˆ                                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # 5 ä¸ªèŒèƒ½åŒ– Workerï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚ Worker 1 (Base):    ingestion, reports       | Prefork, 4 çº¿ç¨‹         â”‚
    # â”‚   â†’ æ­»å®ˆå…¥åº“ï¼Œä¸æ¥ AI æ´»ï¼Œç¡®ä¿æ’ä»¶ä¸Šä¼ ç§’å›                              â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 2 (VIP):     learning                 | Gevent, 50 åç¨‹         â”‚
    # â”‚   â†’ å»ºæ¨¡å¿«è½¦é“ï¼Œä¸“æ”»ç»´åº¦å­¦ä¹ å’Œ 5W å»ºæ¨¡                                  â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 3 (Trans):   translation              | Gevent, 100 åç¨‹        â”‚
    # â”‚   â†’ ç‹¬ç«‹ç¿»è¯‘ç»„ï¼Œä¸“é—¨æ¶ˆåŒ–æµ·é‡ç¿»è¯‘ï¼Œä¸å½±å“åˆ†æ                            â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 4 (Insight): insight_extraction, learning | Gevent, 100 åç¨‹    â”‚
    # â”‚   â†’ æ´å¯Ÿä¸“å‘˜ï¼Œä¸»æ”»æ´å¯Ÿæå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡                                â”‚
    # â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    # â”‚ Worker 5 (Theme):   theme_extraction, learning   | Gevent, 100 åç¨‹    â”‚
    # â”‚   â†’ ä¸»é¢˜ä¸“å‘˜ï¼Œä¸»æ”»ä¸»é¢˜æå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡                                â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    #
    # æ€»å¹¶å‘ï¼š4 + 50 + 100 + 100 + 100 = 354 å¹¶å‘ï¼
    #
    # ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿ï¼š
    # - ç¿»è¯‘ä¸å†æ˜¯å±éšœï¼šç‹¬ç«‹ Workerï¼Œä¸é˜»å¡åˆ†æ
    # - å»ºæ¨¡æ°¸è¿œä¼˜å…ˆï¼šæ‰€æœ‰ AI Worker éƒ½æ”¯æ´ learning
    # - æ´å¯Ÿ/ä¸»é¢˜å¹¶è¡Œï¼šå„æœ‰ä¸“å±é˜Ÿåˆ—ï¼Œä¸äº’ç›¸ç«äº‰
    #
    task_routes={
        # ============== 1. å¿«è½¦é“ï¼šå…¥åº“ (worker-base) ==============
        # ğŸï¸ çº¯ CPU + ç£ç›˜ï¼Œä¿è¯ API ç§’çº§å“åº”
        "app.worker.task_process_ingestion_queue": {"queue": "ingestion"},
        "app.worker.task_check_pending_translations": {"queue": "ingestion"},
        
        # ============== 2. VIP å¿«è½¦é“ï¼šå­¦ä¹ å»ºæ¨¡ (worker-vip) ==============
        # ğŸŒŸ æ–°äº§å“ç§’çº§å»ºæ¨¡ï¼Œç‹¬ç«‹è¿›ç¨‹ä¸å—å¹²æ‰°
        "app.worker.task_full_auto_analysis": {"queue": "learning"},
        "app.worker.task_scientific_learning_and_analysis": {"queue": "learning"},
        
        # ============== 3. ç‹¬ç«‹ï¼šç¿»è¯‘ (worker-trans) ==============
        # ğŸ”„ ä¸“é—¨æ¶ˆåŒ–æµ·é‡ç¿»è¯‘ï¼Œä¸å½±å“å…¶ä»–åˆ†æä»»åŠ¡
        "app.worker.task_translate_bullet_points": {"queue": "translation"},
        "app.worker.task_process_reviews": {"queue": "translation"},
        "app.worker.task_ingest_translation_only": {"queue": "translation"},
        
        # ============== 4. ä¸“å±ï¼šæ´å¯Ÿæå– (worker-insight) ==============
        # ğŸ” ä¸»æ”»æ´å¯Ÿæå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡
        "app.worker.task_extract_insights": {"queue": "insight_extraction"},
        
        # ============== 5. ä¸“å±ï¼šä¸»é¢˜æå– (worker-theme) ==============
        # ğŸ·ï¸ ä¸»æ”»ä¸»é¢˜æå–ï¼Œé—²æ—¶æ”¯æ´å»ºæ¨¡
        "app.worker.task_extract_themes": {"queue": "theme_extraction"},
        
        # ============== 5.5. ç»´åº¦æ€»ç»“ç”Ÿæˆ (worker-insight/vip) ==============
        # ğŸ“Š AI æ€»ç»“ç”Ÿæˆï¼Œéœ€è¦å¤§é‡ AI è°ƒç”¨
        "app.worker.task_generate_dimension_summaries": {"queue": "learning"},
        
        # ============== 6. ç»„è£…ï¼šæŠ¥å‘Šç”Ÿæˆ (worker-base) ==============
        # ğŸ“Š æœ€åçš„æ•´åˆï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
        "app.worker.task_generate_report": {"queue": "reports"},
    },
    # Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®
    beat_schedule={
        # æ¯ 5 ç§’æ¶ˆè´¹ä¸€æ¬¡å…¥åº“é˜Ÿåˆ—
        "process-ingestion-queue": {
            "task": "app.worker.task_process_ingestion_queue",
            "schedule": 5.0,
        },
        # ğŸ”¥ æ¯ 15 ç§’æ£€æŸ¥å¹¶è§¦å‘å¾…ç¿»è¯‘ä»»åŠ¡ï¼ˆç¡®ä¿ç¿»è¯‘æŒç»­è¿›è¡Œï¼‰
        "check-pending-translations": {
            "task": "app.worker.task_check_pending_translations",
            "schedule": 15.0,
        },
        # ğŸ›¡ï¸ æ¯ 5 åˆ†é’Ÿè¿è¡Œè¡¥å…¨å·¡æ£€ï¼ˆæœ€åä¸€é“é˜²çº¿ï¼Œç¡®ä¿æ— é—æ¼ï¼‰
        "analysis-completion-patrol": {
            "task": "app.worker.task_analysis_completion_patrol",
            "schedule": 300.0,  # 5 åˆ†é’Ÿ
        },
    },
)

# ============================================================================
# ğŸ”§ åŒæ­¥æ•°æ®åº“è¿æ¥ï¼ˆCelery Worker ä¸“ç”¨ï¼‰
# ============================================================================
# Celery ä½¿ç”¨ Gevent åç¨‹ï¼Œéœ€è¦ç‰¹æ®Šçš„è¿æ¥æ± é…ç½®
# 
# ç­–ç•¥è¯´æ˜ï¼š
# - ä½¿ç”¨ NullPoolï¼šæ¯æ¬¡æ“ä½œåˆ›å»ºæ–°è¿æ¥ï¼Œé€‚åˆé«˜å¹¶å‘åç¨‹åœºæ™¯
# - é¿å…è¿æ¥æ± ç“¶é¢ˆï¼šGevent 150 åç¨‹ vs é»˜è®¤ pool_size=5 ä¼šä¸¥é‡é˜»å¡
# - PostgreSQL max_connections=500 è¶³ä»¥æ”¯æ’‘
# ============================================================================
from sqlalchemy.pool import NullPool, QueuePool

SYNC_DATABASE_URL = settings.DATABASE_URL.replace("+asyncpg", "")

# ğŸ”¥ é«˜å¹¶å‘è¿æ¥æ± é…ç½®ï¼ˆæ”¯æŒ 400+ å¹¶å‘ Workerï¼‰
sync_engine = create_engine(
    SYNC_DATABASE_URL,
    echo=settings.DEBUG,
    # ä½¿ç”¨ QueuePool é…åˆå¤§å®¹é‡ï¼Œæ¯” NullPool æ›´é«˜æ•ˆ
    poolclass=QueuePool,
    pool_size=100,        # åŸºç¡€è¿æ¥æ•°
    max_overflow=400,     # æº¢å‡ºè¿æ¥æ•°ï¼ˆæ€»å…±æ”¯æŒ 500 è¿æ¥ï¼‰
    pool_timeout=30,      # ç­‰å¾…è¿æ¥è¶…æ—¶
    pool_pre_ping=True,   # æ£€æµ‹æ–­å¼€çš„è¿æ¥
    pool_recycle=1800,    # 30 åˆ†é’Ÿå›æ”¶è¿æ¥ï¼Œé˜²æ­¢æ•°æ®åº“è¶…æ—¶
)
SyncSession = sessionmaker(bind=sync_engine)


def get_sync_db():
    """Get synchronous database session for worker."""
    return SyncSession()


# ============== Worker å¯åŠ¨æ—¶æ¸…ç†å¡ä½çš„ä»»åŠ¡ ==============

def cleanup_stuck_reviews():
    """
    æ¸…ç†å¡åœ¨ 'processing' çŠ¶æ€çš„è¯„è®ºã€‚
    å½“ Worker é‡å¯æ—¶ï¼Œä¹‹å‰æ­£åœ¨å¤„ç†çš„è¯„è®ºå¯èƒ½ä¼šå¡åœ¨ processing çŠ¶æ€ã€‚
    è¿™ä¸ªå‡½æ•°å°†å®ƒä»¬é‡ç½®ä¸º pendingï¼Œè®©å®ƒä»¬å¯ä»¥è¢«é‡æ–°å¤„ç†ã€‚
    """
    from app.models.review import Review
    
    db = get_sync_db()
    try:
        result = db.execute(
            update(Review)
            .where(Review.translation_status == "processing")
            .values(translation_status="pending")
        )
        db.commit()
        
        if result.rowcount > 0:
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {result.rowcount} æ¡å¡ä½çš„è¯„è®ºé‡ç½®ä¸º pending çŠ¶æ€")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¡ä½çš„è¯„è®º")
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†å¡ä½è¯„è®ºå¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


def cleanup_stuck_tasks():
    """
    æ¸…ç†å¡ä½çš„ä»»åŠ¡ï¼ˆå¿ƒè·³è¶…æ—¶ï¼‰ã€‚
    å°† PROCESSING çŠ¶æ€ä½†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡æ ‡è®°ä¸º TIMEOUTã€‚
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone, timedelta
    
    db = get_sync_db()
    try:
        # æŸ¥æ‰¾æ‰€æœ‰ processing çŠ¶æ€çš„ä»»åŠ¡
        result = db.execute(
            select(Task).where(Task.status == TaskStatus.PROCESSING.value)
        )
        tasks = result.scalars().all()
        
        timeout_count = 0
        for task in tasks:
            if task.is_heartbeat_timeout:
                task.status = TaskStatus.TIMEOUT.value
                task.error_message = f"å¿ƒè·³è¶…æ—¶ï¼šæœ€åå¿ƒè·³æ—¶é—´ {task.last_heartbeat}"
                timeout_count += 1
                logger.warning(f"[å¯åŠ¨æ¸…ç†] ä»»åŠ¡ {task.id} ({task.task_type}) å¿ƒè·³è¶…æ—¶ï¼Œæ ‡è®°ä¸º TIMEOUT")
        
        if timeout_count > 0:
            db.commit()
            logger.warning(f"[å¯åŠ¨æ¸…ç†] å·²å°† {timeout_count} ä¸ªè¶…æ—¶ä»»åŠ¡æ ‡è®°ä¸º TIMEOUT")
        else:
            logger.info("[å¯åŠ¨æ¸…ç†] æ²¡æœ‰å‘ç°å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡")
            
    except Exception as e:
        logger.error(f"[å¯åŠ¨æ¸…ç†] æ¸…ç†è¶…æ—¶ä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()


# ============== å¿ƒè·³æ›´æ–°è¾…åŠ©å‡½æ•° ==============

def update_task_heartbeat(db, task_id: str, processed_items: int = None):
    """
    æ›´æ–°ä»»åŠ¡å¿ƒè·³æ—¶é—´ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        processed_items: å¯é€‰ï¼ŒåŒæ—¶æ›´æ–°å·²å¤„ç†æ•°é‡
    """
    from app.models.task import Task
    from datetime import datetime, timezone
    
    try:
        values = {"last_heartbeat": datetime.now(timezone.utc)}
        if processed_items is not None:
            values["processed_items"] = processed_items
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡å¿ƒè·³å¤±è´¥: {e}")
        db.rollback()


def get_or_create_task(db, product_id: str, task_type: str, total_items: int = 0, celery_task_id: str = None):
    """
    è·å–æˆ–åˆ›å»ºä»»åŠ¡è®°å½•ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        product_id: äº§å“ ID
        task_type: ä»»åŠ¡ç±»å‹
        total_items: æ€»é¡¹ç›®æ•°
        celery_task_id: Celery ä»»åŠ¡ ID
        
    Returns:
        Task: ä»»åŠ¡å¯¹è±¡
    """
    from app.models.task import Task, TaskStatus
    from datetime import datetime, timezone
    
    # æŸ¥æ‰¾ç°æœ‰ä»»åŠ¡
    result = db.execute(
        select(Task).where(
            and_(
                Task.product_id == product_id,
                Task.task_type == task_type
            )
        )
    )
    task = result.scalar_one_or_none()
    
    now = datetime.now(timezone.utc)
    
    if task:
        # æ›´æ–°ç°æœ‰ä»»åŠ¡
        task.status = TaskStatus.PROCESSING.value
        task.total_items = total_items
        task.processed_items = 0
        task.last_heartbeat = now
        task.celery_task_id = celery_task_id
        task.error_message = None
    else:
        # åˆ›å»ºæ–°ä»»åŠ¡
        task = Task(
            product_id=product_id,
            task_type=task_type,
            status=TaskStatus.PROCESSING.value,
            total_items=total_items,
            processed_items=0,
            last_heartbeat=now,
            celery_task_id=celery_task_id
        )
        db.add(task)
    
    db.commit()
    db.refresh(task)
    return task


def complete_task(db, task_id: str, success: bool = True, error_message: str = None):
    """
    å®Œæˆä»»åŠ¡ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        task_id: ä»»åŠ¡ ID
        success: æ˜¯å¦æˆåŠŸ
        error_message: é”™è¯¯ä¿¡æ¯ï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    from app.models.task import Task, TaskStatus
    
    try:
        status = TaskStatus.COMPLETED.value if success else TaskStatus.FAILED.value
        values = {
            "status": status,
            "last_heartbeat": None  # æ¸…é™¤å¿ƒè·³ï¼Œè¡¨ç¤ºä»»åŠ¡å·²ç»“æŸ
        }
        if error_message:
            values["error_message"] = error_message
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(**values)
        )
        db.commit()
    except Exception as e:
        logger.error(f"å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
        db.rollback()


# ä½¿ç”¨ Celery ä¿¡å·åœ¨ Worker å¯åŠ¨æ—¶æ‰§è¡Œæ¸…ç†
from celery.signals import worker_ready

@worker_ready.connect
def on_worker_ready(**kwargs):
    """Worker å¯åŠ¨å®Œæˆåæ‰§è¡Œæ¸…ç†"""
    logger.info("Worker å·²å°±ç»ªï¼Œå¼€å§‹æ£€æŸ¥å¡ä½çš„ä»»åŠ¡...")
    cleanup_stuck_reviews()
    cleanup_stuck_tasks()  # [NEW] æ¸…ç†å¿ƒè·³è¶…æ—¶çš„ä»»åŠ¡


# ============== ä»»åŠ¡1: äº”ç‚¹ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=30)
def task_translate_bullet_points(self, product_id: str):
    """
    Translate product bullet points and title.
    This task should run FIRST, before review translation.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.product import Product
    from app.services.translation import translation_service
    import json
    
    logger.info(f"Starting bullet points translation for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Get product
        result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = result.scalar_one_or_none()
        
        if not product:
            logger.error(f"Product {product_id} not found")
            return {"success": False, "error": "Product not found"}
        
        translated_title = None
        translated_bullets = None
        
        # 1. Translate product title if not already translated
        if product.title and not product.title_translated:
            try:
                translated_title = translation_service.translate_product_title(product.title)
                product.title_translated = translated_title
                logger.info(f"Translated product title: {translated_title[:50]}...")
            except Exception as e:
                logger.error(f"Failed to translate product title: {e}")
        
        # 2. Translate bullet points if not already translated
        if product.bullet_points and not product.bullet_points_translated:
            try:
                # Parse bullet points from JSON, PostgreSQL array, or Python list
                bullet_points = []
                if isinstance(product.bullet_points, list):
                    bullet_points = product.bullet_points
                elif isinstance(product.bullet_points, str):
                    bp_str = product.bullet_points.strip()
                    # å°è¯• JSON æ ¼å¼ [...]
                    if bp_str.startswith('['):
                        bullet_points = json.loads(bp_str)
                    # å¤„ç† PostgreSQL æ•°ç»„æ ¼å¼ {...}
                    elif bp_str.startswith('{') and bp_str.endswith('}'):
                        # ç§»é™¤é¦–å°¾çš„ {} å¹¶æŒ‰é€—å·åˆ†å‰²ï¼ˆè€ƒè™‘å¼•å·å†…çš„é€—å·ï¼‰
                        import re
                        # åŒ¹é…å¼•å·å†…çš„å†…å®¹æˆ–éé€—å·å­—ç¬¦
                        content = bp_str[1:-1]  # ç§»é™¤ { }
                        # ä½¿ç”¨æ­£åˆ™åŒ¹é…å¸¦å¼•å·çš„å­—ç¬¦ä¸²
                        matches = re.findall(r'"([^"]*)"', content)
                        if matches:
                            bullet_points = matches
                        else:
                            # ç®€å•åˆ†å‰²ï¼ˆä¸å¸¦å¼•å·çš„æƒ…å†µï¼‰
                            bullet_points = [s.strip() for s in content.split(',') if s.strip()]
                    else:
                        # å°è¯•ç›´æ¥ JSON è§£æ
                        try:
                            bullet_points = json.loads(bp_str)
                        except:
                            bullet_points = [bp_str] if bp_str else []
                
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    # ç»Ÿä¸€ä¿å­˜ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"Translated {len(translated_bullets)} bullet points")
            except Exception as e:
                logger.error(f"Failed to translate bullet points: {e}")
        
        db.commit()
        
        return {
            "success": True,
            "product_id": product_id,
            "title_translated": translated_title is not None,
            "bullet_points_translated": translated_bullets is not None
        }
        
    except Exception as e:
        logger.error(f"Bullet points translation failed for product {product_id}: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡2: è¯„è®ºç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
def task_process_reviews(self, product_id: str, task_id: str):
    """
    Async task to process and translate reviews.
    
    Workflow:
    1. Get pending reviews from database
    2. For each review:
       a. Call Qwen API for translation
       b. Analyze sentiment
       c. Extract insights (æ·±åº¦è§£è¯»)
       d. Update database
       e. Update task progress
    3. Mark task as completed
    
    Args:
        product_id: UUID of the product
        task_id: UUID of the task to track progress
    """
    from app.models.review import Review
    from app.models.task import Task
    from app.models.insight import ReviewInsight
    from app.services.translation import translation_service
    
    logger.info(f"Starting translation task {task_id} for product {product_id}")
    
    db = get_sync_db()
    
    try:
        # Update task status to processing
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(status="processing")
        )
        db.commit()
        
        # Get pending reviews (including processing and failed - to retry stuck/failed translations)
        # ordered by review_date descending (newest first, matching frontend display)
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_(["pending", "processing", "failed"])
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        failed = 0
        
        logger.info(f"Found {total_reviews} pending reviews to translate")
        
        for review in reviews:
            try:
                # Mark as processing
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(translation_status="processing")
                )
                db.commit()
                
                # Validate body_original exists
                if not review.body_original or not review.body_original.strip():
                    logger.warning(f"Review {review.id} has empty body, skipping translation")
                    db.execute(
                        update(Review)
                        .where(Review.id == review.id)
                        .values(translation_status="failed")
                    )
                    db.commit()
                    failed += 1
                    continue
                
                # åªåšç¿»è¯‘ï¼Œä¸æå–æ´å¯Ÿï¼ˆæ´å¯Ÿéœ€è¦ç”¨æˆ·æ‰‹åŠ¨è§¦å‘ï¼‰
                title_translated, body_translated, sentiment, _ = translation_service.translate_review(
                    title=review.title_original,
                    body=review.body_original,
                    extract_insights=False  # å…³é—­è‡ªåŠ¨æ´å¯Ÿæå–
                )
                
                # Validate translation results
                if not body_translated or not body_translated.strip():
                    logger.error(f"Translation returned empty for review {review.id}, body: {review.body_original[:100]}")
                    raise ValueError("Translation returned empty result")
                
                # Update review with translation only
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        title_translated=title_translated if title_translated and title_translated.strip() else None,
                        body_translated=body_translated,
                        sentiment=sentiment.value,
                        translation_status="completed"
                    )
                )
                
                processed += 1
                
                # Update task progress
                db.execute(
                    update(Task)
                    .where(Task.id == task_id)
                    .values(processed_items=processed)
                )
                db.commit()
                
                logger.debug(f"Translated review {review.id}: {review.rating} stars")
                
                # Rate limiting: wait between API calls
                time.sleep(0.2)
                
            except Exception as e:
                logger.error(f"Failed to translate review {review.id}: {e}", exc_info=True)
                failed += 1
                
                # Mark review as failed (don't save empty translations)
                db.execute(
                    update(Review)
                    .where(Review.id == review.id)
                    .values(
                        translation_status="failed",
                        title_translated=None,
                        body_translated=None
                    )
                )
                db.commit()
                
                # Continue with next review
                continue
        
        # Check if there are still pending reviews
        from app.models.review import TranslationStatus
        pending_count_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.PENDING.value
                )
            )
        )
        pending_count = pending_count_result.scalar() or 0
        
        # Update task status - only mark as completed if no pending reviews
        if pending_count == 0:
            final_status = "completed" if failed == 0 else "completed"
        else:
            # Still have pending reviews, keep as processing
            final_status = "processing"
        
        error_msg = f"{failed} reviews failed" if failed > 0 else None
        
        db.execute(
            update(Task)
            .where(Task.id == task_id)
            .values(
                status=final_status,
                processed_items=processed,
                error_message=error_msg
            )
        )
        db.commit()
        
        logger.info(f"Task {task_id} completed: {processed} translated, {failed} failed")
        
        return {
            "task_id": task_id,
            "product_id": product_id,
            "total": total_reviews,
            "processed": processed,
            "failed": failed
        }
        
    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}")
        
        # Mark task as failed
        try:
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(
                    status="failed",
                    error_message=str(e)
                )
            )
            db.commit()
        except:
            pass
        
        # Retry the task
        raise self.retry(exc=e)
        
    finally:
        db.close()


@celery_app.task
def task_health_check():
    """Simple task to verify worker is running."""
    return {"status": "healthy", "worker": "voc_worker"}


@celery_app.task
def task_retry_failed_reviews(product_id: str):
    """
    Retry translation for failed reviews.
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    
    db = get_sync_db()
    
    try:
        # Reset failed reviews to pending
        result = db.execute(
            update(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == "failed"
                )
            )
            .values(translation_status="pending")
        )
        db.commit()
        
        logger.info(f"Reset {result.rowcount} failed reviews to pending")
        
        # Trigger processing
        task_process_reviews.delay(product_id, None)
        
    finally:
        db.close()


@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_extract_insights(self, product_id: str):
    """
    Extract insights for already translated reviews (without re-translating).
    
    This task:
    1. Gets all translated reviews that don't have insights yet
    2. **[NEW] Loads product-specific dimensions if available**
    3. Calls AI to extract insights (using dimensions for categorization)
    4. Saves insights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.insight import ReviewInsight
    from app.models.product_dimension import ProductDimension
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[æ´å¯Ÿæå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting insight extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] è·å–äº§å“çš„ç»´åº¦ Schemaï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        # [UPDATED 2026-01-16] æ”¯æŒ3ç±»ç»´åº¦ä½“ç³»
        dimension_result = db.execute(
            select(ProductDimension)
            .where(ProductDimension.product_id == product_id)
            .order_by(ProductDimension.created_at)
        )
        dimensions = dimension_result.scalars().all()
        
        # [UPDATED 2026-01-16] æŒ‰ç»´åº¦ç±»å‹åˆ†ç»„
        dimension_schema = None
        if dimensions and len(dimensions) > 0:
            # æ£€æŸ¥æ˜¯å¦æœ‰ dimension_type å­—æ®µï¼ˆæ–°ç‰ˆæœ¬æ•°æ®ï¼‰
            has_type_field = hasattr(dimensions[0], 'dimension_type') and dimensions[0].dimension_type
            
            if has_type_field:
                # æ–°æ ¼å¼ï¼šæŒ‰ç±»å‹åˆ†ç»„
                dimension_schema = {
                    "product": [],
                    "scenario": [],
                    "emotion": []
                }
                for dim in dimensions:
                    dim_type = getattr(dim, 'dimension_type', 'product') or 'product'
                    if dim_type in dimension_schema:
                        dimension_schema[dim_type].append({
                            "name": dim.name, 
                            "description": dim.description or ""
                        })
                    else:
                        # æœªçŸ¥ç±»å‹é»˜è®¤å½’å…¥äº§å“ç»´åº¦
                        dimension_schema["product"].append({
                            "name": dim.name, 
                            "description": dim.description or ""
                        })
                
                total_dims = sum(len(v) for v in dimension_schema.values())
                logger.info(f"ä½¿ç”¨3ç±»ç»´åº¦è¿›è¡Œæ´å¯Ÿæå–: æ€»è®¡ {total_dims} ä¸ª "
                           f"(äº§å“:{len(dimension_schema['product'])}, "
                           f"åœºæ™¯:{len(dimension_schema['scenario'])}, "
                           f"æƒ…ç»ª:{len(dimension_schema['emotion'])})")
            else:
                # æ—§æ ¼å¼ï¼šå…¨éƒ¨ä½œä¸ºäº§å“ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤åœºæ™¯å’Œæƒ…ç»ªç»´åº¦
                product_dims = [
                    {"name": dim.name, "description": dim.description or ""}
                    for dim in dimensions
                ]
                dimension_schema = {
                    "product": product_dims,
                    "scenario": [
                        {"name": "æ—¥å¸¸ä½¿ç”¨", "description": "æ—¥å¸¸ç”Ÿæ´»åœºæ™¯"},
                        {"name": "å·¥ä½œåŠå…¬", "description": "åŠå…¬åœºæ™¯"},
                        {"name": "æˆ·å¤–å‡ºè¡Œ", "description": "æˆ·å¤–åœºæ™¯"}
                    ],
                    "emotion": [
                        {"name": "æƒŠå–œå¥½è¯„", "description": "è¶…å‡ºé¢„æœŸçš„æ­£é¢æƒ…ç»ª"},
                        {"name": "å¤±æœ›ä¸æ»¡", "description": "æœŸæœ›è½ç©ºçš„è´Ÿé¢æƒ…ç»ª"},
                        {"name": "æ„Ÿæ¿€æ¨è", "description": "æ„Ÿè°¢å¹¶æ¨è"}
                    ]
                }
                logger.info(f"ä½¿ç”¨ {len(product_dims)} ä¸ªäº§å“ç»´åº¦ + é»˜è®¤åœºæ™¯/æƒ…ç»ªç»´åº¦è¿›è¡Œæ´å¯Ÿæå–")
        else:
            logger.info(f"äº§å“æš‚æ— å®šä¹‰ç»´åº¦ï¼Œä½¿ç”¨é€šç”¨æ´å¯Ÿæå–é€»è¾‘")
        
        # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šè·å–æ€»è¯„è®ºæ•°ï¼ˆæœ‰åŸæ–‡çš„è¯„è®ºï¼Œä¸å†ä¾èµ–ç¿»è¯‘ï¼‰
        total_reviews_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),  # [UPDATED] åªéœ€æœ‰åŸæ–‡å³å¯
                    Review.is_deleted == False
                )
            )
        )
        total_reviews = total_reviews_result.scalar() or 0
        
        # [FIX] è·å–å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°ï¼ˆprocessed_itemsï¼‰
        already_processed_result = db.execute(
            select(func.count(func.distinct(ReviewInsight.review_id)))
            .join(Review, Review.id == ReviewInsight.review_id)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.is_deleted == False
                )
            )
        )
        already_processed = already_processed_result.scalar() or 0
        
        # [NEW] åˆ›å»º/æ›´æ–° Task è®°å½•ï¼ˆtotal_items = æ€»è¯„è®ºæ•°ï¼Œprocessed_items = å·²å¤„ç†æ•°ï¼‰
        task_record = get_or_create_task(
            db=db,
            product_id=product_id,
            task_type=TaskType.INSIGHTS.value,
            total_items=total_reviews,  # [UPDATED] æ€»è¯„è®ºæ•°ï¼ˆä¸å†æ˜¯å·²ç¿»è¯‘æ•°ï¼‰
            celery_task_id=self.request.id
        )
        # è®¾ç½®å·²å¤„ç†æ•°ä¸ºå½“å‰å·²æœ‰æ´å¯Ÿçš„è¯„è®ºæ•°
        task_record.processed_items = already_processed
        db.commit()
        logger.info(f"[è·¨è¯­è¨€æ´å¯Ÿ] Task record: total_items={total_reviews}, processed_items={already_processed}, remaining={total_reviews - already_processed}")
        
        # [FIX] ä½¿ç”¨ NOT EXISTS å­æŸ¥è¯¢æ’é™¤å·²æœ‰æ´å¯Ÿçš„è¯„è®ºï¼Œé¿å…é‡å¤å¤„ç†
        insight_exists_subquery = (
            select(ReviewInsight.id)
            .where(ReviewInsight.review_id == Review.id)
            .exists()
        )
        
        # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šè·å–æœ‰åŸæ–‡çš„è¯„è®ºï¼ˆä¸å†ä¾èµ–ç¿»è¯‘ï¼‰ï¼Œæ’é™¤å·²æœ‰æ´å¯Ÿçš„è¯„è®º
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),  # [UPDATED] åªéœ€æœ‰åŸæ–‡å³å¯ï¼Œä¸å†ä¾èµ–ç¿»è¯‘
                    Review.is_deleted == False,
                    ~insight_exists_subquery  # [FIX] Only process reviews without insights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        reviews_to_process = len(reviews)
        processed = 0
        insights_extracted = 0
        
        # ğŸš€ å¹¶è¡Œåç¨‹ä¼˜åŒ–ï¼šä½¿ç”¨ gevent pool å¹¶è¡Œè°ƒç”¨ AI API
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼ŒæœåŠ¡å™¨ B å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å€¼
        import os
        PARALLEL_SIZE = int(os.environ.get('INSIGHT_PARALLEL_SIZE', '120'))  # 40K RPM ä¼˜åŒ–ï¼š60â†’120
        
        # ğŸ”¥ [OPTIMIZED] BATCH_SIZE = PARALLEL_SIZEï¼Œå……åˆ†åˆ©ç”¨å¹¶è¡Œæ± 
        # ä¹‹å‰ BATCH_SIZE=20 é™åˆ¶äº†çœŸå®å¹¶å‘ï¼Œç°åœ¨ä¸ PARALLEL_SIZE åŒæ­¥
        BATCH_SIZE = PARALLEL_SIZE
        pending_insights = []  # å¾…æäº¤çš„æ´å¯Ÿåˆ—è¡¨
        
        logger.info(f"[è·¨è¯­è¨€æ´å¯Ÿ] Found {reviews_to_process} reviews remaining for insight extraction (total={total_reviews}, already_done={already_processed})")
        logger.info(f"[å¹¶è¡Œä¼˜åŒ–-æ´å¯Ÿ] ä½¿ç”¨ PARALLEL_SIZE={PARALLEL_SIZE} å¹¶è¡Œå¤„ç†, BATCH_SIZE={BATCH_SIZE} æ‰¹é‡å…¥åº“")
        
        # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šåªä½¿ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œæ´å¯Ÿæå–
        def process_single_insight(review):
            """å¹¶è¡Œå¤„ç†å•æ¡è¯„è®ºçš„æ´å¯Ÿæå–ï¼ˆè·¨è¯­è¨€æ¨¡å¼ï¼šè‹±æ–‡è¾“å…¥â†’ä¸­æ–‡è¾“å‡ºï¼‰"""
            try:
                insights = translation_service.extract_insights(
                    original_text=review.body_original or "",
                    # [UPDATED] ä¸å†ä¼ å…¥ translated_textï¼Œè·¨è¯­è¨€æ¨¡å¼ç›´æ¥ä»åŸæ–‡æå–
                    dimension_schema=dimension_schema
                )
                return {
                    "review_id": review.id,
                    "insights": insights,
                    "success": True
                }
            except Exception as e:
                logger.error(f"[è·¨è¯­è¨€æ´å¯Ÿ] Failed to extract insights for review {review.id}: {e}")
                return {
                    "review_id": review.id,
                    "insights": None,
                    "success": False,
                    "error": str(e)
                }
        
        # ä½¿ç”¨ gevent pool å¹¶è¡Œå¤„ç†
        from gevent.pool import Pool
        pool = Pool(PARALLEL_SIZE)
        
        # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        for batch_start in range(0, reviews_to_process, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, reviews_to_process)
            batch_reviews = reviews[batch_start:batch_end]
            
            # ğŸš€ å¹¶è¡Œè°ƒç”¨ AI API
            results = pool.map(process_single_insight, batch_reviews)
            
            # å¤„ç†ç»“æœ
            for result in results:
                # [FIX 2026-01-15] åŒºåˆ†"æˆåŠŸä½†ç©ºç»“æœ"å’Œ"å¤±è´¥"
                # æ³¨æ„ï¼šæ´å¯Ÿæå–Promptè¦æ±‚è‡³å°‘1ä¸ªæ´å¯Ÿï¼Œæ‰€ä»¥ç©ºç»“æœç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿ
                # ä½†å¦‚æœå‘ç”Ÿï¼Œåº”è¯¥è®°å½•è­¦å‘Šè€Œä¸æ˜¯å½“ä½œå¤±è´¥
                if result["success"]:
                    insights = result.get("insights", [])
                    if insights:  # æœ‰æ´å¯Ÿï¼Œæ­£å¸¸å¤„ç†
                        for insight_data in insights:
                            # [UPDATED 2026-01-15] æ·»åŠ  confidence å­—æ®µæ”¯æŒ
                            confidence = insight_data.get('confidence', 'high')
                            if confidence not in ('high', 'medium', 'low'):
                                confidence = 'high'
                            
                            insight = ReviewInsight(
                                review_id=result["review_id"],
                                insight_type=insight_data.get('type', 'emotion'),
                                quote=insight_data.get('quote', ''),
                                quote_translated=insight_data.get('quote_translated'),
                                analysis=insight_data.get('analysis', ''),
                                dimension=insight_data.get('dimension'),
                                confidence=confidence  # [NEW] ç½®ä¿¡åº¦
                            )
                            pending_insights.append(insight)
                        insights_extracted += len(insights)
                    else:
                        # æˆåŠŸä½†ç©ºç»“æœï¼ˆè™½ç„¶Promptè¦æ±‚è‡³å°‘1ä¸ªï¼Œä½†AIå¯èƒ½è¿”å›ç©ºï¼‰
                        logger.warning(f"[è·¨è¯­è¨€æ´å¯Ÿ] è¯„è®º {result['review_id']} AIè¿”å›ç©ºæ´å¯Ÿæ•°ç»„ï¼ˆä¸ç¬¦åˆPromptè¦æ±‚ï¼Œä½†è§†ä¸ºæˆåŠŸï¼‰")
                else:
                    # ğŸ›¡ï¸ [FIX v3] åŸºäºé‡è¯•æ¬¡æ•°åˆ¤æ–­ï¼Œé¿å…æ— é™å¾ªç¯
                    from app.core.redis import get_sync_redis
                    redis_client = get_sync_redis()
                    review_id_str = str(result["review_id"])
                    retry_key = f"insight_retry:{review_id_str}"
                    
                    # å¢åŠ å¤±è´¥è®¡æ•°
                    retry_count = redis_client.incr(retry_key)
                    redis_client.expire(retry_key, 86400)  # 24å°æ—¶åè¿‡æœŸ
                    
                    if retry_count >= 3:
                        # å·²é‡è¯• 3 æ¬¡ï¼ŒAI ä»æ— æ³•æå–ï¼Œæ ‡è®°ä¸º"å·²å¤„ç†"
                        review_obj = next((r for r in reviews if str(r.id) == review_id_str), None)
                        review_text = review_obj.body_original[:100] if review_obj and review_obj.body_original else None
                        
                        empty_marker = ReviewInsight(
                            review_id=result["review_id"],
                            insight_type="_ai_no_content",
                            quote=review_text or "",
                            analysis=f"AIå¤šæ¬¡å°è¯•ååˆ¤å®šæ— æ³•æå–æœ‰æ„ä¹‰æ´å¯Ÿï¼ˆé‡è¯•{retry_count}æ¬¡ï¼‰"
                        )
                        pending_insights.append(empty_marker)
                        redis_client.delete(retry_key)  # æ¸…é™¤è®¡æ•°
                        logger.info(f"[è·¨è¯­è¨€æ´å¯Ÿ] â­ï¸ è¯„è®º {review_id_str} é‡è¯•{retry_count}æ¬¡åAIåˆ¤å®šæ— æ³•æå–ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†")
                    else:
                        # æœªè¾¾åˆ°é‡è¯•ä¸Šé™ï¼Œå…è®¸ä¸‹æ¬¡é‡è¯•
                        error_msg = result.get("error", "Unknown error")
                        logger.warning(f"[è·¨è¯­è¨€æ´å¯Ÿ] âš ï¸ è¯„è®º {review_id_str} æå–å¤±è´¥(ç¬¬{retry_count}æ¬¡): {error_msg}ï¼Œå°†åœ¨ä¸‹æ¬¡ä»»åŠ¡ä¸­é‡è¯•")
                
                processed += 1
            
            # ğŸ”¥ æ‰¹é‡æäº¤æ•°æ®åº“
            if pending_insights:
                db.add_all(pending_insights)
                db.commit()
                logger.info(f"[å¹¶è¡Œå…¥åº“] å·²æäº¤ {len(pending_insights)} æ¡æ´å¯Ÿï¼ˆè¿›åº¦: {processed}/{reviews_to_process}ï¼‰")
                pending_insights = []
                # æ³¨ï¼šç¼“å­˜é€šè¿‡ API å±‚ 2 ç§’ TTL è‡ªåŠ¨è¿‡æœŸï¼Œæ— éœ€åœ¨æ­¤å¤„æ‰‹åŠ¨æ¸…é™¤
            
            # æ›´æ–° Task è¿›åº¦
            if task_record:
                task_record.processed_items = already_processed + processed
                db.commit()
            
            # [OPTIMIZED] æ‰¹æ¬¡é—´å¾®å°ä¼‘æ¯ï¼Œé˜¿é‡Œäº‘ API é™æµä¸€èˆ¬ 60-100 QPSï¼Œ0.05s è¶³å¤Ÿ
            time.sleep(0.05)
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†æ´å¯Ÿ
        if pending_insights:
            db.add_all(pending_insights)
            db.commit()
            logger.info(f"[å¹¶è¡Œå…¥åº“] æœ€ç»ˆæäº¤ {len(pending_insights)} æ¡æ´å¯Ÿ")
        
        logger.info(f"[è·¨è¯­è¨€æ´å¯Ÿ] Insight extraction completed: processed {processed} new reviews (total={total_reviews}, now_done={already_processed + processed}), {insights_extracted} insights extracted")
        
        # ğŸš€ ç¼“å­˜å¤±æ•ˆ - æ´å¯Ÿæå–å®Œæˆåæ¸…é™¤äº§å“ç›¸å…³ç¼“å­˜
        if insights_extracted > 0:
            try:
                from app.core.cache import get_cache_service_sync
                from app.models.product import Product
                product_result = db.execute(select(Product).where(Product.id == product_id))
                product = product_result.scalar_one_or_none()
                if product:
                    cache = get_cache_service_sync()
                    cache.invalidate_all_for_product(product.asin)
                    logger.info(f"[Cache] Invalidated caches for product {product.asin} after insight extraction")
            except Exception as cache_error:
                logger.warning(f"[Cache] Failed to invalidate cache: {cache_error}")
        
        # ğŸ›¡ï¸ [NEW] æœ«å°¾è¡¥å…¨æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰é—æ¼çš„è¯„è®º
        # é‡æ–°æŸ¥è¯¢æ˜¯å¦æœ‰é—æ¼ï¼ˆå¯èƒ½å› ä¸ºæ—¶åºé—®é¢˜æˆ–å¤„ç†å¤±è´¥ï¼‰
        final_check_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.is_deleted == False,
                    ~insight_exists_subquery
                )
            )
        )
        remaining = final_check_result.scalar() or 0
        
        if remaining > 0:
            logger.warning(f"[è·¨è¯­è¨€æ´å¯Ÿ] âš ï¸ å‘ç° {remaining} æ¡é—æ¼è¯„è®ºï¼Œ5ç§’åè§¦å‘è¡¥å…¨ä»»åŠ¡...")
            # çŸ­æš‚å»¶è¿Ÿåè§¦å‘è¡¥å…¨ä»»åŠ¡ï¼ˆé¿å…ç«‹å³é€’å½’å¯¼è‡´èµ„æºäº‰æŠ¢ï¼‰
            time.sleep(5)
            task_extract_insights.apply_async(
                args=[product_id],
                countdown=10  # 10ç§’åæ‰§è¡Œï¼Œé¿å…ä»»åŠ¡å †ç§¯
            )
            logger.info(f"[è·¨è¯­è¨€æ´å¯Ÿ] ğŸ”„ è¡¥å…¨ä»»åŠ¡å·²è§¦å‘ï¼Œå°†å¤„ç† {remaining} æ¡é—æ¼è¯„è®º")
        
        # [FIX] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.processed_items = already_processed + processed  # æœ€ç»ˆå¤„ç†æ•°
            db.commit()
        
        return {
            "product_id": product_id,
            "total_reviews": total_reviews,  # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šæ€»è¯„è®ºæ•°ï¼ˆä¸å†æ˜¯å·²ç¿»è¯‘æ•°ï¼‰
            "processed": processed,
            "insights_extracted": insights_extracted,
            "remaining": remaining  # [NEW] è¿”å›å‰©ä½™æœªå¤„ç†æ•°
        }
        
    except Exception as e:
        logger.error(f"Insight extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== ä»»åŠ¡4: ä¸»é¢˜é«˜äº®æå– ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30, time_limit=1800, soft_time_limit=1700)
def task_extract_themes(self, product_id: str):
    """
    Extract 5W theme keywords for already translated reviews.
    
    This task:
    1. **[NEW] Auto-generates 5W context labels if not exists (Definition phase)**
    2. Gets all translated reviews that don't have theme highlights yet
    3. **[NEW] Uses context labels for forced categorization (Execution phase)**
    4. Calls AI to extract 5W themes with evidence and explanation
    5. Saves theme highlights to database
    
    Args:
        product_id: UUID of the product
    """
    from app.models.review import Review
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.product_context_label import ProductContextLabel
    from app.models.task import Task, TaskType, TaskStatus
    from app.services.translation import translation_service
    from sqlalchemy import delete, exists, func
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿ
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ä¸»é¢˜æå–] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"Starting theme extraction for product {product_id}")
    
    db = get_sync_db()
    task_record = None
    
    try:
        # [NEW] Step 1: æ£€æŸ¥æ˜¯å¦æœ‰ 5W æ ‡ç­¾åº“ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ç”Ÿæˆ
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        context_schema = None
        labels_generated = False
        
        if label_count == 0:
            logger.info(f"äº§å“ {product_id} æš‚æ—  5W æ ‡ç­¾åº“ï¼Œå¼€å§‹è‡ªåŠ¨å­¦ä¹ ...")
            
            # [NEW] å…ˆè·å–äº§å“ä¿¡æ¯ï¼ˆæ ‡é¢˜å’Œäº”ç‚¹ï¼‰
            from app.models.product import Product
            import json as json_lib
            
            product_result = db.execute(
                select(Product).where(Product.id == product_id)
            )
            product = product_result.scalar_one_or_none()
            
            product_title = ""
            bullet_points = []
            
            if product:
                product_title = product.title or ""
                # è§£æäº”ç‚¹ï¼ˆå­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²ï¼‰
                if product.bullet_points:
                    try:
                        bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
                    except:
                        bullet_points = []
                logger.info(f"ğŸ“¦ äº§å“ä¿¡æ¯ï¼š{product.asin}ï¼Œæ ‡é¢˜é•¿åº¦={len(product_title)}ï¼Œäº”ç‚¹={len(bullet_points)}æ¡")
            
            # è·å–å·²ç¿»è¯‘çš„è¯„è®ºæ ·æœ¬ï¼ˆè‡³å°‘10æ¡ï¼‰
            sample_result = db.execute(
                select(Review.body_original, Review.body_translated)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == "completed",
                        Review.body_translated.isnot(None),
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(50)
            )
            sample_reviews = sample_result.all()
            
            # [UPDATED 2026-01-19] é™ä½æœ€ä½æ ·æœ¬è¦æ±‚ï¼Œåªè¦æœ‰è¯„è®ºå°±è¿›è¡Œå­¦ä¹ 
            if len(sample_reviews) >= 1:
                # å‡†å¤‡æ ·æœ¬æ–‡æœ¬
                sample_texts = []
                for row in sample_reviews:
                    text = row.body_translated or row.body_original
                    if text and text.strip():
                        sample_texts.append(text.strip())
                
                if len(sample_texts) >= 1:
                    logger.info(f"ğŸ“ æ ·æœ¬æ•°é‡: {len(sample_texts)} æ¡ï¼Œå¼€å§‹å­¦ä¹  5W æ ‡ç­¾åº“...")
                    # [UPDATED] è°ƒç”¨ AI å­¦ä¹ æ ‡ç­¾åº“ï¼ˆä¼ å…¥äº§å“ä¿¡æ¯ï¼‰
                    learned_labels = translation_service.learn_context_labels(
                        reviews_text=sample_texts,
                        product_title=product_title,      # [NEW] äº§å“æ ‡é¢˜
                        bullet_points=bullet_points       # [NEW] äº”ç‚¹å–ç‚¹
                    )
                    
                    if learned_labels:
                        # å­˜å…¥æ•°æ®åº“ï¼ˆæ‰©å±•ç‰ˆï¼šbuyer/user æ›¿ä»£ whoï¼‰
                        for context_type in ["buyer", "user", "who", "where", "when", "why", "what"]:
                            labels = learned_labels.get(context_type, [])
                            for item in labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                        
                        db.commit()
                        labels_generated = True
                        total_labels = sum(len(v) for v in learned_labels.values())
                        logger.info(f"âœ… è‡ªåŠ¨ç”Ÿæˆ 5W æ ‡ç­¾åº“æˆåŠŸï¼Œå…± {total_labels} ä¸ªæ ‡ç­¾")
                    else:
                        logger.warning(f"âš ï¸ AI å­¦ä¹ æ ‡ç­¾åº“å¤±è´¥ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
                else:
                    logger.warning(f"âš ï¸ æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
            else:
                logger.warning(f"âš ï¸ æ²¡æœ‰å¯ç”¨è¯„è®ºï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # Step 2: è·å–æ ‡ç­¾åº“ Schemaï¼ˆå¦‚æœå­˜åœ¨æˆ–åˆšç”Ÿæˆï¼‰
        if label_count > 0 or labels_generated:
            label_result = db.execute(
                select(ProductContextLabel)
                .where(ProductContextLabel.product_id == product_id)
                .order_by(ProductContextLabel.type, ProductContextLabel.created_at)
            )
            labels = label_result.scalars().all()
            
            if labels:
                context_schema = {}
                for label in labels:
                    if label.type not in context_schema:
                        context_schema[label.type] = []
                    context_schema[label.type].append({
                        "name": label.name,
                        "description": label.description or ""
                    })
                logger.info(f"âœ… ä½¿ç”¨ 5W æ ‡ç­¾åº“è¿›è¡Œå¼ºåˆ¶å½’ç±»ï¼Œå…± {len(labels)} ä¸ªæ ‡ç­¾")
        else:
            logger.info(f"â„¹ï¸ æœªä½¿ç”¨æ ‡ç­¾åº“ï¼Œå°†ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šè·å–æœ‰åŸæ–‡çš„è¯„è®ºï¼ˆä¸å†ä¾èµ–ç¿»è¯‘ï¼‰ï¼Œæ’é™¤å·²æœ‰ä¸»é¢˜çš„è¯„è®º
        # Use a subquery to check for existing theme highlights
        theme_exists_subquery = (
            select(ReviewThemeHighlight.id)
            .where(ReviewThemeHighlight.review_id == Review.id)
            .exists()
        )
        
        # Ordered by review_date to match page display order
        result = db.execute(
            select(Review)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),  # [UPDATED] åªéœ€æœ‰åŸæ–‡å³å¯ï¼Œä¸å†ä¾èµ–ç¿»è¯‘
                    Review.is_deleted == False,
                    ~theme_exists_subquery  # Reviews without theme highlights
                )
            )
            .order_by(Review.review_date.desc().nullslast(), Review.created_at.desc())
        )
        reviews = result.scalars().all()
        
        total_reviews = len(reviews)
        processed = 0
        themes_extracted = 0
        
        logger.info(f"[è·¨è¯­è¨€5W] Found {total_reviews} reviews for theme extraction (no translation required)")
        
        # [NEW] åˆ›å»º/æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œå¯ç”¨å¿ƒè·³
        if total_reviews > 0:
            task_record = get_or_create_task(
                db=db,
                product_id=product_id,
                task_type=TaskType.THEMES.value,
                total_items=total_reviews,
                celery_task_id=self.request.id
            )
            logger.info(f"ä»»åŠ¡è®°å½•å·²åˆ›å»º: {task_record.id}")
        
        # ğŸ”¥ ä¼˜å…ˆä» Redis ç¼“å­˜è·å–æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆé¿å…é¢‘ç¹æŸ¥ PostgreSQLï¼‰
        label_id_map = label_cache.get_label_id_map(str(product_id))
        
        if label_id_map:
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âœ… å‘½ä¸­ç¼“å­˜ï¼Œå…± {len(label_id_map)} ä¸ªæ ‡ç­¾")
        elif context_schema:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“æ„å»ºå¹¶ç¼“å­˜
            label_id_map = {}
            for label in labels:
                key = (label.type, label.name)
                label_id_map[key] = label.id
            
            # å­˜å…¥ Redis ç¼“å­˜
            label_cache.set_label_id_map(str(product_id), label_id_map)
            logger.info(f"[æ ‡ç­¾ç¼“å­˜] âš¡ å·²æ„å»ºå¹¶ç¼“å­˜ {len(label_id_map)} ä¸ªæ ‡ç­¾")
        else:
            label_id_map = {}
            logger.debug(f"æ— æ ‡ç­¾åº“ï¼Œä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
        
        # ğŸš€ å¹¶è¡Œåç¨‹ä¼˜åŒ–ï¼šä½¿ç”¨ gevent pool å¹¶è¡Œè°ƒç”¨ AI API
        # æ”¯æŒç¯å¢ƒå˜é‡é…ç½®ï¼ŒæœåŠ¡å™¨ B å¯ä»¥ä½¿ç”¨æ›´é«˜çš„å€¼
        import os
        PARALLEL_SIZE = int(os.environ.get('THEME_PARALLEL_SIZE', '150'))  # 40K RPM ä¼˜åŒ–ï¼š80â†’150
        
        # ğŸ”¥ [OPTIMIZED] BATCH_SIZE = PARALLEL_SIZEï¼Œå……åˆ†åˆ©ç”¨å¹¶è¡Œæ± 
        # ä¹‹å‰ BATCH_SIZE=20 é™åˆ¶äº†çœŸå®å¹¶å‘ï¼Œç°åœ¨ä¸ PARALLEL_SIZE åŒæ­¥
        BATCH_SIZE = PARALLEL_SIZE
        pending_themes = []  # å¾…æäº¤çš„ä¸»é¢˜åˆ—è¡¨
        
        logger.info(f"[å¹¶è¡Œä¼˜åŒ–-ä¸»é¢˜] ä½¿ç”¨ PARALLEL_SIZE={PARALLEL_SIZE} å¹¶è¡Œå¤„ç†, BATCH_SIZE={BATCH_SIZE} æ‰¹é‡å…¥åº“")
        
        # [UPDATED] è·¨è¯­è¨€æ¨¡å¼ï¼šåªä½¿ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œ5Wä¸»é¢˜æå–
        def process_single_theme(review):
            """å¹¶è¡Œå¤„ç†å•æ¡è¯„è®ºçš„ä¸»é¢˜æå–ï¼ˆè·¨è¯­è¨€æ¨¡å¼ï¼šè‹±æ–‡è¾“å…¥â†’ä¸­æ–‡è¾“å‡ºï¼‰"""
            try:
                themes = translation_service.extract_themes(
                    original_text=review.body_original or "",
                    # [UPDATED] ä¸å†ä¼ å…¥ translated_textï¼Œè·¨è¯­è¨€æ¨¡å¼ç›´æ¥ä»åŸæ–‡æå–
                    context_schema=context_schema
                )
                return {
                    "review_id": review.id,
                    "themes": themes,
                    "success": True
                }
            except Exception as e:
                logger.error(f"[è·¨è¯­è¨€5W] Failed to extract themes for review {review.id}: {e}")
                return {
                    "review_id": review.id,
                    "themes": None,
                    "success": False,
                    "error": str(e)
                }
        
        # ä½¿ç”¨ gevent pool å¹¶è¡Œå¤„ç†
        from gevent.pool import Pool
        pool = Pool(PARALLEL_SIZE)
        
        # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
        for batch_start in range(0, total_reviews, BATCH_SIZE):
            batch_end = min(batch_start + BATCH_SIZE, total_reviews)
            batch_reviews = reviews[batch_start:batch_end]
            
            # ğŸš€ å¹¶è¡Œè°ƒç”¨ AI API
            results = pool.map(process_single_theme, batch_reviews)
            
            # å¤„ç†ç»“æœ
            batch_themes_count = 0
            for result in results:
                # [FIX 2026-01-15] åŒºåˆ†"æˆåŠŸä½†ç©ºç»“æœ"å’Œ"å¤±è´¥"
                # - success=True, themes={} â†’ æˆåŠŸä½†æ— ä¸»é¢˜ï¼ˆç¬¦åˆ"æœ‰å‹‡æ°”è¯´æ²¡æœ‰"è§„åˆ™ï¼‰ï¼Œä¸åˆ›å»ºè®°å½•
                # - success=False â†’ çœŸæ­£çš„å¤±è´¥ï¼Œéœ€è¦é‡è¯•
                if result["success"]:
                    # æˆåŠŸï¼šå¤„ç†æœ‰ä¸»é¢˜çš„æƒ…å†µï¼Œç©ºå­—å…¸è¡¨ç¤ºAIåˆ¤å®šæ— ä¸»é¢˜ï¼Œè¿™æ˜¯æ­£ç¡®çš„
                    themes = result.get("themes", {})
                    if themes:  # åªæœ‰å½“themeséç©ºæ—¶æ‰å¤„ç†
                        for theme_type, items in themes.items():
                            if not items or len(items) == 0:
                                continue
                            
                            for item in items:
                                label_name = item.get("content", "").strip()
                                quote = item.get("quote") or item.get("content_original") or None
                                quote_translated = item.get("quote_translated") or item.get("content_translated") or None
                                explanation = item.get("explanation") or None
                                # [NEW 2026-01-15] è·å–ç½®ä¿¡åº¦
                                confidence = item.get("confidence", "high")
                                if confidence not in ("high", "medium", "low"):
                                    confidence = "high"
                                
                                if not label_name:
                                    continue
                                
                                context_label_id = label_id_map.get((theme_type, label_name))
                                
                                theme_highlight = ReviewThemeHighlight(
                                    review_id=result["review_id"],
                                    theme_type=theme_type,
                                    label_name=label_name,
                                    quote=quote,
                                    quote_translated=quote_translated,
                                    explanation=explanation,
                                    confidence=confidence,  # [NEW] ç½®ä¿¡åº¦
                                    context_label_id=context_label_id,
                                    items=[item]
                                )
                                pending_themes.append(theme_highlight)
                                batch_themes_count += 1
                    else:
                        # ğŸ”¥ [FIX 2026-01-15] themesä¸ºç©ºå­—å…¸ï¼Œè¡¨ç¤ºAIåˆ¤å®šè¯¥è¯„è®ºæ— ä¸»é¢˜
                        # åˆ›å»ºä¸€ä¸ª skipped ç±»å‹çš„è®°å½•ï¼Œé¿å…è¢«æ ‡è®°ä¸º"é—æ¼"è€Œæ— é™é‡è¯•
                        skipped_highlight = ReviewThemeHighlight(
                            review_id=result["review_id"],
                            theme_type="skipped",
                            label_name="æ— ä¸»é¢˜",
                            quote=None,
                            quote_translated=None,
                            explanation="AIåˆ¤å®šè¯¥è¯„è®ºå†…å®¹è¿‡çŸ­æˆ–æ— æ˜ç¡®5Wä¸»é¢˜ä¿¡æ¯",
                            confidence="high",
                            context_label_id=None,
                            items=[]
                        )
                        pending_themes.append(skipped_highlight)
                        logger.debug(f"[è·¨è¯­è¨€5W] è¯„è®º {result['review_id']} AIåˆ¤å®šæ— ä¸»é¢˜ï¼Œåˆ›å»ºskippedæ ‡è®°")
                else:
                    # ğŸ›¡ï¸ [FIX v3] åŸºäºé‡è¯•æ¬¡æ•°åˆ¤æ–­ï¼Œé¿å…æ— é™å¾ªç¯
                    # ä½¿ç”¨ Redis è®°å½•å¤±è´¥æ¬¡æ•°ï¼Œè¶…è¿‡ 3 æ¬¡å°±æ ‡è®°ä¸º"AIåˆ¤å®šæ— æ³•æå–"
                    from app.core.redis import get_sync_redis
                    redis_client = get_sync_redis()
                    review_id_str = str(result["review_id"])
                    retry_key = f"theme_retry:{review_id_str}"
                    
                    # å¢åŠ å¤±è´¥è®¡æ•°
                    retry_count = redis_client.incr(retry_key)
                    redis_client.expire(retry_key, 86400)  # 24å°æ—¶åè¿‡æœŸ
                    
                    if retry_count >= 3:
                        # å·²é‡è¯• 3 æ¬¡ï¼ŒAI ä»æ— æ³•æå–ï¼Œæ ‡è®°ä¸º"å·²å¤„ç†"
                        review_obj = next((r for r in reviews if str(r.id) == review_id_str), None)
                        review_text = review_obj.body_original[:100] if review_obj and review_obj.body_original else None
                        
                        empty_marker = ReviewThemeHighlight(
                            review_id=result["review_id"],
                            theme_type="skipped",
                            label_name="_ai_no_content",
                            quote=review_text,
                            explanation=f"AIå¤šæ¬¡å°è¯•ååˆ¤å®šæ— æ³•æå–æœ‰æ„ä¹‰ä¸»é¢˜ï¼ˆé‡è¯•{retry_count}æ¬¡ï¼‰"
                        )
                        pending_themes.append(empty_marker)
                        redis_client.delete(retry_key)  # æ¸…é™¤è®¡æ•°
                        logger.info(f"[è·¨è¯­è¨€ä¸»é¢˜] â­ï¸ è¯„è®º {review_id_str} é‡è¯•{retry_count}æ¬¡åAIåˆ¤å®šæ— æ³•æå–ï¼Œæ ‡è®°ä¸ºå·²å¤„ç†")
                    else:
                        # æœªè¾¾åˆ°é‡è¯•ä¸Šé™ï¼Œå…è®¸ä¸‹æ¬¡é‡è¯•
                        error_msg = result.get("error", "Unknown error")
                        logger.warning(f"[è·¨è¯­è¨€ä¸»é¢˜] âš ï¸ è¯„è®º {review_id_str} æå–å¤±è´¥(ç¬¬{retry_count}æ¬¡): {error_msg}ï¼Œå°†åœ¨ä¸‹æ¬¡ä»»åŠ¡ä¸­é‡è¯•")
                
                processed += 1
            
            themes_extracted += batch_themes_count
            
            # ğŸ”¥ æ‰¹é‡æäº¤æ•°æ®åº“
            if pending_themes:
                db.add_all(pending_themes)
                db.commit()
                logger.info(f"[å¹¶è¡Œå…¥åº“] å·²æäº¤ {len(pending_themes)} æ¡ä¸»é¢˜ï¼ˆè¿›åº¦: {processed}/{total_reviews}ï¼‰")
                pending_themes = []
                # æ³¨ï¼šç¼“å­˜é€šè¿‡ API å±‚ 2 ç§’ TTL è‡ªåŠ¨è¿‡æœŸï¼Œæ— éœ€åœ¨æ­¤å¤„æ‰‹åŠ¨æ¸…é™¤
            
            # æ›´æ–° Task è¿›åº¦
            if task_record:
                update_task_heartbeat(db, str(task_record.id), processed_items=processed)
            
            # [OPTIMIZED] æ‰¹æ¬¡é—´å¾®å°ä¼‘æ¯ï¼Œé˜¿é‡Œäº‘ API é™æµä¸€èˆ¬ 60-100 QPSï¼Œ0.05s è¶³å¤Ÿ
            time.sleep(0.05)
        
        # ğŸ”¥ æäº¤å‰©ä½™çš„å¾…å¤„ç†ä¸»é¢˜
        if pending_themes:
            db.add_all(pending_themes)
            db.commit()
            logger.info(f"[å¹¶è¡Œå…¥åº“] æœ€ç»ˆæäº¤ {len(pending_themes)} æ¡ä¸»é¢˜")
        
        logger.info(f"Theme extraction completed: {processed}/{total_reviews} reviews processed, {themes_extracted} theme entries created")
        
        # ğŸ”¥ [NEW 2026-01-15] åŒæ­¥æ›´æ–° context_labels çš„ count å€¼
        # æ ¹æ® review_theme_highlights è¡¨ä¸­çš„å…³è”æƒ…å†µï¼Œæ›´æ–°ç»Ÿè®¡æ•°é‡
        if themes_extracted > 0 and context_schema:
            try:
                from sqlalchemy import update as sql_update
                
                # è·å–æ‰€æœ‰å…³è”çš„ label ç»Ÿè®¡
                count_result = db.execute(
                    select(ReviewThemeHighlight.context_label_id, func.count(ReviewThemeHighlight.id))
                    .join(Review, ReviewThemeHighlight.review_id == Review.id)
                    .where(
                        and_(
                            Review.product_id == product_id,
                            ReviewThemeHighlight.context_label_id.isnot(None)
                        )
                    )
                    .group_by(ReviewThemeHighlight.context_label_id)
                )
                label_counts = {row[0]: row[1] for row in count_result.all()}
                
                # æ‰¹é‡æ›´æ–° count å­—æ®µ
                if label_counts:
                    for label_id, count in label_counts.items():
                        db.execute(
                            sql_update(ProductContextLabel)
                            .where(ProductContextLabel.id == label_id)
                            .values(count=count)
                        )
                    db.commit()
                    logger.info(f"[5Wæ ‡ç­¾åŒæ­¥] âœ… å·²æ›´æ–° {len(label_counts)} ä¸ªæ ‡ç­¾çš„ count å€¼")
            except Exception as count_error:
                logger.error(f"[5Wæ ‡ç­¾åŒæ­¥] âŒ æ›´æ–° count å¤±è´¥: {count_error}")
        
        # ğŸš€ ç¼“å­˜å¤±æ•ˆ - ä¸»é¢˜æå–å®Œæˆåæ¸…é™¤äº§å“ç›¸å…³ç¼“å­˜
        if themes_extracted > 0:
            try:
                from app.core.cache import get_cache_service_sync
                from app.models.product import Product
                product_result = db.execute(select(Product).where(Product.id == product_id))
                product = product_result.scalar_one_or_none()
                if product:
                    cache = get_cache_service_sync()
                    cache.invalidate_all_for_product(product.asin)
                    logger.info(f"[Cache] Invalidated caches for product {product.asin} after theme extraction")
            except Exception as cache_error:
                logger.warning(f"[Cache] Failed to invalidate cache: {cache_error}")
        
        # ğŸ›¡ï¸ [NEW] æœ«å°¾è¡¥å…¨æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰é—æ¼çš„è¯„è®º
        # é‡æ–°æŸ¥è¯¢æ˜¯å¦æœ‰é—æ¼ï¼ˆå¯èƒ½å› ä¸ºæ—¶åºé—®é¢˜æˆ–å¤„ç†å¤±è´¥ï¼‰
        final_check_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.is_deleted == False,
                    ~theme_exists_subquery
                )
            )
        )
        remaining = final_check_result.scalar() or 0
        
        if remaining > 0:
            logger.warning(f"[è·¨è¯­è¨€ä¸»é¢˜] âš ï¸ å‘ç° {remaining} æ¡é—æ¼è¯„è®ºï¼Œ5ç§’åè§¦å‘è¡¥å…¨ä»»åŠ¡...")
            # çŸ­æš‚å»¶è¿Ÿåè§¦å‘è¡¥å…¨ä»»åŠ¡ï¼ˆé¿å…ç«‹å³é€’å½’å¯¼è‡´èµ„æºäº‰æŠ¢ï¼‰
            time.sleep(5)
            task_extract_themes.apply_async(
                args=[product_id],
                countdown=10  # 10ç§’åæ‰§è¡Œï¼Œé¿å…ä»»åŠ¡å †ç§¯
            )
            logger.info(f"[è·¨è¯­è¨€ä¸»é¢˜] ğŸ”„ è¡¥å…¨ä»»åŠ¡å·²è§¦å‘ï¼Œå°†å¤„ç† {remaining} æ¡é—æ¼è¯„è®º")
        
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå®Œæˆ
        if task_record:
            task_record.status = TaskStatus.COMPLETED.value
            task_record.total_items = total_reviews
            task_record.processed_items = processed
            db.commit()
        
        # [NOTE 2026-01-22] ç»´åº¦æ€»ç»“æ”¹ä¸ºç”¨æˆ·æ‰‹åŠ¨è§¦å‘ï¼ˆé€šè¿‡åˆ†äº«é¡µé¢çš„"ç”ŸæˆAIåˆ†æ"æŒ‰é’®ï¼‰
        # ä¸å†è‡ªåŠ¨è§¦å‘ï¼Œé¿å…åœ¨æ•°æ®ä¸å®Œæ•´æ—¶ç”Ÿæˆï¼ŒåŒæ—¶èŠ‚çœAIè°ƒç”¨æˆæœ¬
        
        return {
            "product_id": product_id,
            "total_reviews": total_reviews,
            "processed": processed,
            "themes_extracted": themes_extracted,
            "remaining": remaining  # [NEW] è¿”å›å‰©ä½™æœªå¤„ç†æ•°
        }
        
    except Exception as e:
        logger.error(f"Theme extraction failed for product {product_id}: {e}")
        # [NEW] æ›´æ–° Task çŠ¶æ€ä¸ºå¤±è´¥
        if task_record:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = str(e)
            db.commit()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡5: æµå¼è½»é‡ç¿»è¯‘ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_ingest_translation_only(self, product_id: str):
    """
    æµå¼è½»é‡ç¿»è¯‘ä»»åŠ¡ (Stream Translation Only)
    
    æ•°æ®å…¥åº“åç«‹å³è¿è¡Œï¼Œåªè´Ÿè´£ï¼š
    1. Title/BulletPoints ç¿»è¯‘
    2. Review Text ç¿»è¯‘
    
    ä¸è´Ÿè´£ï¼š
    - ç»´åº¦æå–
    - æ´å¯Ÿåˆ†æ
    - ä¸»é¢˜æå–
    
    è®¾è®¡ç†å¿µï¼šè®©ç”¨æˆ·åœ¨å‰å°"è¾¹é‡‡è¾¹çœ‹"ç¿»è¯‘ç»“æœ
    
    ğŸ”’ å¹¶å‘ç­–ç•¥ï¼šä½¿ç”¨ PostgreSQL è¡Œçº§é”ï¼ˆSELECT FOR UPDATE SKIP LOCKEDï¼‰
       - å¤šä¸ªä»»åŠ¡å¯ä»¥å¹¶å‘å¤„ç†åŒä¸€äº§å“çš„ä¸åŒè¯„è®º
       - è‡ªåŠ¨é¿å…é‡å¤å¤„ç†åŒä¸€æ¡è¯„è®º
       - æ— éœ€æ‰‹åŠ¨ç®¡ç†åˆ†å¸ƒå¼é”
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.services.translation import translation_service
    import json
    
    # ğŸš¦ æ…¢è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆæ›´å¤§çš„å»¶è¿Ÿï¼Œé¿å…ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.2, 1.0)
    logger.info(f"[ç¿»è¯‘ä»»åŠ¡] ğŸ¢ æ…¢è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}s")
    time.sleep(startup_delay)
    
    logger.info(f"[æµå¼ç¿»è¯‘] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    product_asin = None  # ç”¨äº finally ä¸­é‡Šæ”¾é”
    
    try:
        # 1. è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        product_asin = product.asin  # ä¿å­˜ asin ç”¨äºé‡Šæ”¾é”
        
        # 2. ç¿»è¯‘äº§å“æ ‡é¢˜ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.title and not product.title_translated:
            try:
                product.title_translated = translation_service.translate_product_title(product.title)
                logger.info(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å®Œæˆ: {product.title_translated[:30]}...")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
        
        # 3. ç¿»è¯‘äº”ç‚¹æè¿°ï¼ˆå¦‚æœæœªç¿»è¯‘ï¼‰
        if product.bullet_points and not product.bullet_points_translated:
            try:
                # Parse bullet points from JSON, PostgreSQL array, or Python list
                bullet_points = []
                if isinstance(product.bullet_points, list):
                    bullet_points = product.bullet_points
                elif isinstance(product.bullet_points, str):
                    bp_str = product.bullet_points.strip()
                    # å°è¯• JSON æ ¼å¼ [...]
                    if bp_str.startswith('['):
                        bullet_points = json.loads(bp_str)
                    # å¤„ç† PostgreSQL æ•°ç»„æ ¼å¼ {...}
                    elif bp_str.startswith('{') and bp_str.endswith('}'):
                        import re
                        content = bp_str[1:-1]  # ç§»é™¤ { }
                        matches = re.findall(r'"([^"]*)"', content)
                        if matches:
                            bullet_points = matches
                        else:
                            bullet_points = [s.strip() for s in content.split(',') if s.strip()]
                    else:
                        try:
                            bullet_points = json.loads(bp_str)
                        except:
                            bullet_points = [bp_str] if bp_str else []
                    
                if bullet_points and len(bullet_points) > 0:
                    translated_bullets = translation_service.translate_bullet_points(bullet_points)
                    # ç»Ÿä¸€ä¿å­˜ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
                    product.bullet_points_translated = json.dumps(translated_bullets, ensure_ascii=False)
                    logger.info(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å®Œæˆ: {len(translated_bullets)} æ¡")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] äº”ç‚¹ç¿»è¯‘å¤±è´¥: {e}")
        
        db.commit()
        
        # =========================================================================
        # 4. ğŸ¯ æ™ºèƒ½æ‰¹é‡ç¿»è¯‘ï¼ˆå·®å¼‚åŒ–å¤„ç†ï¼šVIPå•ç‹¬/æ ‡å‡†5æ¡/çŸ­è¯„20æ¡ï¼‰
        # =========================================================================
        # 
        # ç­–ç•¥ï¼š
        # - VIP è¯„è®ºï¼ˆ>200å­—æˆ–æç«¯æ˜Ÿçº§>100å­—ï¼‰ï¼šå•ç‹¬ç¿»è¯‘ï¼Œä¿è¯è´¨é‡
        # - æ ‡å‡†è¯„è®ºï¼ˆ50-200å­—ï¼‰ï¼š5 æ¡ä¸€æ‰¹
        # - çŸ­è¯„è®ºï¼ˆâ‰¤50å­—ï¼‰ï¼š20 æ¡ä¸€æ‰¹ï¼Œæœ€å¤§åŒ–æ•ˆç‡
        #
        # ä¼˜åŠ¿ï¼š
        # - è´¨é‡ä¿è¯ï¼šé‡è¦è¯„è®ºä¸é™ä½ç¿»è¯‘è´¨é‡
        # - æ•ˆç‡æœ€å¤§åŒ–ï¼šçŸ­è¯„è®º QPS æ¶ˆè€—é™ä½ 20 å€
        # - çµæ´»å¹³è¡¡ï¼šä¸­ç­‰è¯„è®ºå…¼é¡¾è´¨é‡å’Œæ•ˆç‡
        #
        translated_count = 0
        failed_count = 0
        
        # ç»Ÿè®¡ä¸åŒç±»åˆ«çš„å¤„ç†æƒ…å†µ
        category_stats = {
            'vip': {'total': 0, 'success': 0},
            'standard': {'total': 0, 'success': 0},
            'short': {'total': 0, 'success': 0}
        }
        
        # æ¯æ¬¡è·å–æ›´å¤šè¯„è®ºï¼ŒæŒ‰åˆ†ç±»å¤„ç†
        MAX_FETCH_SIZE = 100  # æ¯æ¬¡æœ€å¤šè·å– 100 æ¡å¾…ç¿»è¯‘è¯„è®º
        
        while True:
            # ğŸ”’ è·å–å¾…ç¿»è¯‘è¯„è®ºï¼ˆä½¿ç”¨ PostgreSQL è¡Œçº§é”ï¼‰
            # [FIXED] åªå¤„ç† pending çŠ¶æ€ï¼Œä¸å†è‡ªåŠ¨é‡è¯• failedï¼ˆé¿å…å†…å®¹å®¡æŸ¥å¤±è´¥æ— é™å¾ªç¯ï¼‰
            pending_result = db.execute(
                select(Review)
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.PENDING.value,
                        Review.is_deleted == False
                    )
                )
                .order_by(Review.created_at.desc())
                .limit(MAX_FETCH_SIZE)
                .with_for_update(skip_locked=True)
            )
            pending_reviews = pending_result.scalars().all()
            
            if not pending_reviews:
                logger.info(f"[æ™ºèƒ½ç¿»è¯‘] æ²¡æœ‰æ›´å¤šå¾…ç¿»è¯‘çš„è¯„è®º")
                break
            
            # ğŸ¯ æŒ‰é•¿åº¦å’Œè´¨é‡åˆ†ç±»
            grouped_reviews = ReviewClassifier.group_reviews(pending_reviews)
            
            logger.info(
                f"[æ™ºèƒ½ç¿»è¯‘] ğŸ“Š è¯„è®ºåˆ†ç±»: "
                f"VIP={len(grouped_reviews['vip'])} | "
                f"æ ‡å‡†={len(grouped_reviews['standard'])} | "
                f"çŸ­è¯„={len(grouped_reviews['short'])}"
            )
            
            # å¤„ç†é¡ºåºï¼šçŸ­è¯„ â†’ æ ‡å‡† â†’ VIPï¼ˆä¼˜å…ˆå¿«é€Ÿå¤„ç†å¤§é‡çŸ­è¯„ï¼‰
            for category in ['short', 'standard', 'vip']:
                reviews = grouped_reviews[category]
                if not reviews:
                    continue
                
                batch_size = ReviewClassifier.get_batch_size(category)
                category_stats[category]['total'] += len(reviews)
                
                logger.info(f"[æ™ºèƒ½ç¿»è¯‘] ğŸš€ å¤„ç† {category} ç±»è¯„è®º: {len(reviews)} æ¡ï¼Œæ‰¹é‡å¤§å°={batch_size}")
                
                # æŒ‰æ‰¹é‡å¤§å°åˆ†æ‰¹å¤„ç†
                for i in range(0, len(reviews), batch_size):
                    batch = reviews[i:i+batch_size]
                    
                    # æ ‡è®°ä¸ºå¤„ç†ä¸­
                    for review in batch:
                        review.translation_status = TranslationStatus.PROCESSING.value
                    db.commit()
                    
                    # æ„å»ºæ‰¹é‡ç¿»è¯‘è¯·æ±‚
                    batch_input = []
                    for review in batch:
                        text = review.body_original or ""
                        if text.strip():
                            batch_input.append({
                                "id": str(review.id),
                                "text": text
                            })
                    
                    # ğŸ”¥ æ‰¹é‡ç¿»è¯‘ï¼ˆVIP=1æ¡ï¼Œæ ‡å‡†=5æ¡ï¼ŒçŸ­è¯„=20æ¡ï¼‰
                    try:
                        if batch_size == 1:
                            # VIP è¯„è®ºï¼šå•ç‹¬ç¿»è¯‘
                            review = batch[0]
                            translated = translation_service.translate_text(review.body_original)
                            batch_results = {str(review.id): translated}
                        else:
                            # æ ‡å‡†/çŸ­è¯„ï¼šæ‰¹é‡ç¿»è¯‘
                            batch_results = translation_service.translate_batch_with_fallback(batch_input)
                        
                        logger.info(f"[æ™ºèƒ½ç¿»è¯‘] {category} æ‰¹æ¬¡ç¿»è¯‘å®Œæˆ: {len(batch_results)}/{len(batch)} æ¡")
                    except Exception as e:
                        logger.error(f"[æ™ºèƒ½ç¿»è¯‘] {category} æ‰¹æ¬¡ç¿»è¯‘å¤±è´¥: {e}")
                        batch_results = {}
                    
                    # æ‰¹é‡æ›´æ–°æ•°æ®åº“
                    for review in batch:
                        review_id_str = str(review.id)
                        
                        if review_id_str in batch_results and batch_results[review_id_str]:
                            # ç¿»è¯‘æˆåŠŸ
                            review.body_translated = batch_results[review_id_str]
                            
                            # æ ‡é¢˜å•ç‹¬ç¿»è¯‘
                            if review.title_original and not review.title_translated:
                                try:
                                    review.title_translated = translation_service.translate_text(review.title_original)
                                except:
                                    pass
                            
                            # æƒ…æ„Ÿåˆ†æ
                            try:
                                sentiment = translation_service.analyze_sentiment(review.body_translated)
                                review.sentiment = sentiment.value
                            except:
                                review.sentiment = "neutral"
                            
                            review.translation_status = TranslationStatus.COMPLETED.value
                            translated_count += 1
                            category_stats[category]['success'] += 1
                        else:
                            # ç¿»è¯‘å¤±è´¥
                            review.translation_status = TranslationStatus.FAILED.value
                            failed_count += 1
                    
                    # æäº¤æœ¬æ‰¹æ›´æ–°
                    db.commit()
                    
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å… QPS å†²é«˜
                    time.sleep(0.3 if batch_size == 1 else 0.5)
            
            # å¦‚æœè·å–çš„è¯„è®ºå°‘äº MAX_FETCH_SIZEï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šäº†
            if len(pending_reviews) < MAX_FETCH_SIZE:
                break
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(
            f"[æ™ºèƒ½ç¿»è¯‘] âœ… å®Œæˆ: æ€»è®¡ {translated_count} æ¡æˆåŠŸ, {failed_count} æ¡å¤±è´¥\n"
            f"  ğŸ“Š VIP è¯„è®º: {category_stats['vip']['success']}/{category_stats['vip']['total']} æ¡\n"
            f"  ğŸ“Š æ ‡å‡†è¯„è®º: {category_stats['standard']['success']}/{category_stats['standard']['total']} æ¡\n"
            f"  ğŸ“Š çŸ­è¯„è®º: {category_stats['short']['success']}/{category_stats['short']['total']} æ¡"
        )
        
        logger.info(f"[æµå¼ç¿»è¯‘] å®Œæˆ: ç¿»è¯‘ {translated_count} æ¡, å¤±è´¥ {failed_count} æ¡")
        
        # ğŸš€ ç¼“å­˜å¤±æ•ˆ - ç¿»è¯‘å®Œæˆåæ¸…é™¤äº§å“ç›¸å…³ç¼“å­˜
        if translated_count > 0:
            try:
                from app.core.cache import get_cache_service_sync
                cache = get_cache_service_sync()
                cache.invalidate_all_for_product(product.asin)
                logger.info(f"[Cache] Invalidated caches for product {product.asin} after translation")
            except Exception as cache_error:
                logger.warning(f"[Cache] Failed to invalidate cache: {cache_error}")
        
        return {
            "success": True,
            "product_id": product_id,
            "translated_count": translated_count,
            "failed_count": failed_count
        }
        
    except Exception as e:
        logger.error(f"[æµå¼ç¿»è¯‘] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()
        # ğŸ”’ PostgreSQL è¡Œçº§é”ä¼šåœ¨äº‹åŠ¡ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
        
        # ğŸ”“ é‡Šæ”¾ Redis ç¿»è¯‘ä»»åŠ¡é”ï¼Œå…è®¸åç»­æ–°è¯„è®ºè§¦å‘ç¿»è¯‘
        if product_asin:
            try:
                from app.core.redis import get_sync_redis
                redis_client = get_sync_redis()
                redis_client.delete(f"lock:translation:{product_asin}")
                logger.debug(f"[æµå¼ç¿»è¯‘] å·²é‡Šæ”¾äº§å“ {product_asin} çš„ç¿»è¯‘é”")
            except Exception as e:
                logger.warning(f"[æµå¼ç¿»è¯‘] é‡Šæ”¾ç¿»è¯‘é”å¤±è´¥: {e}")


# ============== [NEW] ä»»åŠ¡6: ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡« ==============

@celery_app.task(bind=True, max_retries=1, default_retry_delay=60)
def task_scientific_learning_and_analysis(self, product_id: str):
    """
    ç§‘å­¦å­¦ä¹ ä¸å…¨é‡å›å¡«ä»»åŠ¡ (Scientific Learning & Backfill)
    
    ç”¨æˆ·ç‚¹å‡»"å¼€å§‹åˆ†æ"æˆ–é‡‡é›†å®Œæˆåè§¦å‘ã€‚
    åˆ©ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œ Schema å­¦ä¹ ï¼Œç„¶åå›å¡«æ‰€æœ‰æ•°æ®ã€‚
    
    æµç¨‹ï¼š
    1. ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰å¾…ç¿»è¯‘ï¼‰
    2. è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ï¼ˆç»´åº¦ + 5Wæ ‡ç­¾ï¼‰
    3. å…¨é‡æ´å¯Ÿå›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–æ´å¯Ÿï¼‰
    4. å…¨é‡ä¸»é¢˜å›å¡«ï¼ˆå¯¹å·²ç¿»è¯‘çš„è¯„è®ºæå–5Wä¸»é¢˜ï¼‰
    
    Args:
        product_id: äº§å“ UUID
    """
    from app.models.product import Product
    from app.models.product_dimension import ProductDimension
    from app.models.product_context_label import ProductContextLabel
    from app.services.translation import translation_service
    import json as json_lib
    import asyncio
    
    logger.info(f"[ç§‘å­¦å­¦ä¹ ] å¼€å§‹å¤„ç†äº§å“ {product_id}")
    
    db = get_sync_db()
    
    try:
        # === Step 0: è·å–äº§å“ä¿¡æ¯ ===
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # === Step 1: ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 1: ç§‘å­¦é‡‡æ ·ä¸­...")
        
        # éœ€è¦åŒæ­¥æ–¹å¼æ‰§è¡Œå¼‚æ­¥æ–¹æ³•
        from app.services.review_service import ReviewService
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from app.core.config import settings
        from app.models.review import Review
        
        # ä½¿ç”¨åŒæ­¥æŸ¥è¯¢è·å–ç§‘å­¦é‡‡æ ·
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        # [UPDATED 2026-01-19] ç§»é™¤æœ€ä½æ ·æœ¬æ•°é™åˆ¶ï¼Œåªè¦æœ‰è¯„è®ºå°±è¿›è¡Œå­¦ä¹ 
        if len(raw_samples) < 1:
            logger.warning(f"[ç§‘å­¦å­¦ä¹ ] æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼Œè·³è¿‡å­¦ä¹ ")
            return {"success": False, "error": "æ²¡æœ‰å¯ç”¨è¯„è®ºæ ·æœ¬"}
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] æ ·æœ¬æ•°é‡: {len(raw_samples)} æ¡è‹±æ–‡è¯„è®º")
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] é‡‡æ ·å®Œæˆ: {len(raw_samples)} æ¡é«˜è´¨é‡è‹±æ–‡è¯„è®º")
        
        # === Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹  ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 2: è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ä¸­...")
        
        # 2.1 å­¦ä¹ ç»´åº¦ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        # [UPDATED 2026-01-16] æ”¯æŒ3ç±»ç»´åº¦ä½“ç³»
        dim_count_result = db.execute(
            select(func.count(ProductDimension.id))
            .where(ProductDimension.product_id == product_id)
        )
        dim_count = dim_count_result.scalar() or 0
        
        dimensions_learned = 0
        if dim_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ 3ç±»äº§å“ç»´åº¦ä¸­...")
            
            # [FIX 2026-01-19] å¢åŠ é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
            dims_result = None
            max_retries = 3
            for attempt in range(max_retries):
                dims_result = translation_service.learn_dimensions_from_raw(
                    raw_reviews=raw_samples,
                    product_title=product_title,
                    bullet_points="\n".join(bullet_points) if bullet_points else ""
                )
                if dims_result and isinstance(dims_result, dict):
                    break  # å­¦ä¹ æˆåŠŸ
                logger.warning(f"[ç§‘å­¦å­¦ä¹ ] ç»´åº¦å­¦ä¹ ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œ"
                              f"{'é‡è¯•ä¸­...' if attempt < max_retries - 1 else 'å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°'}")
                if attempt < max_retries - 1:
                    time.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•
            
            # [UPDATED 2026-01-16] è§£æ3ç±»ç»´åº¦å¹¶ä¿å­˜
            if dims_result and isinstance(dims_result, dict):
                # æ–°æ ¼å¼ï¼š3ç±»ç»´åº¦
                for dim_type in ["product", "scenario", "emotion"]:
                    type_dims = dims_result.get(dim_type, [])
                    for dim in type_dims:
                        if isinstance(dim, dict) and dim.get("name"):
                            dimension = ProductDimension(
                                product_id=product_id,
                                name=dim["name"].strip(),
                                description=dim.get("description", "").strip() or None,
                                dimension_type=dim_type,  # [NEW] è®¾ç½®ç»´åº¦ç±»å‹
                                is_ai_generated=True
                            )
                            db.add(dimension)
                            dimensions_learned += 1
                db.commit()
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] 3ç±»ç»´åº¦å­¦ä¹ å®Œæˆ: {dimensions_learned} ä¸ª "
                           f"(äº§å“:{len(dims_result.get('product', []))}, "
                           f"åœºæ™¯:{len(dims_result.get('scenario', []))}, "
                           f"æƒ…ç»ª:{len(dims_result.get('emotion', []))})")
            elif dims_result and isinstance(dims_result, list):
                # å‘åå…¼å®¹ï¼šæ—§æ ¼å¼ï¼ˆå•ä¸€åˆ—è¡¨ï¼‰
                for dim in dims_result:
                    dimension = ProductDimension(
                        product_id=product_id,
                        name=dim["name"],
                        description=dim.get("description", ""),
                        dimension_type="product",  # é»˜è®¤ä¸ºäº§å“ç»´åº¦
                        is_ai_generated=True
                    )
                    db.add(dimension)
                    dimensions_learned += 1
                db.commit()
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] ç»´åº¦å­¦ä¹ å®Œæˆ(æ—§æ ¼å¼): {dimensions_learned} ä¸ª")
            else:
                # [FIX 2026-01-19] ç»´åº¦å­¦ä¹ å¤±è´¥ï¼Œé˜»æ–­æµç¨‹
                logger.error(f"[ç§‘å­¦å­¦ä¹ ] âŒ ç»´åº¦å­¦ä¹ å¤±è´¥ï¼ˆé‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥ï¼‰ï¼Œé˜»æ–­åç»­æµç¨‹")
                raise ValueError(f"ç»´åº¦å­¦ä¹ å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†ææµç¨‹ã€‚è¯·æ£€æŸ¥ AI æœåŠ¡æˆ–é‡è¯•ã€‚")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {dim_count} ä¸ªç»´åº¦ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # 2.2 å­¦ä¹ 5Wæ ‡ç­¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        label_count_result = db.execute(
            select(func.count(ProductContextLabel.id))
            .where(ProductContextLabel.product_id == product_id)
        )
        label_count = label_count_result.scalar() or 0
        
        labels_learned = 0
        if label_count == 0:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
            labels = translation_service.learn_context_labels_from_raw(
                raw_reviews=raw_samples,
                product_title=product_title,
                bullet_points=bullet_points
            )
            
            if labels:
                # [UPDATED 2026-01-14] æ”¯æŒ buyer/user æ‹†åˆ†
                for context_type in ["buyer", "user", "who", "where", "when", "why", "what"]:
                    type_labels = labels.get(context_type, [])
                    for item in type_labels:
                        if isinstance(item, dict) and item.get("name"):
                            label = ProductContextLabel(
                                product_id=product_id,
                                type=context_type,
                                name=item["name"].strip(),
                                description=item.get("description", "").strip() or None,
                                count=0,
                                is_ai_generated=True
                            )
                            db.add(label)
                            labels_learned += 1
                db.commit()
                logger.info(f"[ç§‘å­¦å­¦ä¹ ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_learned} ä¸ª")
        else:
            logger.info(f"[ç§‘å­¦å­¦ä¹ ] äº§å“å·²æœ‰ {label_count} ä¸ª5Wæ ‡ç­¾ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # === Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 3: è§¦å‘å…¨é‡æ´å¯Ÿå›å¡«...")
        task_extract_insights.delay(product_id)
        
        # === Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡« ===
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] Step 4: è§¦å‘å…¨é‡ä¸»é¢˜å›å¡«...")
        task_extract_themes.delay(product_id)
        
        logger.info(f"[ç§‘å­¦å­¦ä¹ ] å®Œæˆ: ç»´åº¦ +{dimensions_learned}, æ ‡ç­¾ +{labels_learned}")
        
        return {
            "success": True,
            "product_id": product_id,
            "samples_used": len(raw_samples),
            "dimensions_learned": dimensions_learned,
            "labels_learned": labels_learned,
            "backfill_triggered": True
        }
        
    except Exception as e:
        logger.error(f"[ç§‘å­¦å­¦ä¹ ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡7: å…¨è‡ªåŠ¨åˆ†æï¼ˆé‡‡é›†å®Œæˆåè§¦å‘ï¼‰==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=120)
def task_full_auto_analysis(self, product_id: str, task_id: str):
    """
    ğŸš€ å…¨è‡ªåŠ¨åˆ†æä»»åŠ¡ (Full Auto Analysis Pipeline) - æµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆ
    
    é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆ†ææµæ°´çº¿ã€‚
    
    â­ æ ¸å¿ƒä¼˜åŒ–ï¼šç¿»è¯‘åœ¨ ingest æ—¶å°±å·²å¼€å§‹ï¼ˆæµå¼ä¸Šä¼ è¾¹å­˜è¾¹è¯‘ï¼‰
    
    æµå¼å¹¶è¡Œæµç¨‹ï¼š
    
    [æ•°æ®é‡‡é›†é˜¶æ®µ - åœ¨æ­¤ä»»åŠ¡è§¦å‘ä¹‹å‰]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘ï¼ˆtask_ingest_translation_onlyï¼‰
    æ’å…¥æ•°æ® â†’ ç«‹å³è§¦å‘ç¿»è¯‘
    ...ï¼ˆæŒç»­è¿›è¡Œä¸­ï¼‰
    
    [é‡‡é›†å®Œæˆåè§¦å‘æ­¤ä»»åŠ¡]
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Step 1: å­¦ä¹ ç»´åº¦+5Wæ ‡ç­¾ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ç­‰ç¿»è¯‘ï¼‰
                              â†“
    Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–ï¼ˆç¿»è¯‘æ­¤æ—¶å·²åœ¨è¿›è¡Œä¸­ï¼ï¼‰
                              â†“
    Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆ
            â”œâ”€ ç¿»è¯‘ï¼ˆå·²åœ¨è¿›è¡Œï¼Œä¼šå…ˆå®Œæˆï¼‰
            â”œâ”€ æ´å¯Ÿæå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
            â””â”€ ä¸»é¢˜æå–ï¼ˆè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
                              â†“
    Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
    
    æ—¶é—´ä¼˜åŒ–ï¼š
    - ç¿»è¯‘åœ¨é‡‡é›†æ—¶å°±å¼€å§‹ â†’ ä¸ç­‰å¾…
    - å­¦ä¹ åŸºäºè‹±æ–‡åŸæ–‡ â†’ ä¸ä¾èµ–ç¿»è¯‘
    - ä¸‰ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ â†’ å¤§å¹…å‡å°‘ç­‰å¾…æ—¶é—´
    - é¢„è®¡èŠ‚çœ 50%+ çš„æ€»æ—¶é—´
    
    Args:
        product_id: äº§å“ UUID
        task_id: AUTO_ANALYSIS ä»»åŠ¡ UUIDï¼ˆç”¨äºæ›´æ–°è¿›åº¦ï¼‰
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    from app.models.task import Task, TaskStatus, TaskType
    from app.models.insight import ReviewInsight
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.report import ProductReport, ReportType, ReportStatus
    from app.services.translation import translation_service
    from datetime import datetime, timezone
    import json as json_lib
    
    # ğŸš¦ VIP å¿«è½¦é“ï¼šå¯åŠ¨éšæœºå»¶è¿Ÿï¼ˆé¿å… Worker é‡å¯æ—¶ç¬é—´å†²é«˜ QPSï¼‰
    startup_delay = random.uniform(0.1, 0.5)
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸŒŸ VIP å¿«è½¦é“å¯åŠ¨ï¼Œå»¶è¿Ÿ {startup_delay:.2f}sï¼ˆé˜²æ­¢ QPS å†²é«˜ï¼‰")
    time.sleep(startup_delay)
    
    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸš€ å¼€å§‹å¤„ç†äº§å“ {product_id}ï¼Œä»»åŠ¡ {task_id}")
    
    db = get_sync_db()
    
    def update_task_progress(step: int, status: str = TaskStatus.PROCESSING.value, error: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            task_update = {
                "processed_items": step,
                "status": status,
                "last_heartbeat": datetime.now(timezone.utc)
            }
            if error:
                task_update["error_message"] = error
            db.execute(
                update(Task)
                .where(Task.id == task_id)
                .values(**task_update)
            )
            db.commit()
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
    
    try:
        # ==========================================
        # Step 0: ç­‰å¾…å…¥åº“é˜Ÿåˆ—æ¸…ç©ºï¼ˆç¡®ä¿æ‰€æœ‰è¯„è®ºéƒ½å·²å…¥åº“ï¼‰
        # ==========================================
        # ğŸ›¡ï¸ é˜²æŠ¤æœºåˆ¶ï¼šé¿å…å› æ—¶åºç«æ€å¯¼è‡´è¯„è®ºé—æ¼
        from app.core.redis import ReviewIngestionQueueSync, get_sync_redis
        redis_cli = get_sync_redis()
        queue = ReviewIngestionQueueSync(redis_cli)
        
        max_wait = 60  # æœ€å¤šç­‰å¾… 60 ç§’
        waited = 0
        while waited < max_wait:
            queue_len = queue.length()
            if queue_len == 0:
                logger.info("[å…¨è‡ªåŠ¨åˆ†æ] âœ… å…¥åº“é˜Ÿåˆ—å·²æ¸…ç©ºï¼Œæ‰€æœ‰è¯„è®ºå·²å…¥åº“")
                break
            logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] â³ ç­‰å¾…å…¥åº“é˜Ÿåˆ—æ¸…ç©º... å‰©ä½™ {queue_len} æ¡")
            time.sleep(5)
            waited += 5
        
        # é¢å¤–ç­‰å¾… 5 ç§’ï¼Œç¡®ä¿æ•°æ®åº“äº‹åŠ¡å®Œå…¨æäº¤
        if waited > 0:
            logger.info("[å…¨è‡ªåŠ¨åˆ†æ] â³ ç­‰å¾…äº‹åŠ¡æäº¤...")
            time.sleep(5)
        
        # è·å–äº§å“ä¿¡æ¯
        product_result = db.execute(
            select(Product).where(Product.id == product_id)
        )
        product = product_result.scalar_one_or_none()
        
        if not product:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} ä¸å­˜åœ¨")
            update_task_progress(0, TaskStatus.FAILED.value, "äº§å“ä¸å­˜åœ¨")
            return {"success": False, "error": "Product not found"}
        
        # ==========================================
        # Step 1: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼Œä¸ä¾èµ–ç¿»è¯‘ï¼ï¼‰
        # ==========================================
        update_task_progress(1, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 1/3: ç§‘å­¦å­¦ä¹ ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰...")
        
        # ç›´æ¥è°ƒç”¨ç§‘å­¦å­¦ä¹ ä»»åŠ¡çš„é€»è¾‘ï¼ˆåŒæ­¥æ‰§è¡Œï¼‰
        from app.models.product_dimension import ProductDimension
        from app.models.product_context_label import ProductContextLabel
        
        # è§£æäº§å“ä¿¡æ¯
        product_title = product.title or ""
        bullet_points = []
        if product.bullet_points:
            try:
                bullet_points = json_lib.loads(product.bullet_points) if isinstance(product.bullet_points, str) else product.bullet_points
            except:
                bullet_points = []
        
        # ç§‘å­¦é‡‡æ ·ï¼ˆåŸºäºè‹±æ–‡åŸæ–‡ï¼‰
        sample_stmt = (
            select(Review.body_original)
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.body_original.isnot(None),
                    Review.body_original != "",
                    Review.is_deleted == False
                )
            )
            .order_by(Review.helpful_votes.desc(), func.length(Review.body_original).desc())
            .limit(50)
        )
        sample_result = db.execute(sample_stmt)
        raw_samples = [r[0] for r in sample_result.all() if r[0] and r[0].strip()]
        
        # [UPDATED 2026-01-19] ç§»é™¤æœ€ä½æ ·æœ¬æ•°é™åˆ¶ï¼Œåªè¦æœ‰è¯„è®ºå°±è¿›è¡Œå­¦ä¹ 
        if len(raw_samples) >= 1:
            logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] æ ·æœ¬æ•°é‡: {len(raw_samples)} æ¡è‹±æ–‡è¯„è®º")
            # å­¦ä¹ ç»´åº¦
            dim_count_result = db.execute(
                select(func.count(ProductDimension.id))
                .where(ProductDimension.product_id == product_id)
            )
            dim_count = dim_count_result.scalar() or 0
            
            if dim_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ 3ç±»äº§å“ç»´åº¦ä¸­...")
                
                # [FIX 2026-01-19] å¢åŠ é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
                dims_result = None
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        dims_result = translation_service.learn_dimensions_from_raw(
                            raw_reviews=raw_samples,
                            product_title=product_title,
                            bullet_points="\n".join(bullet_points) if bullet_points else ""
                        )
                        if dims_result and isinstance(dims_result, dict):
                            break  # å­¦ä¹ æˆåŠŸ
                        logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ ç¬¬ {attempt + 1} æ¬¡å¤±è´¥ï¼Œ"
                                      f"{'é‡è¯•ä¸­...' if attempt < max_retries - 1 else 'å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°'}")
                    except Exception as e:
                        logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ ç¬¬ {attempt + 1} æ¬¡å¼‚å¸¸: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•
                
                # [UPDATED 2026-01-16] æ”¯æŒ3ç±»ç»´åº¦ä½“ç³»
                dimensions_learned = 0
                if dims_result and isinstance(dims_result, dict):
                    # æ–°æ ¼å¼ï¼š3ç±»ç»´åº¦
                    for dim_type in ["product", "scenario", "emotion"]:
                        type_dims = dims_result.get(dim_type, [])
                        for dim in type_dims:
                            if isinstance(dim, dict) and dim.get("name"):
                                dimension = ProductDimension(
                                    product_id=product_id,
                                    name=dim["name"].strip(),
                                    description=dim.get("description", "").strip() or None,
                                    dimension_type=dim_type,
                                    is_ai_generated=True
                                )
                                db.add(dimension)
                                dimensions_learned += 1
                    db.commit()
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] 3ç±»ç»´åº¦å­¦ä¹ å®Œæˆ: {dimensions_learned} ä¸ª "
                               f"(äº§å“:{len(dims_result.get('product', []))}, "
                               f"åœºæ™¯:{len(dims_result.get('scenario', []))}, "
                               f"æƒ…ç»ª:{len(dims_result.get('emotion', []))})")
                elif dims_result and isinstance(dims_result, list):
                    # å‘åå…¼å®¹ï¼šæ—§æ ¼å¼
                    for dim in dims_result:
                        dimension = ProductDimension(
                            product_id=product_id,
                            name=dim["name"],
                            description=dim.get("description", ""),
                            dimension_type="product",
                            is_ai_generated=True
                        )
                        db.add(dimension)
                        dimensions_learned += 1
                    db.commit()
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»´åº¦å­¦ä¹ å®Œæˆ(æ—§æ ¼å¼): {dimensions_learned} ä¸ª")
                else:
                    # [FIX 2026-01-19] ç»´åº¦å­¦ä¹ å¤±è´¥ï¼Œé˜»æ–­æµç¨‹
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] âŒ ç»´åº¦å­¦ä¹ å¤±è´¥ï¼ˆé‡è¯• {max_retries} æ¬¡åä»ç„¶å¤±è´¥ï¼‰ï¼Œé˜»æ–­åç»­æµç¨‹")
                    raise ValueError(f"ç»´åº¦å­¦ä¹ å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†ææµç¨‹ã€‚è¯·æ£€æŸ¥ AI æœåŠ¡æˆ–é‡è¯•ã€‚")
            
            # å­¦ä¹ 5Wæ ‡ç­¾
            label_count_result = db.execute(
                select(func.count(ProductContextLabel.id))
                .where(ProductContextLabel.product_id == product_id)
            )
            label_count = label_count_result.scalar() or 0
            
            if label_count == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å­¦ä¹ 5Wæ ‡ç­¾åº“ä¸­...")
                try:
                    labels = translation_service.learn_context_labels_from_raw(
                        raw_reviews=raw_samples,
                        product_title=product_title,
                        bullet_points=bullet_points
                    )
                    if labels:
                        labels_saved = 0
                        # [UPDATED 2026-01-14] æ”¯æŒ buyer/user æ‹†åˆ†
                        for context_type in ["buyer", "user", "who", "where", "when", "why", "what"]:
                            type_labels = labels.get(context_type, [])
                            for item in type_labels:
                                if isinstance(item, dict) and item.get("name"):
                                    label = ProductContextLabel(
                                        product_id=product_id,
                                        type=context_type,
                                        name=item["name"].strip(),
                                        description=item.get("description", "").strip() or None,
                                        count=0,
                                        is_ai_generated=True
                                    )
                                    db.add(label)
                                    labels_saved += 1
                        db.commit()
                        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å®Œæˆ: {labels_saved} ä¸ª")
                except Exception as e:
                    logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] 5Wæ ‡ç­¾å­¦ä¹ å¤±è´¥: {e}")
        else:
            logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] æ²¡æœ‰å¯ç”¨æ ·æœ¬ï¼Œè·³è¿‡å­¦ä¹ ")
        
        # ==========================================
        # Step 2: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–
        # æ³¨æ„ï¼šç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²ç»å¯åŠ¨äº†ï¼ä¸éœ€è¦åœ¨è¿™é‡Œè§¦å‘
        # ==========================================
        update_task_progress(2, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 2/4: è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–...")
        
        # æ£€æŸ¥å½“å‰ç¿»è¯‘è¿›åº¦ï¼ˆç¿»è¯‘åœ¨ ingest æ—¶å°±å·²ç»å¼€å§‹äº†ï¼‰
        pending_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status.in_([
                        TranslationStatus.PENDING.value,
                        TranslationStatus.PROCESSING.value
                    ]),
                    Review.is_deleted == False
                )
            )
        )
        pending_translation = pending_result.scalar() or 0
        
        translated_result = db.execute(
            select(func.count(Review.id))
            .where(
                and_(
                    Review.product_id == product_id,
                    Review.translation_status == TranslationStatus.COMPLETED.value,
                    Review.is_deleted == False
                )
            )
        )
        translated_count = translated_result.scalar() or 0
        
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å½“å‰ç¿»è¯‘çŠ¶æ€: å·²ç¿»è¯‘ {translated_count} æ¡, å¾…ç¿»è¯‘ {pending_translation} æ¡")
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ’¡ ç¿»è¯‘ä»»åŠ¡åœ¨ ingest æ—¶å°±å·²å¯åŠ¨ï¼Œç°åœ¨è§¦å‘æ´å¯Ÿ+ä¸»é¢˜æå–")
        
        # è§¦å‘æ´å¯Ÿå’Œä¸»é¢˜æå–ï¼ˆå®ƒä»¬ä¼šå¤„ç†å·²ç¿»è¯‘çš„è¯„è®ºï¼Œè¾¹ç¿»è¯‘è¾¹æå–ï¼‰
        task_extract_insights.delay(product_id)
        task_extract_themes.delay(product_id)
        
        # ==========================================
        # Step 3: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘ + æ´å¯Ÿ + ä¸»é¢˜ï¼‰
        # ==========================================
        update_task_progress(3, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 3/4: ç­‰å¾…ä¸‰ä»»åŠ¡å¹¶è¡Œå®Œæˆï¼ˆç¿»è¯‘+æ´å¯Ÿ+ä¸»é¢˜ï¼‰...")
        
        # å¹¶è¡Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰ 30 åˆ†é’Ÿï¼Œä» 15 åˆ†é’Ÿæå‡ï¼‰
        max_wait_seconds = 1800  # ğŸ”¥ ä» 900 ç§’ï¼ˆ15 åˆ†é’Ÿï¼‰æå‡åˆ° 1800 ç§’ï¼ˆ30 åˆ†é’Ÿï¼‰
        wait_interval = 15
        waited = 0
        last_log_time = 0
        
        while waited < max_wait_seconds:
            time.sleep(wait_interval)
            waited += wait_interval
            
            # æ£€æŸ¥ç¿»è¯‘è¿›åº¦
            pending_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status.in_([
                            TranslationStatus.PENDING.value,
                            TranslationStatus.PROCESSING.value
                        ]),
                        Review.is_deleted == False
                    )
                )
            )
            pending_translation = pending_result.scalar() or 0
            
            # æ£€æŸ¥å·²ç¿»è¯‘çš„è¯„è®ºæ•°
            translated_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False
                    )
                )
            )
            translated_count = translated_result.scalar() or 0
            
            # æ£€æŸ¥æ´å¯Ÿæå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–æ´å¯Ÿçš„è¯„è®ºï¼‰
            no_insight_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewInsight.review_id).where(ReviewInsight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_insights = no_insight_result.scalar() or 0
            
            # æ£€æŸ¥ä¸»é¢˜æå–è¿›åº¦ï¼ˆå·²ç¿»è¯‘ä½†æœªæå–ä¸»é¢˜çš„è¯„è®ºï¼‰
            no_theme_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.translation_status == TranslationStatus.COMPLETED.value,
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewThemeHighlight.review_id).where(ReviewThemeHighlight.review_id == Review.id)
                        )
                    )
                )
            )
            pending_themes = no_theme_result.scalar() or 0
            
            # æ¯30ç§’æ‰“å°ä¸€æ¬¡è¿›åº¦
            if waited - last_log_time >= 30:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ“Š å¹¶è¡Œè¿›åº¦ - å¾…ç¿»è¯‘:{pending_translation} | å¾…æ´å¯Ÿ:{pending_insights} | å¾…ä¸»é¢˜:{pending_themes}")
                last_log_time = waited
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%å®Œæˆåº¦ï¼Œå¯ä»¥æå‰è§¦å‘æŠ¥å‘Šç”Ÿæˆ
            insights_completion = (translated_count - pending_insights) / translated_count if translated_count > 0 else 0
            themes_completion = (translated_count - pending_themes) / translated_count if translated_count > 0 else 0
            
            # ğŸš€ ä¼˜åŒ–ï¼š90%å®Œæˆåº¦å³å¯è§¦å‘æŠ¥å‘Šç”Ÿæˆ
            if insights_completion >= 0.90 and themes_completion >= 0.90:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… è¾¾åˆ°90%å®Œæˆåº¦ï¼Œè§¦å‘æŠ¥å‘Šç”Ÿæˆï¼æ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}")
                break
            
            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆï¼ˆ100%ï¼‰
            if pending_translation == 0 and pending_insights == 0 and pending_themes == 0:
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… å¹¶è¡Œå¤„ç†å…¨éƒ¨å®Œæˆï¼å·²ç¿»è¯‘:{translated_count}æ¡")
                break
            
            # [OPTIMIZED] æ¯ 120 ç§’æ£€æŸ¥ä¸€æ¬¡ï¼Œåªåœ¨è¿›åº¦åœæ»æ—¶é‡æ–°è§¦å‘
            # é¿å…é¢‘ç¹è§¦å‘å¯¼è‡´ä»»åŠ¡å †ç§¯ï¼Œå½±å“å…¶ä»–ç”¨æˆ·
            if waited % 120 == 0 and waited > 0:
                # ç¿»è¯‘ä»»åŠ¡ï¼šåªåœ¨æœ‰å¤§é‡å¾…ç¿»è¯‘æ—¶è§¦å‘
                if pending_translation > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ğŸ”„ é‡æ–°è§¦å‘ç¿»è¯‘ä»»åŠ¡ï¼ˆè¿˜æœ‰{pending_translation}æ¡å¾…å¤„ç†ï¼‰")
                    task_ingest_translation_only.delay(product_id)
                # æ´å¯Ÿ/ä¸»é¢˜ï¼šåªè§¦å‘ä¸€ä¸ªï¼Œé¿å…å ç”¨å¤ªå¤šèµ„æº
                if pending_insights > 10:
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘æ´å¯Ÿæå–ï¼ˆè¿˜æœ‰{pending_insights}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_insights.delay(product_id)
                elif pending_themes > 10:  # ç”¨ elif é¿å…åŒæ—¶è§¦å‘
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] é‡æ–°è§¦å‘ä¸»é¢˜æå–ï¼ˆè¿˜æœ‰{pending_themes}æ¡å¾…å¤„ç†ï¼‰")
                    task_extract_themes.delay(product_id)
            
            # æ›´æ–°å¿ƒè·³
            update_task_progress(3, TaskStatus.PROCESSING.value)
        
        if waited >= max_wait_seconds:
            # ğŸ”¥ ä¼˜åŒ–ï¼šæ”¾å®½å®Œæˆåº¦è¦æ±‚ï¼Œä» 95% é™åˆ° 80%
            # ç†ç”±ï¼š80% å·²è¶³å¤Ÿç”Ÿæˆé«˜è´¨é‡æŠ¥å‘Šï¼Œå‰©ä½™ä»»åŠ¡å¯å¼‚æ­¥ç»§ç»­
            insights_completion = (translated_count - pending_insights) / translated_count if translated_count > 0 else 0
            themes_completion = (translated_count - pending_themes) / translated_count if translated_count > 0 else 0
            
            if insights_completion < 0.80 or themes_completion < 0.80:
                logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] âš ï¸ ç­‰å¾…è¶…æ—¶ä¸”å®Œæˆåº¦ <80%ï¼ˆæ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}ï¼‰")
                update_task_progress(3, TaskStatus.FAILED.value, f"å¤„ç†è¶…æ—¶ï¼Œæ´å¯Ÿå®Œæˆåº¦:{insights_completion:.0%}ï¼Œä¸»é¢˜å®Œæˆåº¦:{themes_completion:.0%}")
                return {
                    "success": False,
                    "product_id": product_id,
                    "task_id": task_id,
                    "error": f"å¹¶è¡Œå¤„ç†è¶…æ—¶ä¸”å®Œæˆåº¦ä¸è¶³80%ï¼Œè¯·ç¨åé‡è¯•ã€‚æ´å¯Ÿ:{insights_completion:.0%}ï¼Œä¸»é¢˜:{themes_completion:.0%}"
                }
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] å¹¶è¡Œå¤„ç†ç­‰å¾…è¶…æ—¶ï¼Œä½†å®Œæˆåº¦è¾¾åˆ°80%ä»¥ä¸Šï¼Œç»§ç»­ç”ŸæˆæŠ¥å‘Šï¼ˆæ´å¯Ÿ:{insights_completion:.0%}, ä¸»é¢˜:{themes_completion:.0%}ï¼‰")
        
        # ==========================================
        # Step 4: ç”Ÿæˆç»¼åˆæˆ˜ç•¥ç‰ˆæŠ¥å‘Š
        # ==========================================
        update_task_progress(4, TaskStatus.PROCESSING.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] Step 4/4: ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
        
        try:
            # ä½¿ç”¨åŒæ­¥æ–¹å¼è°ƒç”¨æŠ¥å‘Šç”Ÿæˆ
            # ç”±äº SummaryService æ˜¯å¼‚æ­¥çš„ï¼Œéœ€è¦ä½¿ç”¨ asyncio
            import asyncio
            from app.services.summary_service import SummaryService
            
            async def generate_report_async():
                # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥ï¼šengine å’Œ async_session_maker
                from app.db.session import async_session_maker
                
                async with async_session_maker() as async_db:
                    summary_service = SummaryService(async_db)
                    result = await summary_service.generate_report(
                        product_id=product_id,
                        report_type="comprehensive",  # ç»¼åˆæˆ˜ç•¥ç‰ˆ
                        min_reviews=30,  # [UPDATED 2026-01-19] æŠ¥å‘Šéœ€è¦è‡³å°‘30æ¡è¯„è®º
                        save_to_db=True,
                        force_regenerate=False,  # [NEW] ä¸å¼ºåˆ¶é‡æ–°ç”Ÿæˆï¼Œæ£€æŸ¥å»é‡
                        require_full_completion=False  # [ä¼˜åŒ–] å…è®¸90%å®Œæˆåº¦ç”ŸæˆæŠ¥å‘Š
                    )
                    await async_db.commit()  # ç¡®ä¿æäº¤
                    return result
            
            # è¿è¡Œå¼‚æ­¥å‡½æ•° - ä¿®å¤äº‹ä»¶å¾ªç¯é—®é¢˜
            try:
                report_result = asyncio.run(generate_report_async())
            except RuntimeError:
                # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    report_result = loop.run_until_complete(generate_report_async())
                finally:
                    pending = asyncio.all_tasks(loop)
                    for task in pending:
                        task.cancel()
                    if pending:
                        loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
                    loop.close()
            
            if report_result.get("success"):
                report_id = report_result.get("report_id")
                logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼ŒæŠ¥å‘ŠID: {report_id}")
                
                # æ›´æ–°ä»»åŠ¡è®°å½•ï¼Œä¿å­˜æŠ¥å‘Š ID
                try:
                    db.execute(
                        update(Task)
                        .where(Task.id == task_id)
                        .values(error_message=f"report_id:{report_id}")  # ä¸´æ—¶å­˜å‚¨æŠ¥å‘ŠID
                    )
                    db.commit()
                except Exception as save_err:
                    logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ä¿å­˜æŠ¥å‘ŠIDå¤±è´¥: {save_err}")
            else:
                logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] ç»¼åˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {report_result.get('error')}")
                
        except Exception as e:
            logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            # ä¸å› æŠ¥å‘Šç”Ÿæˆå¤±è´¥è€Œä¸­æ–­æ•´ä¸ªä»»åŠ¡
        
        # ==========================================
        # å®Œæˆ
        # ==========================================
        update_task_progress(4, TaskStatus.COMPLETED.value)
        logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] âœ… äº§å“ {product_id} å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ï¼ˆæµå¼å¹¶è¡Œä¼˜åŒ–ç‰ˆï¼‰")
        
        # æ¸…ç†ç›¸å…³åˆ†äº«é“¾æ¥çš„ç¼“å­˜ï¼ˆä½¿åˆ†äº«é¡µé¢è·å–æœ€æ–°æ•°æ®ï¼‰
        try:
            from app.models.share_link import ShareLink
            from app.core.redis import get_redis
            share_links = db.query(ShareLink).filter(
                ShareLink.product_id == product_id,
                ShareLink.is_active == True
            ).all()
            if share_links:
                redis = get_redis()
                for link in share_links:
                    cache_key = f"cache:share:data:{link.token}"
                    redis.delete(cache_key)
                    logger.info(f"[å…¨è‡ªåŠ¨åˆ†æ] å·²æ¸…ç†åˆ†äº«ç¼“å­˜: {cache_key}")
        except Exception as cache_err:
            logger.warning(f"[å…¨è‡ªåŠ¨åˆ†æ] æ¸…ç†åˆ†äº«ç¼“å­˜å¤±è´¥: {cache_err}")
        
        return {
            "success": True,
            "product_id": product_id,
            "task_id": task_id,
            "message": "å…¨è‡ªåŠ¨åˆ†æå®Œæˆï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰"
        }
        
    except Exception as e:
        logger.error(f"[å…¨è‡ªåŠ¨åˆ†æ] äº§å“ {product_id} å¤„ç†å¤±è´¥: {e}")
        update_task_progress(0, TaskStatus.FAILED.value, str(e))
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡8: å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ ==============

@celery_app.task(bind=True, max_retries=0)
def task_check_pending_translations(self):
    """
    ğŸ”„ å®šæ—¶æ£€æŸ¥å¾…ç¿»è¯‘ä»»åŠ¡ (Periodic Translation Check)
    
    æ¯ 15 ç§’ç”± Celery Beat è§¦å‘ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¾…ç¿»è¯‘çš„äº§å“ã€‚
    å¦‚æœæœ‰ï¼Œä¸ºæ¯ä¸ªäº§å“è§¦å‘ 3 ä¸ªå¹¶è¡Œç¿»è¯‘ä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨å¤š Worker å¹¶å‘ã€‚
    
    è®¾è®¡ç†å¿µï¼š
    - ç¿»è¯‘ä»»åŠ¡ä½¿ç”¨è¡Œçº§é”ï¼ˆSKIP LOCKEDï¼‰ï¼Œå¤šä»»åŠ¡å¯ä»¥å®‰å…¨å¹¶å‘
    - è§¦å‘å¤šä¸ªä»»åŠ¡è®© 6 ä¸ª Worker çº¿ç¨‹éƒ½æœ‰æ´»å¹²
    - é¿å…ç¿»è¯‘å› è¡Œçº§é”ç«äº‰è€Œæå‰ç»“æŸ
    """
    from app.models.product import Product
    from app.models.review import Review, TranslationStatus
    
    db = get_sync_db()
    
    try:
        # æŸ¥æ‰¾æœ‰å¾…ç¿»è¯‘è¯„è®ºçš„äº§å“ï¼ˆæœ€å¤šå¤„ç† 5 ä¸ªäº§å“ï¼‰
        # [FIXED] åªæ£€æŸ¥ pending çŠ¶æ€ï¼Œä¸å†è‡ªåŠ¨é‡è¯• failed çŠ¶æ€ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
        products_with_pending = db.execute(
            select(Product.id, func.count(Review.id).label("pending_count"))
            .join(Review, Review.product_id == Product.id)
            .where(
                and_(
                    Review.translation_status == TranslationStatus.PENDING.value,
                    Review.is_deleted == False
                )
            )
            .group_by(Product.id)
            .having(func.count(Review.id) > 0)
            .order_by(func.count(Review.id).desc())
            .limit(5)
        )
        
        pending_products = products_with_pending.all()
        
        if not pending_products:
            return {"triggered": 0, "message": "No pending translations"}
        
        triggered = 0
        for product_id, pending_count in pending_products:
            # ğŸ”¥ ä¸ºæ¯ä¸ªäº§å“è§¦å‘å¤šä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆå……åˆ†åˆ©ç”¨å¹¶å‘ï¼‰
            # æ ¹æ®å¾…ç¿»è¯‘æ•°é‡å†³å®šè§¦å‘å‡ ä¸ªä»»åŠ¡
            num_tasks = min(3, max(1, pending_count // 20))  # æ¯ 20 æ¡è§¦å‘ 1 ä¸ªä»»åŠ¡ï¼Œæœ€å¤š 3 ä¸ª
            
            for _ in range(num_tasks):
                task_ingest_translation_only.delay(str(product_id))
                triggered += 1
            
            logger.info(f"[ç¿»è¯‘è°ƒåº¦] äº§å“ {product_id} å¾…ç¿»è¯‘ {pending_count} æ¡ï¼Œè§¦å‘ {num_tasks} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        return {
            "triggered": triggered,
            "products": len(pending_products),
            "message": f"Triggered {triggered} translation tasks for {len(pending_products)} products"
        }
        
    except Exception as e:
        logger.error(f"[ç¿»è¯‘è°ƒåº¦] æ£€æŸ¥å¤±è´¥: {e}")
        return {"triggered": 0, "error": str(e)}
        
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡9: å¼‚æ­¥æŠ¥å‘Šç”Ÿæˆ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=30)
def task_generate_report(self, product_id: str, report_type: str = "comprehensive"):
    """
    ğŸš€ å¼‚æ­¥æŠ¥å‘Šç”Ÿæˆä»»åŠ¡ (Async Report Generation)
    
    åå°ç”Ÿæˆ AI åˆ†ææŠ¥å‘Šï¼Œç”¨æˆ·å¯ä»¥ç¦»å¼€é¡µé¢ï¼Œä»»åŠ¡ç»§ç»­è¿è¡Œã€‚
    
    å‚æ•°ï¼š
        product_id: äº§å“ UUID
        report_type: æŠ¥å‘Šç±»å‹ (comprehensive/operations/product/supply_chain)
    
    è¿”å›ï¼š
        ç”Ÿæˆç»“æœï¼ŒåŒ…å«æŠ¥å‘Š ID
    """
    import asyncio
    from app.services.summary_service import SummaryService
    from app.models.task import Task, TaskType, TaskStatus
    
    logger.info(f"[æŠ¥å‘Šç”Ÿæˆ] å¼€å§‹ä¸ºäº§å“ {product_id} ç”Ÿæˆ {report_type} æŠ¥å‘Š")
    
    # æŠ¥å‘Šè¿›åº¦ - å‡†å¤‡ä¸­
    self.update_state(state='PROGRESS', meta={
        'progress': 5,
        'current_step': 'å‡†å¤‡ä¸­...'
    })
    
    db = get_sync_db()
    
    try:
        # åˆ›å»º/æ›´æ–°ä»»åŠ¡è®°å½•
        task_record = get_or_create_task(
            db=db,
            product_id=product_id,
            task_type="report_generation",
            total_items=1,
            celery_task_id=self.request.id
        )
        task_record.status = TaskStatus.PROCESSING.value
        db.commit()
        
        # æŠ¥å‘Šè¿›åº¦ - å¼€å§‹ç”Ÿæˆ
        self.update_state(state='PROGRESS', meta={
            'progress': 15,
            'current_step': 'æ­£åœ¨æ”¶é›†è¯„è®ºæ•°æ®...'
        })
        
        # å¼‚æ­¥ç”ŸæˆæŠ¥å‘Š - ä¿®å¤äº‹ä»¶å¾ªç¯é—®é¢˜
        # åœ¨å‡½æ•°å†…éƒ¨åˆ›å»ºæ–°çš„æ•°æ®åº“å¼•æ“ï¼Œé¿å…ä½¿ç”¨å…¨å±€çš„ async_session_maker
        # è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨æ­£ç¡®çš„äº‹ä»¶å¾ªç¯ä¸­åˆ›å»ºè¿æ¥
        async def generate_report_async():
            from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
            from app.core.config import settings
            
            # åœ¨å‡½æ•°å†…éƒ¨åˆ›å»ºæ–°çš„å¼•æ“å’Œä¼šè¯ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
            engine = create_async_engine(
                settings.DATABASE_URL,
                echo=False,
                pool_pre_ping=True,
                pool_size=5,
                max_overflow=10,
            )
            async_session_maker = async_sessionmaker(
                engine,
                class_=AsyncSession,
                expire_on_commit=False,
            )
            
            try:
                async with async_session_maker() as async_db:
                    summary_service = SummaryService(async_db)
                    # æŠ¥å‘Šè¿›åº¦ - è°ƒç”¨ AI
                    self.update_state(state='PROGRESS', meta={
                        'progress': 30,
                        'current_step': 'AI æ­£åœ¨åˆ†æè¯„è®ºæ•°æ®...'
                    })
                    result = await summary_service.generate_report(
                        product_id=product_id,
                        report_type=report_type,
                        min_reviews=30,  # [UPDATED 2026-01-19] æŠ¥å‘Šéœ€è¦è‡³å°‘30æ¡è¯„è®º
                        save_to_db=True
                    )
                    return result
            finally:
                # å…³é—­å¼•æ“ï¼Œé‡Šæ”¾è¿æ¥
                await engine.dispose()
        
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        # åœ¨ Celery worker çš„ ForkPoolWorker ä¸­ï¼Œæ¯ä¸ªä»»åŠ¡åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œ
        # åº”è¯¥æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ asyncio.run()
        try:
            report_result = asyncio.run(generate_report_async())
        except RuntimeError as e:
            # å¦‚æœå·²æœ‰äº‹ä»¶å¾ªç¯ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œè®°å½•é”™è¯¯å¹¶é‡è¯•
            logger.error(f"[æŠ¥å‘Šç”Ÿæˆ] äº‹ä»¶å¾ªç¯é”™è¯¯: {e}")
            # å°è¯•åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                report_result = loop.run_until_complete(generate_report_async())
            finally:
                try:
                    loop.close()
                except:
                    pass
        
        # æŠ¥å‘Šè¿›åº¦ - ä¿å­˜ç»“æœ
        self.update_state(state='PROGRESS', meta={
            'progress': 90,
            'current_step': 'æ­£åœ¨ä¿å­˜æŠ¥å‘Š...'
        })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if report_result.get("success"):
            task_record.status = TaskStatus.COMPLETED.value
            task_record.processed_items = 1
            report_data = report_result.get("report", {})
            report_id = report_data.get("id") if isinstance(report_data, dict) else None
            logger.info(f"[æŠ¥å‘Šç”Ÿæˆ] æˆåŠŸç”ŸæˆæŠ¥å‘Š {report_id}")
        else:
            task_record.status = TaskStatus.FAILED.value
            task_record.error_message = report_result.get("error", "æœªçŸ¥é”™è¯¯")
            logger.error(f"[æŠ¥å‘Šç”Ÿæˆ] å¤±è´¥: {report_result.get('error')}")
        
        db.commit()
        
        return {
            "success": report_result.get("success", False),
            "product_id": product_id,
            "report_type": report_type,
            "report_id": report_data.get("id") if report_result.get("success") and isinstance(report_data, dict) else None,
            "error": report_result.get("error") if not report_result.get("success") else None
        }
        
    except Exception as e:
        logger.error(f"[æŠ¥å‘Šç”Ÿæˆ] å¼‚å¸¸: {e}")
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            if task_record:
                task_record.status = TaskStatus.FAILED.value
                task_record.error_message = str(e)
                db.commit()
        except:
            pass
        raise self.retry(exc=e)
    finally:
        db.close()


# ============== [NEW] ä»»åŠ¡10: é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ ==============

@celery_app.task(bind=True, max_retries=3, default_retry_delay=10)
def task_process_ingestion_queue(self):
    """
    ğŸš€ é˜Ÿåˆ—æ¶ˆè´¹å…¥åº“ä»»åŠ¡ (Ingestion Queue Consumer)
    
    ä» Redis é˜Ÿåˆ—æ‰¹é‡æ¶ˆè´¹è¯„è®ºæ•°æ®ï¼Œå…¥åº“åˆ° PostgreSQLã€‚
    
    è®¾è®¡ç‰¹ç‚¹ï¼š
    1. é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–ï¼šAPI å±‚åªå†™ Redisï¼Œæœ¬ä»»åŠ¡æ‰¹é‡å…¥åº“
    2. ä¸‰å±‚å»é‡ï¼šRedis Set â†’ å†…å­˜ Set â†’ DB ON CONFLICT
    3. æŒ‰ ASIN åˆ†ç»„å¤„ç†ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
    4. å…¥åº“æˆåŠŸåè§¦å‘ç¿»è¯‘ä»»åŠ¡
    
    è°ƒåº¦æ–¹å¼ï¼š
    - Celery Beat æ¯ 5 ç§’è§¦å‘ä¸€æ¬¡
    - æ¯æ¬¡æœ€å¤šå¤„ç† 100 æ¡é˜Ÿåˆ—æ•°æ®
    
    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    from app.core.redis import ReviewIngestionQueueSync, get_sync_redis
    from app.services.ingestion_service import IngestionService
    
    logger.debug("[Ingestion] å¼€å§‹æ¶ˆè´¹é˜Ÿåˆ—...")
    
    redis_cli = get_sync_redis()
    queue = ReviewIngestionQueueSync(redis_cli)
    
    # Step 1: ä»é˜Ÿåˆ—æ‰¹é‡å–å‡ºæ•°æ®
    items = queue.pop_batch(count=100)
    
    if not items:
        logger.debug("[Ingestion] é˜Ÿåˆ—ä¸ºç©ºï¼Œè·³è¿‡")
        return {"processed": 0, "items": 0}
    
    logger.info(f"[Ingestion] ä»é˜Ÿåˆ—å–å‡º {len(items)} æ¡æ•°æ®")
    
    db = get_sync_db()
    
    try:
        # Step 2: è°ƒç”¨å…¥åº“æœåŠ¡å¤„ç†
        service = IngestionService(db, redis_cli)
        results = service.process_queue_items(items)
        
        # Step 3: ç»Ÿè®¡ç»“æœ
        total_inserted = sum(r.get("inserted", 0) for r in results.values())
        total_skipped = sum(r.get("skipped", 0) for r in results.values())
        
        logger.info(
            f"[Ingestion] å¤„ç†å®Œæˆ: {len(results)} ä¸ªäº§å“, "
            f"æ–°å¢ {total_inserted} æ¡, è·³è¿‡ {total_skipped} æ¡"
        )
        
        # Step 4: ä¸ºæœ‰æ–°æ•°æ®çš„äº§å“è§¦å‘ç¿»è¯‘ï¼ˆä½¿ç”¨ Redis é”é˜²æ­¢é‡å¤è§¦å‘ï¼‰
        from app.core.redis import get_sync_redis
        redis_client = get_sync_redis()
        
        for asin, result in results.items():
            if result.get("inserted", 0) > 0:
                # ä½¿ç”¨ Redis SETNX å®ç°åˆ†å¸ƒå¼é”ï¼Œé˜²æ­¢åŒä¸€äº§å“é‡å¤è§¦å‘ç¿»è¯‘ä»»åŠ¡
                # é”æœ‰æ•ˆæœŸ 5 åˆ†é’Ÿï¼ˆç¿»è¯‘ä»»åŠ¡é€šå¸¸åœ¨å‡ åˆ†é’Ÿå†…å®Œæˆï¼‰
                lock_key = f"lock:translation:{asin}"
                lock_acquired = redis_client.set(lock_key, "1", nx=True, ex=300)
                
                if not lock_acquired:
                    logger.debug(f"[Ingestion] äº§å“ {asin} ç¿»è¯‘ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­ï¼Œè·³è¿‡è§¦å‘")
                    continue
                
                # è·å– product_id
                from app.models.product import Product
                product_result = db.execute(
                    select(Product).where(Product.asin == asin)
                )
                product = product_result.scalar_one_or_none()
                
                if product:
                    # è§¦å‘æµå¼ç¿»è¯‘
                    task_ingest_translation_only.delay(str(product.id))
                    logger.info(f"[Ingestion] äº§å“ {asin} å·²è§¦å‘ç¿»è¯‘ä»»åŠ¡")
        
        return {
            "processed": len(items),
            "products": len(results),
            "inserted": total_inserted,
            "skipped": total_skipped,
            "details": results
        }
        
    except Exception as e:
        logger.error(f"[Ingestion] å¤„ç†å¤±è´¥: {e}")
        db.rollback()
        raise self.retry(exc=e)
        
    finally:
        db.close()


# ============== [NEW] è¾…åŠ©å‡½æ•°ï¼šåŒæ­¥å·²æœ‰ review_id åˆ° Redis ==============

@celery_app.task
def task_sync_product_reviews_to_redis(asin: str):
    """
    å°†äº§å“çš„å·²æœ‰ review_id åŒæ­¥åˆ° Redis
    
    ç”¨äºï¼š
    1. Redis é‡å¯åæ¢å¤å»é‡æ•°æ®
    2. æ‰‹åŠ¨è§¦å‘åŒæ­¥
    
    Args:
        asin: äº§å“ ASIN
    """
    from app.services.ingestion_service import IngestionService
    
    db = get_sync_db()
    
    try:
        service = IngestionService(db)
        service.sync_redis_from_db(asin)
        logger.info(f"[Sync] äº§å“ {asin} çš„ review_id å·²åŒæ­¥åˆ° Redis")
        return {"success": True, "asin": asin}
    except Exception as e:
        logger.error(f"[Sync] åŒæ­¥å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()


# ============== [NEW] å®šæ—¶ä»»åŠ¡ï¼šåˆ†æè¡¥å…¨å·¡æ£€ ==============

@celery_app.task(bind=True)
def task_analysis_completion_patrol(self):
    """
    ğŸ›¡ï¸ åˆ†æè¡¥å…¨å·¡æ£€ä»»åŠ¡ (Analysis Completion Patrol)
    
    å®šæœŸæ£€æŸ¥æ‰€æœ‰äº§å“ï¼Œæ‰¾å‡ºæœ‰é—æ¼æ´å¯Ÿ/ä¸»é¢˜çš„è¯„è®ºï¼Œè§¦å‘è¡¥å…¨å¤„ç†ã€‚
    
    è¿™æ˜¯"ä¸‰å±‚é˜²æŠ¤æœºåˆ¶"çš„æœ€åä¸€é“é˜²çº¿ï¼š
    1. ç¬¬ä¸€å±‚ï¼šå…¥åº“é˜Ÿåˆ—ç­‰å¾…ï¼ˆtask_full_auto_analysisï¼‰
    2. ç¬¬äºŒå±‚ï¼šä»»åŠ¡æœ«å°¾è¡¥å…¨æ£€æŸ¥ï¼ˆtask_extract_insights/themesï¼‰
    3. ç¬¬ä¸‰å±‚ï¼šæœ¬ä»»åŠ¡ - å®šæ—¶å…¨å±€å·¡æ£€
    
    è¿è¡Œé¢‘ç‡ï¼šæ¯ 5 åˆ†é’Ÿ
    
    æ£€æŸ¥é€»è¾‘ï¼š
    1. æ‰¾å‡ºæœ€è¿‘ 24 å°æ—¶å†…æœ‰è¯„è®ºçš„äº§å“
    2. å¯¹æ¯ä¸ªäº§å“æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„æ´å¯Ÿ/ä¸»é¢˜
    3. å¦‚æœæœ‰é—æ¼ä¸”æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œè§¦å‘è¡¥å…¨
    
    è®¾è®¡åŸåˆ™ï¼š
    - è½»é‡çº§ï¼šåªæ£€æŸ¥æ´»è·ƒäº§å“ï¼Œä¸å…¨è¡¨æ‰«æ
    - éä¾µå…¥ï¼šåªåœ¨ç¡®å®éœ€è¦æ—¶æ‰è§¦å‘è¡¥å…¨
    - é˜²é‡å¤ï¼šæ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œé¿å…é‡å¤è§¦å‘
    """
    from app.models.product import Product
    from app.models.review import Review
    from app.models.insight import ReviewInsight
    from app.models.theme_highlight import ReviewThemeHighlight
    from app.models.task import Task, TaskType, TaskStatus
    from datetime import datetime, timezone, timedelta
    
    logger.info("[å·¡æ£€] ğŸ” å¼€å§‹åˆ†æè¡¥å…¨å·¡æ£€...")
    
    db = get_sync_db()
    
    try:
        # æ‰¾å‡ºæœ€è¿‘ 24 å°æ—¶å†…æœ‰è¯„è®ºçš„äº§å“
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=24)
        
        active_products_result = db.execute(
            select(Product.id, Product.asin)
            .where(
                Product.id.in_(
                    select(Review.product_id)
                    .where(Review.created_at >= cutoff_time)
                    .distinct()
                )
            )
        )
        active_products = active_products_result.all()
        
        if not active_products:
            logger.info("[å·¡æ£€] âœ… æ— æ´»è·ƒäº§å“ï¼Œè·³è¿‡")
            return {"checked": 0, "triggered": 0}
        
        logger.info(f"[å·¡æ£€] å‘ç° {len(active_products)} ä¸ªæ´»è·ƒäº§å“")
        
        triggered_insights = 0
        triggered_themes = 0
        
        for product_id, asin in active_products:
            product_id_str = str(product_id)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„åˆ†æä»»åŠ¡
            running_task_result = db.execute(
                select(Task.id)
                .where(
                    and_(
                        Task.product_id == product_id,
                        Task.status.in_([TaskStatus.PENDING.value, TaskStatus.PROCESSING.value]),
                        Task.task_type.in_([
                            TaskType.INSIGHTS.value,
                            TaskType.THEMES.value,
                            TaskType.AUTO_ANALYSIS.value
                        ])
                    )
                )
                .limit(1)
            )
            if running_task_result.scalar_one_or_none():
                logger.debug(f"[å·¡æ£€] äº§å“ {asin} æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼Œè·³è¿‡")
                continue
            
            # ğŸ”§ [FIX] å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç»´åº¦å’Œæ ‡ç­¾ï¼ˆç§‘å­¦å­¦ä¹ çš„å‰ç½®æ¡ä»¶ï¼‰
            from app.models.product_dimension import ProductDimension
            from app.models.product_context_label import ProductContextLabel
            
            dim_count_result = db.execute(
                select(func.count(ProductDimension.id))
                .where(ProductDimension.product_id == product_id)
            )
            has_dimensions = (dim_count_result.scalar() or 0) > 0
            
            label_count_result = db.execute(
                select(func.count(ProductContextLabel.id))
                .where(ProductContextLabel.product_id == product_id)
            )
            has_labels = (label_count_result.scalar() or 0) > 0
            
            # æ£€æŸ¥é—æ¼çš„æ´å¯Ÿ
            missing_insights_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.body_original.isnot(None),
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewInsight.review_id).distinct()
                        )
                    )
                )
            )
            missing_insights = missing_insights_result.scalar() or 0
            
            # æ£€æŸ¥é—æ¼çš„ä¸»é¢˜
            missing_themes_result = db.execute(
                select(func.count(Review.id))
                .where(
                    and_(
                        Review.product_id == product_id,
                        Review.body_original.isnot(None),
                        Review.is_deleted == False,
                        ~Review.id.in_(
                            select(ReviewThemeHighlight.review_id).distinct()
                        )
                    )
                )
            )
            missing_themes = missing_themes_result.scalar() or 0
            
            # ğŸ”§ [FIX] æ™ºèƒ½è§¦å‘ç­–ç•¥ï¼š
            # 1. å¦‚æœæ²¡æœ‰ç»´åº¦æˆ–æ ‡ç­¾ï¼Œè§¦å‘å®Œæ•´æµç¨‹ï¼ˆåŒ…å«ç§‘å­¦å­¦ä¹ ï¼‰
            # 2. å¦‚æœå·²æœ‰ç»´åº¦å’Œæ ‡ç­¾ï¼Œåªè§¦å‘è¡¥å…¨ä»»åŠ¡
            if missing_insights > 0 or missing_themes > 0:
                if not has_dimensions or not has_labels:
                    # æ²¡æœ‰ç»´åº¦æˆ–æ ‡ç­¾ï¼Œè§¦å‘å®Œæ•´æµç¨‹ï¼ˆåŒ…å«ç§‘å­¦å­¦ä¹ ï¼‰
                    logger.warning(f"[å·¡æ£€] âš ï¸ äº§å“ {asin} ç¼ºå°‘ç§‘å­¦å­¦ä¹ ï¼ˆç»´åº¦:{has_dimensions}, æ ‡ç­¾:{has_labels}ï¼‰ï¼Œè§¦å‘å®Œæ•´åˆ†ææµç¨‹")
                    # åˆ›å»ºä»»åŠ¡è®°å½•
                    from app.models.task import Task
                    import uuid
                    new_task_id = str(uuid.uuid4())
                    new_task = Task(
                        id=new_task_id,
                        product_id=product_id,
                        task_type=TaskType.AUTO_ANALYSIS.value,
                        status=TaskStatus.PENDING.value,
                        total_items=4  # 4ä¸ªæ­¥éª¤
                    )
                    db.add(new_task)
                    db.commit()
                    
                    # è§¦å‘å®Œæ•´åˆ†æï¼ˆåŒ…å«ç§‘å­¦å­¦ä¹ ï¼‰
                    task_full_auto_analysis.apply_async(
                        args=[product_id_str, new_task_id],
                        countdown=5
                    )
                    triggered_insights += 1
                    triggered_themes += 1
                else:
                    # å·²æœ‰ç»´åº¦å’Œæ ‡ç­¾ï¼Œåªè§¦å‘è¡¥å…¨ä»»åŠ¡
                    if missing_insights > 0:
                        logger.warning(f"[å·¡æ£€] âš ï¸ äº§å“ {asin} å‘ç° {missing_insights} æ¡é—æ¼æ´å¯Ÿï¼Œè§¦å‘è¡¥å…¨")
                        task_extract_insights.apply_async(
                            args=[product_id_str],
                            countdown=5  # 5ç§’åæ‰§è¡Œ
                        )
                        triggered_insights += 1
                    
                    if missing_themes > 0:
                        logger.warning(f"[å·¡æ£€] âš ï¸ äº§å“ {asin} å‘ç° {missing_themes} æ¡é—æ¼ä¸»é¢˜ï¼Œè§¦å‘è¡¥å…¨")
                        task_extract_themes.apply_async(
                            args=[product_id_str],
                            countdown=10  # 10ç§’åæ‰§è¡Œï¼Œé”™å¼€æ´å¯Ÿä»»åŠ¡
                        )
                        triggered_themes += 1
        
        result = {
            "checked": len(active_products),
            "triggered_insights": triggered_insights,
            "triggered_themes": triggered_themes
        }
        
        if triggered_insights > 0 or triggered_themes > 0:
            logger.info(f"[å·¡æ£€] ğŸ”„ å·¡æ£€å®Œæˆï¼Œè§¦å‘ {triggered_insights} ä¸ªæ´å¯Ÿè¡¥å…¨ + {triggered_themes} ä¸ªä¸»é¢˜è¡¥å…¨")
        else:
            logger.info(f"[å·¡æ£€] âœ… å·¡æ£€å®Œæˆï¼Œæ‰€æœ‰äº§å“åˆ†æå®Œæ•´")
        
        return result
        
    except Exception as e:
        logger.error(f"[å·¡æ£€] âŒ å·¡æ£€å¤±è´¥: {e}")
        return {"error": str(e)}
    finally:
        db.close()


# ============== [NEW 2026-01-22] ä»»åŠ¡: ç»´åº¦æ€»ç»“ç”Ÿæˆ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=60)
def task_generate_dimension_summaries(self, product_id: str):
    """
    ç”Ÿæˆäº§å“ç»´åº¦æ€»ç»“ï¼ˆä¸­è§‚å±‚AIåˆ†æï¼‰
    
    æ‰“é€šå¾®è§‚(å•æ¡è¯„è®ºæ´å¯Ÿ)åˆ°å®è§‚(é¡¹ç›®æŠ¥å‘Š)çš„æ¡¥æ¢ï¼ŒåŒ…æ‹¬ï¼š
    - 5Wä¸»é¢˜æ€»ç»“ (buyer/user/where/when/why/what)
    - äº§å“ç»´åº¦æ€»ç»“ (å„è¯„ä»·ç»´åº¦çš„ä¼˜åŠ£åŠ¿æ€»ç»“)
    - æƒ…æ„Ÿç»´åº¦æ€»ç»“
    - åœºæ™¯ç»´åº¦æ€»ç»“
    - æ¶ˆè´¹è€…åŸå‹ (3-5ä¸ªå…¸å‹ç”¨æˆ·ç”»åƒ)
    - æ•´ä½“æ•°æ®æ€»ç»“
    
    è§¦å‘æ¡ä»¶ï¼šä¸»é¢˜æå–ä»»åŠ¡å®Œæˆåè‡ªåŠ¨è§¦å‘
    
    Args:
        product_id: UUID of the product
    """
    import asyncio
    from app.services.dimension_summary_service import DimensionSummaryService
    
    logger.info(f"[ç»´åº¦æ€»ç»“] å¼€å§‹ç”Ÿæˆäº§å“ç»´åº¦æ€»ç»“: {product_id}")
    
    # è·å–å¼‚æ­¥æ•°æ®åº“ä¼šè¯
    async def run_async():
        from app.db.session import async_session_maker
        async with async_session_maker() as session:
            service = DimensionSummaryService(session)
            return await service.generate_all_summaries(product_id)
    
    try:
        # åœ¨ worker çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(run_async())
        finally:
            loop.close()
        
        summary_counts = {
            "themes": len(result.get("theme_summaries", [])),
            "dimensions": len(result.get("dimension_summaries", [])),
            "emotions": len(result.get("emotion_summaries", [])),
            "scenarios": len(result.get("scenario_summaries", [])),
            "personas": len(result.get("consumer_personas", [])),
            "overall": 1 if result.get("overall_summary") else 0,
        }
        
        logger.info(f"[ç»´åº¦æ€»ç»“] âœ… ç”Ÿæˆå®Œæˆ: {product_id}, ç»Ÿè®¡: {summary_counts}")
        
        return {
            "product_id": product_id,
            "success": True,
            "summary_counts": summary_counts
        }
        
    except Exception as e:
        logger.error(f"[ç»´åº¦æ€»ç»“] âŒ ç”Ÿæˆå¤±è´¥: {product_id}, é”™è¯¯: {e}")
        raise self.retry(exc=e)


# ============== [NEW 2026-01-24] ä»»åŠ¡: æ•°æ®é€è§†AIæ´å¯Ÿç”Ÿæˆ ==============

@celery_app.task(bind=True, max_retries=2, default_retry_delay=60)
def task_generate_pivot_insights(self, product_id: str):
    """
    ç”Ÿæˆäº§å“æ•°æ®é€è§†AIæ´å¯Ÿ
    
    åŒ…æ‹¬ï¼š
    - äººç¾¤æ´å¯Ÿ (audience): å†³ç­–é“¾è·¯ã€äººç¾¤-å–ç‚¹åŒ¹é…
    - éœ€æ±‚æ´å¯Ÿ (demand): éœ€æ±‚æ»¡è¶³åº¦çŸ©é˜µ
    - äº§å“æ´å¯Ÿ (product): è‡´å‘½ç¼ºé™·ã€ä¼˜åŠ£åŠ¿å¯¹æ¯”ã€æ”¹è¿›ä¼˜å…ˆçº§
    - è¿ç§» dimension_summaries åˆ°æ–°è¡¨
    
    Args:
        product_id: UUID of the product
    """
    from app.services.pivot_insight_service import PivotInsightService
    
    logger.info(f"[æ•°æ®é€è§†æ´å¯Ÿ] å¼€å§‹ç”Ÿæˆ: {product_id}")
    
    db = get_sync_db()
    
    try:
        service = PivotInsightService(db)
        result = service.generate_all_insights(UUID(product_id))
        
        if result.get("success"):
            logger.info(f"[æ•°æ®é€è§†æ´å¯Ÿ] âœ… ç”Ÿæˆå®Œæˆ: {product_id}, ç”Ÿæˆæ•°é‡: {result.get('total_generated', 0)}")
            return {
                "product_id": product_id,
                "success": True,
                "total_generated": result.get("total_generated", 0),
                "insights": result.get("generated_insights", [])
            }
        else:
            error = result.get("error", "æœªçŸ¥é”™è¯¯")
            logger.error(f"[æ•°æ®é€è§†æ´å¯Ÿ] âŒ ç”Ÿæˆå¤±è´¥: {product_id}, é”™è¯¯: {error}")
            raise Exception(error)
        
    except Exception as e:
        logger.error(f"[æ•°æ®é€è§†æ´å¯Ÿ] âŒ ä»»åŠ¡å¼‚å¸¸: {product_id}, é”™è¯¯: {e}")
        raise self.retry(exc=e)
    finally:
        db.close()


# ============================================================================
# ğŸš€ å¯¹æ¯”åˆ†æä»»åŠ¡ (Comparison Analysis Task)
# ============================================================================

@celery_app.task(bind=True, max_retries=2, default_retry_delay=60, time_limit=600, soft_time_limit=540)
def task_run_comparison_analysis(self, project_id: str):
    """
    ğŸš€ å¯¹æ¯”åˆ†æå¼‚æ­¥ä»»åŠ¡ (Async Comparison Analysis)
    
    åœ¨ Celery Worker ä¸­æ‰§è¡Œå¯¹æ¯”åˆ†æï¼Œæ”¯æŒï¼š
    1. è¿›åº¦å®æ—¶è¿½è¸ªï¼ˆé€šè¿‡ Redisï¼‰
    2. å¤±è´¥è‡ªåŠ¨é‡è¯•
    3. è¶…æ—¶ä¿æŠ¤ï¼ˆ10åˆ†é’Ÿï¼‰
    
    å‚æ•°ï¼š
        project_id: åˆ†æé¡¹ç›® UUID
    
    è¿”å›ï¼š
        {
            "project_id": "...",
            "success": True/False,
            "status": "completed/failed",
            "message": "..."
        }
    """
    import asyncio
    from app.core.redis import get_sync_redis, AnalysisProgressTrackerSync
    from app.models.analysis import AnalysisProject, AnalysisStatus
    
    logger.info(f"[å¯¹æ¯”åˆ†æ] ğŸš€ å¼€å§‹æ‰§è¡Œ: {project_id}")
    
    # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
    redis_client = get_sync_redis()
    progress_tracker = AnalysisProgressTrackerSync(redis_client)
    progress_tracker.init_progress(project_id, total_steps=5)
    
    db = get_sync_db()
    
    try:
        # è·å–é¡¹ç›®
        from app.models.analysis import AnalysisProject
        project = db.query(AnalysisProject).filter(AnalysisProject.id == project_id).first()
        
        if not project:
            progress_tracker.complete(project_id, success=False, error_message="é¡¹ç›®ä¸å­˜åœ¨")
            return {"project_id": project_id, "success": False, "message": "é¡¹ç›®ä¸å­˜åœ¨"}
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        project.status = AnalysisStatus.PROCESSING.value
        db.commit()
        
        progress_tracker.update_progress(project_id, 1, "æ•°æ®æ”¶é›†", 10, "æ­£åœ¨æ”¶é›†äº§å“æ•°æ®...")
        
        # å¼‚æ­¥æ‰§è¡Œåˆ†æï¼ˆåœ¨åŒæ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡Œå¼‚æ­¥ä»£ç ï¼‰
        async def run_async_analysis():
            from app.db.session import async_session_maker
            from app.services.analysis_service import AnalysisService
            
            async def sync_progress_callback(step: int, step_name: str, percent: int, message: str = ""):
                """åŒæ­¥è¿›åº¦å›è°ƒåŒ…è£…å™¨"""
                progress_tracker.update_progress(project_id, step, step_name, percent, message)
            
            async with async_session_maker() as async_db:
                service = AnalysisService(async_db)
                result = await service.run_analysis(project_id, progress_callback=sync_progress_callback)
                await async_db.commit()
                return result
        
        # è¿è¡Œå¼‚æ­¥åˆ†æ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(run_async_analysis())
        finally:
            loop.close()
        
        # æ ‡è®°å®Œæˆ
        progress_tracker.complete(project_id, success=True)
        
        logger.info(f"[å¯¹æ¯”åˆ†æ] âœ… å®Œæˆ: {project_id}")
        return {
            "project_id": project_id,
            "success": True,
            "status": "completed",
            "message": "åˆ†æå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"[å¯¹æ¯”åˆ†æ] âŒ å¤±è´¥: {project_id}, é”™è¯¯: {e}")
        progress_tracker.complete(project_id, success=False, error_message=str(e))
        
        # æ›´æ–°é¡¹ç›®çŠ¶æ€
        try:
            project = db.query(AnalysisProject).filter(AnalysisProject.id == project_id).first()
            if project:
                project.status = AnalysisStatus.FAILED.value
                project.error_message = str(e)
                db.commit()
        except Exception as update_error:
            logger.error(f"[å¯¹æ¯”åˆ†æ] æ›´æ–°çŠ¶æ€å¤±è´¥: {update_error}")
        
        raise self.retry(exc=e)
