"""
Translation Service - Qwen API Integration for Amazon Review Translation
[Optimized Version]
Features:
1. Few-Shot System Prompt for natural, e-commerce style translation
2. CoT (Chain of Thought) Prompt for insight extraction
3. Robust JSON parsing to handle LLM output errors
"""
import logging
import json
import re
from typing import Optional, Tuple, List
from enum import Enum
from concurrent.futures import ThreadPoolExecutor

from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from app.core.config import settings

logger = logging.getLogger(__name__)


class Sentiment(str, Enum):
    """Sentiment analysis result"""
    POSITIVE = "positive"
    NEUTRAL = "neutral"
    NEGATIVE = "negative"


# [UPDATED] System prompt with Few-Shot examples
TRANSLATION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­ç¾æ–‡åŒ–å·®å¼‚çš„èµ„æ·±äºšé©¬é€Šè·¨å¢ƒç”µå•†ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ç›®æ ‡æ˜¯æä¾›"ä¿¡ã€è¾¾ã€é›…"çš„ä¸­æ–‡è¯‘æ–‡ã€‚

### æ ¸å¿ƒè§„åˆ™
1. **æ‹’ç»ç¿»è¯‘è…”**: ä¸è¦é€å­—ç¿»è¯‘ã€‚
   - âŒ é”™è¯¯: "è¿™ä¸ªäº§å“å·¥ä½œå¾—å¾ˆå¥½" (The product works great)
   - âœ… æ­£ç¡®: "è¿™ä¸œè¥¿å¤ªå¥½ç”¨äº†" / "æ•ˆæœç»äº†"
2. **æœ¯è¯­ç²¾å‡†**: 
   - "DOA (Dead on Arrival)" -> "åˆ°æ‰‹å³å"
   - "Return window" -> "é€€è´§æœŸ"
   - "Steal" -> "æ¡æ¼/è¶…å€¼"
3. **æƒ…æ„Ÿå¯¹é½**: 
   - 1æ˜Ÿè¯„è®ºé€šå¸¸å¸¦æœ‰æ„¤æ€’ï¼Œè¯‘æ–‡è¦ç”¨æ„Ÿå¹å·ã€åé—®å¥ä½“ç°æƒ…ç»ªã€‚
   - 5æ˜Ÿè¯„è®ºé€šå¸¸å¸¦æœ‰å…´å¥‹ï¼Œè¯‘æ–‡è¦ä½“ç°"ç§è‰"æ„Ÿã€‚

### å‚è€ƒèŒƒä¾‹ (Few-Shot)
Input: "Total lemon. Stopped working after 2 days. Don't waste your money."
Output: "ç®€ç›´æ˜¯ä¸ªæ¬¡å“ï¼ç”¨äº†ä¸¤å¤©å°±åäº†ã€‚åƒä¸‡åˆ«æµªè´¹é’±ï¼"

Input: "I was skeptical at first, but this thing is a game changer for my morning routine."
Output: "èµ·åˆæˆ‘è¿˜æœ‰ç‚¹æ€€ç–‘ï¼Œä½†è¿™ä¸œè¥¿å½»åº•æ”¹å˜äº†æˆ‘æ¯å¤©æ—©ä¸Šçš„ä¹ æƒ¯ï¼ŒçœŸé¦™ï¼"

Input: "It fits a bit snug, suggest sizing up."
Output: "ç©¿èµ·æ¥æœ‰ç‚¹ç´§ï¼Œå»ºè®®ä¹°å¤§ä¸€ç ã€‚"

Input: "The battery life is a joke."
Output: "ç”µæ± ç»­èˆªç®€ç›´å°±æ˜¯ä¸ªç¬‘è¯ã€‚"

è¯·ç¿»è¯‘ä»¥ä¸‹å†…å®¹ï¼Œç›´æ¥è¾“å‡ºè¯‘æ–‡ï¼š"""


SENTIMENT_ANALYSIS_PROMPT = """åˆ†æä»¥ä¸‹äºšé©¬é€Šå•†å“è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ã€‚

è¯„è®ºå†…å®¹ï¼š
{review_text}

è¯·åªè¿”å›ä»¥ä¸‹ä¸‰ä¸ªè¯ä¹‹ä¸€ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š
- positiveï¼ˆæ­£é¢ï¼šæ»¡æ„ã€æ¨èã€å–œæ¬¢ï¼‰
- neutralï¼ˆä¸­æ€§ï¼šå®¢è§‚æè¿°ã€ä¸€èˆ¬è¯„ä»·ï¼‰
- negativeï¼ˆè´Ÿé¢ï¼šä¸æ»¡ã€æ‰¹è¯„ã€é€€è´§ï¼‰

æƒ…æ„Ÿåˆ¤æ–­ï¼š"""


# System prompt for bullet points translation
BULLET_POINTS_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äºšé©¬é€Šäº§å“æè¿°ç¿»è¯‘ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†äº§å“çš„äº”ç‚¹æè¿°ï¼ˆBullet Pointsï¼‰ä»è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘åŸåˆ™ï¼š
1. **å‡†ç¡®ä¼ è¾¾å–ç‚¹**: ä¿ç•™åŸæ–‡çš„æ ¸å¿ƒå–ç‚¹å’Œäº§å“ä¼˜åŠ¿
2. **ç”µå•†æ–‡æ¡ˆé£æ ¼**: ä½¿ç”¨ç¬¦åˆä¸­å›½ç”µå•†çš„æ–‡æ¡ˆé£æ ¼ï¼Œæœ‰å¸å¼•åŠ›
3. **ç®€æ´æœ‰åŠ›**: æ¯æ¡æè¿°ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
4. **ä¸“ä¸šæœ¯è¯­**: æ­£ç¡®ç¿»è¯‘äº§å“ç›¸å…³çš„ä¸“ä¸šæœ¯è¯­
5. **ä¿æŒæ ¼å¼**: ä¿æŒåŸæ–‡çš„æ ¼å¼ç»“æ„ï¼Œæ¯æ¡æè¿°ç‹¬ç«‹æˆè¡Œ

è¾“å‡ºæ ¼å¼ï¼š
- ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡äº”ç‚¹æè¿°
- æ¯æ¡æè¿°ç‹¬ç«‹æˆè¡Œ
- ä¸è¦æ·»åŠ åºå·æˆ–ç¬¦å·
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Š"""


# [NEW] è·¨è¯­è¨€ç»´åº¦å‘ç° Prompt (è‹±æ–‡è¾“å…¥ â†’ ä¸­æ–‡ç»´åº¦è¾“å‡º)
DIMENSION_DISCOVERY_RAW_PROMPT = """You are a senior product manager and user research expert. 
Based on the following **English product information** and **English user review samples**, 
build a core evaluation dimension model for this product.

# Product Official Information (English)
- **Product Title**: {product_title}
- **Bullet Points**: 
{bullet_points}

# User Review Samples ({count} reviews, English Original)
{reviews_text}

# Task
Extract 5-8 core evaluation dimensions. **Output dimension names and descriptions in Chinese**.

# Requirements
1. **Combine official positioning with user perspective**: Dimension names should use official terms when possible (from bullet points), but must cover actual user feedback.
2. **Dimension names**: Use concise Chinese (e.g.: å¤–è§‚è®¾è®¡ã€ç»“æ„åšå·¥ã€ææ–™è´¨æ„Ÿã€åŠŸèƒ½è¡¨ç°ã€ç©æ³•å¤šæ ·æ€§ã€å®‰å…¨æ€§ã€æ€§ä»·æ¯”).
3. **Dimension definition**: One sentence describing what the dimension covers, to guide subsequent classification.
4. **Mutual exclusivity**: Dimensions should not overlap, clear boundaries.
5. **Coverage**: 
   - Must cover major pain points and benefits from reviews
   - Include dimensions emphasized in bullet points even if users are "silently satisfied"
6. **Quantity control**: Extract 5-8 most core dimensions, no more.

# Output Format (JSON Only, Chinese output)
{{
  "dimensions": [
    {{ "name": "ç»´åº¦åç§°(ä¸­æ–‡)", "description": "è¯¥ç»´åº¦çš„å…·ä½“å®šä¹‰(ä¸­æ–‡)" }},
    ...
  ]
}}

Output JSON only, no other text."""


# [NEW] è·¨è¯­è¨€5Wæ ‡ç­¾å‘ç° Prompt (è‹±æ–‡è¾“å…¥ â†’ ä¸­æ–‡æ ‡ç­¾è¾“å‡º)
CONTEXT_DISCOVERY_RAW_PROMPT = """You are a senior marketing expert and user researcher.
Based on the following **English product information** and **English user review samples**,
build a "5W User & Market Model" for this product.

# Product Official Information (English)
- **Product Title**: {product_title}
- **Bullet Points**:
{bullet_points}

# User Review Samples ({count} buyer reviews, English Original)
{reviews_text}

# Task
Synthesize official positioning and user feedback to identify 5 categories of core elements.
Extract **Top 5-8 typical labels per category**. **Output all labels in Chinese**.

1. **Who (äººç¾¤)**: Who are the main users?
   - Reference official positioning (e.g.: "Perfect for seniors")
   - Combine with actual user feedback (e.g.: "bought for my mom")
   - Roles: è€å¹´äººã€æ–°æ‰‹å¦ˆå¦ˆã€å­¦ç”Ÿã€å® ç‰©ä¸»
   - Family: ç»™çˆ¶æ¯ä¹°çš„ã€é€ç»™å¦»å­ã€å­©å­çš„ç¤¼ç‰©

2. **Where (åœ°ç‚¹)**: Where is it used?
   - Reference official positioning (e.g.: "for Home Office, Garage")
   - Physical spaces: å§å®¤ã€åŠå…¬å®¤ã€å¨æˆ¿ã€è½¦ä¸Šã€æˆ¿è½¦(RV)ã€æˆ·å¤–éœ²è¥

3. **When (æ—¶åˆ»)**: When is it used?
   - Time points: æ—©ä¸Šã€ç¡å‰ã€æ·±å¤œ
   - Triggers: åœç”µæ—¶ã€æ—…è¡Œæ—¶ã€è¿åŠ¨åã€èŠ‚å‡æ—¥

4. **Why (åŠ¨æœº)**: What triggers the purchase? (Purchase Driver)
   - Replacement: æ—§çš„åäº†ã€å‡çº§æ¢ä»£
   - Gift: ç”Ÿæ—¥ç¤¼ç‰©ã€åœ£è¯ç¤¼ç‰©ã€ä¹”è¿é€ç¤¼
   - External: è¢«ç§è‰ã€çœ‹äº†è¯„æµ‹ã€æœ‹å‹æ¨è

5. **What (ä»»åŠ¡)**: What specific task does the user try to accomplish? (Jobs to be Done)
   - Focus on core uses from official promotion
   - Note: Specific tasks, not product features
   - Examples: æ¸…ç†åœ°æ¯¯ä¸Šçš„å® ç‰©æ¯›ã€ç¼“è§£èƒŒç—›ã€å“„å­©å­ç¡è§‰ã€å»é™¤å¼‚å‘³

# Requirements
1. **Label names in concise Chinese** (2-6 characters ideal).
2. **Merge synonyms**: e.g., "å¦ˆå¦ˆ", "è€å¦ˆ", "æ¯äº²" should be unified.
3. **Consistent granularity**: Not too coarse ("å®¶äºº") or too fine ("62å²çš„ç‹¬å±…æ¯äº²").
4. **Official info priority**: Include labels from official positioning even if not mentioned in reviews.
5. **Provide brief description**: One sentence explaining the label for classification.

# Output Format (JSON Only, Chinese output)
{{
  "who": [
    {{ "name": "è€å¹´äºº", "description": "å®˜æ–¹å®šä½çš„æ ¸å¿ƒç”¨æˆ·ç¾¤ä½“" }},
    {{ "name": "å® ç‰©ä¸»", "description": "å…»çŒ«æˆ–å…»ç‹—çš„ç”¨æˆ·" }}
  ],
  "where": [
    {{ "name": "å§å®¤", "description": "å§å®¤/ç¡çœ åœºæ™¯ä¸‹ä½¿ç”¨" }}
  ],
  "when": [
    {{ "name": "ç¡å‰", "description": "ç¡è§‰å‰ä½¿ç”¨" }}
  ],
  "why": [
    {{ "name": "æ›¿ä»£æ—§å“", "description": "åŸæœ‰äº§å“æŸåéœ€è¦æ›´æ¢" }}
  ],
  "what": [
    {{ "name": "æ¸…ç†å® ç‰©æ¯›", "description": "å®˜æ–¹æ ¸å¿ƒç”¨é€”ï¼šæ¸…ç†å®¶ä¸­çš„çŒ«æ¯›ç‹—æ¯›" }}
  ]
}}

Output JSON only, no other text."""


# [UPDATED] ç»´åº¦å‘ç° Prompt (åŠ å…¥äº§å“ä¿¡æ¯ç‰ˆ)
DIMENSION_DISCOVERY_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“ç»ç†å’Œç”¨æˆ·ç ”ç©¶ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹**äº§å“å®˜æ–¹ä¿¡æ¯**å’Œ**ç”¨æˆ·è¯„è®ºæ ·æœ¬**ï¼Œæ„å»ºè¯¥äº§å“çš„æ ¸å¿ƒè¯„ä»·ç»´åº¦æ¨¡å‹ã€‚

# äº§å“å®˜æ–¹ä¿¡æ¯
- **äº§å“æ ‡é¢˜**: {product_title}
- **æ ¸å¿ƒå–ç‚¹ (Bullet Points)**: 
{bullet_points}

# ç”¨æˆ·è¯„è®ºæ ·æœ¬ ({count}æ¡)
{reviews_text}

# ä»»åŠ¡
æç‚¼å‡º 5-8 ä¸ªæ ¸å¿ƒè¯„ä»·ç»´åº¦ã€‚

# è¦æ±‚
1. **ç»“åˆå®˜æ–¹å®šä¹‰ä¸ç”¨æˆ·è§†è§’**: ç»´åº¦åç§°åº”å°½é‡ä½¿ç”¨å®˜æ–¹æœ¯è¯­ï¼ˆå¦‚æ¥è‡ªå–ç‚¹ï¼‰ï¼Œä½†å¿…é¡»èƒ½è¦†ç›–ç”¨æˆ·çš„å®é™…åé¦ˆã€‚
2. **ç»´åº¦åç§°**: ä½¿ç”¨ç®€ç»ƒçš„ä¸­æ–‡ï¼ˆå¦‚ï¼šå¤–è§‚è®¾è®¡ã€ç»“æ„åšå·¥ã€ææ–™è´¨æ„Ÿã€åŠŸèƒ½è¡¨ç°ã€å®‰å…¨æ€§ã€æ€§ä»·æ¯”ï¼‰ã€‚
3. **ç»´åº¦å®šä¹‰**: ç”¨ä¸€å¥è¯æè¿°è¯¥ç»´åº¦åŒ…å«çš„å…·ä½“å†…å®¹ï¼Œç”¨äºæŒ‡å¯¼åç»­åˆ†ç±»ã€‚
4. **äº’æ–¥æ€§**: ç»´åº¦ä¹‹é—´ä¸è¦é‡å ï¼Œå„ç»´åº¦å®šä¹‰è¾¹ç•Œæ¸…æ™°ã€‚
5. **è¦†ç›–ç‡**: 
   - å¿…é¡»è¦†ç›–è¯„è®ºä¸­å‡ºç°çš„ä¸»è¦ç—›ç‚¹å’Œçˆ½ç‚¹
   - ä¹Ÿè¦åŒ…å«äº§å“å–ç‚¹ä¸­å¼ºè°ƒä½†ç”¨æˆ·å¯èƒ½"æ²‰é»˜æ»¡æ„"çš„ç»´åº¦ï¼ˆä¾¿äºåç»­ç›‘æ§ï¼‰
6. **æ•°é‡æ§åˆ¶**: æç‚¼ 5-8 ä¸ªæœ€æ ¸å¿ƒçš„ç»´åº¦ï¼Œä¸è¦è¿‡å¤šã€‚

# è¾“å‡ºæ ¼å¼ (JSON Only)
{{
  "dimensions": [
    {{ "name": "ç»´åº¦åç§°", "description": "è¯¥ç»´åº¦çš„å…·ä½“å®šä¹‰ï¼Œæè¿°å®ƒåŒ…å«å“ªäº›å†…å®¹" }},
    ...
  ]
}}

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""


# [UPDATED] åŠ¨æ€ç»´åº¦æå– Prompt - 5ç±»æ´å¯Ÿç³»ç»Ÿ
INSIGHT_EXTRACTION_PROMPT_DYNAMIC = """# Role
äºšé©¬é€Šè¯„è®ºæ·±åº¦åˆ†æå¸ˆ

# Task
åˆ†æè¯„è®ºï¼Œæå–å…³é”®æ´å¯Ÿï¼Œå¹¶å°†å…¶**ä¸¥æ ¼å½’ç±»**åˆ°æŒ‡å®šçš„äº§å“ç»´åº¦ä¸­ã€‚

# Input
åŸæ–‡: {original_text}
è¯‘æ–‡: {translated_text}

# å¿…é¡»éµå¾ªçš„ç»´åº¦æ ‡å‡† (Schema)
è¯·åªä½¿ç”¨ä»¥ä¸‹ç»´åº¦è¿›è¡Œå½’ç±»ã€‚å¦‚æœå†…å®¹å®Œå…¨ä¸å±äºä»¥ä¸‹ä»»ä½•ç»´åº¦ï¼Œè¯·å½’ç±»ä¸º "å…¶ä»–"ã€‚
{schema_str}

# 5ç±»æ´å¯Ÿç±»å‹å®šä¹‰ (CRITICAL - è¯·ä¸¥æ ¼åŒºåˆ†)
è¯·å°†è¯„è®ºæ‹†è§£ä¸ºå…·ä½“çš„æ´å¯Ÿç‚¹ï¼Œå¹¶å½’ç±»ä¸ºä»¥ä¸‹ 5 ç§ç±»å‹ä¹‹ä¸€ï¼š

1. **strength (äº§å“ä¼˜åŠ¿/å–ç‚¹)**: ç”¨æˆ·æ˜ç¡®è¡¨æ‰¬çš„åŠŸèƒ½æˆ–ä½“éªŒã€‚
   - ç¤ºä¾‹: "å¸åŠ›éå¸¸å¼ºåŠ²"ã€"ç»­èˆªæ¯”é¢„æœŸé•¿"ã€"å¤–è§‚æ¼‚äº®"
   - ç”¨é€”: æç‚¼ä¸º Listing å–ç‚¹

2. **weakness (æ”¹è¿›ç©ºé—´/ç—›ç‚¹)**: ç”¨æˆ·åæ§½çš„ç¼ºé™·ã€Bug æˆ–ä¸æ»¡ã€‚
   - ç¤ºä¾‹: "ç”µæ± ç»­èˆªå¤ªçŸ­äº†"ã€"å¡‘æ–™æ„Ÿå¼º"ã€"å™ªéŸ³å¤ªå¤§"
   - ç”¨é€”: äº§å“æ”¹è¿›ä¾æ®

3. **suggestion (ç”¨æˆ·å»ºè®®/Feature Request)**: ç”¨æˆ·ä¸»åŠ¨æå‡ºçš„æ”¹è¿›å»ºè®®æˆ–æœŸæœ›åŠŸèƒ½ã€‚
   - ç¤ºä¾‹: "å¦‚æœèƒ½åŠ ä¸ªLEDç¯å°±å¥½äº†"ã€"å¸Œæœ›å¢åŠ å®šæ—¶åŠŸèƒ½"
   - ç”¨é€”: äº§å“ç»ç†ç›´æ¥éœ€æ±‚æ¥æº

4. **scenario (å…·ä½“ä½¿ç”¨åœºæ™¯/è¡Œä¸ºæ•…äº‹)**: æè¿°**å…·ä½“çš„**ä½¿ç”¨è¿‡ç¨‹æˆ–è¡Œä¸ºæ•…äº‹ã€‚
   - ç¤ºä¾‹: "æˆ‘è¯•å›¾ç”¨æ¥æ¸…ç†è½¦åº“çš„é”¯æœ«ï¼Œä½†æ˜¯å¸å˜´è¢«å µä½äº†"ã€"ç»™å®å®ç”¨ç€éå¸¸æ–¹ä¾¿ï¼Œæ™šä¸Šå–‚å¥¶æ—¶ä¸€é”®å¼€å¯"
   - âš ï¸ é‡è¦åŒºåˆ«ï¼šä¸5Wæ ‡ç­¾ï¼ˆWhere/Whenï¼‰ä¸åŒï¼
     - 5Wæ ‡ç­¾æ˜¯**ç®€å•åè¯**: "å§å®¤"ã€"å¨æˆ¿"ã€"æ—©ä¸Š"
     - scenarioæ˜¯**åŠ¨æ€è¡Œä¸ºæè¿°**: "åœ¨å¨æˆ¿åšé¥­æ—¶è¯•å›¾æ¸…ç†é¢ç²‰"
   - å¦‚æœåªæ˜¯ç®€å•çš„åœ°ç‚¹/æ—¶é—´åè¯ï¼Œ**ä¸è¦**æå–ä¸º scenario

5. **emotion (å¼ºçƒˆæƒ…æ„Ÿæ´å¯Ÿ)**: ç”¨æˆ·è¡¨è¾¾çš„å¼ºçƒˆæƒ…ç»ªï¼ˆæ„¤æ€’/æƒŠå–œ/å¤±æœ›/æ„ŸåŠ¨ï¼‰ã€‚
   - ç¤ºä¾‹: "æˆ‘å¯¹æ­¤æ„Ÿåˆ°æå…¶å¤±æœ›"ã€"è¿™æ˜¯æˆ‘ä¹°è¿‡æœ€å¥½çš„ä¸œè¥¿"ã€"åæ‚”æ²¡æ—©ç‚¹ä¹°"
   - ç”¨é€”: è¿è¥å›¢é˜Ÿæƒ…ç»ªé¢„è­¦ã€å¥½è¯„ç´ æ

# Output Format (JSON Array)
[
  {{
    "type": "weakness", 
    "dimension": "ä»ä¸Šè¿°ç»´åº¦ä¸­é€‰æ‹©ä¸€ä¸ª", 
    "quote": "åŸæ–‡å¼•ç”¨", 
    "quote_translated": "å¼•ç”¨ç¿»è¯‘",
    "analysis": "ç®€è¦åˆ†æ",
    "sentiment": "positive/negative/neutral"
  }}
]

# é‡è¦è§„åˆ™
1. **æ¯æ¡è¯„è®ºå¿…é¡»è‡³å°‘æå–1ä¸ªæ´å¯Ÿ**ï¼Œå³ä½¿è¯„è®ºå¾ˆçŸ­ã€‚
2. **dimension å­—æ®µå¿…é¡»ä»ç»´åº¦æ ‡å‡†ä¸­é€‰æ‹©**ï¼Œä¸èƒ½è‡ªå·±ç¼–é€ æ–°ç»´åº¦ã€‚
3. å¯¹äºç®€çŸ­çš„æ­£é¢è¯„è®ºï¼ˆå¦‚"Amazing!"ï¼‰ï¼Œæå–ä¸º emotion ç±»å‹ã€‚
4. å¯¹äºç®€çŸ­çš„è´Ÿé¢è¯„è®ºï¼ˆå¦‚"Terrible"ï¼‰ï¼Œæå–ä¸º weakness ç±»å‹ã€‚
5. æå–è¦"é¢—ç²’åº¦ç»†"ï¼Œä¸è¦ç¬¼ç»Ÿåœ°è¯´"è´¨é‡ä¸å¥½"ï¼Œè¦è¯´"å¡‘æ–™æ„Ÿå¼º"æˆ–"æŒ‰é”®æ¾åŠ¨"ã€‚
6. ç»å¯¹ä¸è¦è¿”å›ç©ºæ•°ç»„ []ï¼Œè‡³å°‘è¦æœ‰1ä¸ªæ´å¯Ÿã€‚
7. scenario å¿…é¡»æ˜¯**å…·ä½“çš„è¡Œä¸ºæè¿°**ï¼Œä¸èƒ½æ˜¯ç®€å•çš„åœ°ç‚¹/æ—¶é—´åè¯ã€‚
"""


# [UPDATED] Insight extraction prompt - 5ç±»æ´å¯Ÿç³»ç»Ÿ (æ— ç»´åº¦ Schema ç‰ˆæœ¬)
INSIGHT_EXTRACTION_PROMPT = """# Role
äºšé©¬é€Šè¯„è®ºæ·±åº¦åˆ†æå¸ˆ

# Task
åˆ†æä»¥ä¸‹è¯„è®ºï¼Œæå–å…³é”®çš„ç”¨æˆ·æ´å¯Ÿã€‚**æ¯æ¡è¯„è®ºå¿…é¡»è‡³å°‘æå–1ä¸ªæ´å¯Ÿ**ã€‚

# Input
åŸæ–‡: {original_text}
è¯‘æ–‡: {translated_text}

# 5ç±»æ´å¯Ÿç±»å‹å®šä¹‰ (CRITICAL - è¯·ä¸¥æ ¼åŒºåˆ†)
è¯·å°†è¯„è®ºæ‹†è§£ä¸ºå…·ä½“çš„æ´å¯Ÿç‚¹ï¼Œå¹¶å½’ç±»ä¸ºä»¥ä¸‹ 5 ç§ç±»å‹ä¹‹ä¸€ï¼š

1. **strength (äº§å“ä¼˜åŠ¿/å–ç‚¹)**: ç”¨æˆ·æ˜ç¡®è¡¨æ‰¬çš„åŠŸèƒ½æˆ–ä½“éªŒã€‚
   - ç¤ºä¾‹: "å¸åŠ›éå¸¸å¼ºåŠ²"ã€"ç»­èˆªæ¯”é¢„æœŸé•¿"
   - ç”¨é€”: æç‚¼ä¸º Listing å–ç‚¹

2. **weakness (æ”¹è¿›ç©ºé—´/ç—›ç‚¹)**: ç”¨æˆ·åæ§½çš„ç¼ºé™·ã€Bug æˆ–ä¸æ»¡ã€‚
   - ç¤ºä¾‹: "ç”µæ± ç»­èˆªå¤ªçŸ­äº†"ã€"å¡‘æ–™æ„Ÿå¼º"
   - ç”¨é€”: äº§å“æ”¹è¿›ä¾æ®

3. **suggestion (ç”¨æˆ·å»ºè®®/Feature Request)**: ç”¨æˆ·ä¸»åŠ¨æå‡ºçš„æ”¹è¿›å»ºè®®æˆ–æœŸæœ›åŠŸèƒ½ã€‚
   - ç¤ºä¾‹: "å¦‚æœèƒ½åŠ ä¸ªLEDç¯å°±å¥½äº†"
   - ç”¨é€”: äº§å“ç»ç†ç›´æ¥éœ€æ±‚æ¥æº

4. **scenario (å…·ä½“ä½¿ç”¨åœºæ™¯/è¡Œä¸ºæ•…äº‹)**: æè¿°**å…·ä½“çš„**ä½¿ç”¨è¿‡ç¨‹æˆ–è¡Œä¸ºæ•…äº‹ã€‚
   - ç¤ºä¾‹: "æˆ‘è¯•å›¾ç”¨æ¥æ¸…ç†è½¦åº“çš„é”¯æœ«ï¼Œä½†æ˜¯å¸å˜´è¢«å µä½äº†"
   - âš ï¸ é‡è¦ï¼šä¸è¦å’Œç®€å•çš„åœ°ç‚¹/æ—¶é—´åè¯æ··æ·†ï¼

5. **emotion (å¼ºçƒˆæƒ…æ„Ÿæ´å¯Ÿ)**: ç”¨æˆ·è¡¨è¾¾çš„å¼ºçƒˆæƒ…ç»ªã€‚
   - ç¤ºä¾‹: "æˆ‘å¯¹æ­¤æ„Ÿåˆ°æå…¶å¤±æœ›"ã€"è¿™æ˜¯æˆ‘ä¹°è¿‡æœ€å¥½çš„ä¸œè¥¿"
   - ç”¨é€”: è¿è¥å›¢é˜Ÿæƒ…ç»ªé¢„è­¦

# ç»´åº¦åˆ¤æ–­
è¯·æ ¹æ®è¯„è®ºå†…å®¹è‡ªåŠ¨åˆ¤æ–­ç»´åº¦ï¼ˆå¦‚ï¼šæ•´ä½“æ»¡æ„åº¦ã€äº§å“è´¨é‡ã€ä½¿ç”¨ä½“éªŒã€ç‰©æµæœåŠ¡ã€æ€§ä»·æ¯”ç­‰ï¼‰ã€‚

# Output Format (JSON Array)
[
  {{
    "type": "strength", 
    "dimension": "æ•´ä½“æ»¡æ„åº¦",
    "quote": "Amazing toy", 
    "quote_translated": "å¤ªæ£’äº†",
    "analysis": "ç”¨æˆ·å¯¹äº§å“é«˜åº¦è®¤å¯ï¼Œè¡¨è¾¾äº†å¼ºçƒˆçš„æ­£é¢æƒ…æ„Ÿ",
    "sentiment": "positive"
  }},
  {{
    "type": "emotion",
    "dimension": "è´­ä¹°ä½“éªŒ",
    "quote": "Great buy",
    "quote_translated": "ä¹°å¾—å€¼",
    "analysis": "ç”¨æˆ·è®¤ä¸ºè¿™æ¬¡è´­ä¹°ç‰©è¶…æ‰€å€¼",
    "sentiment": "positive"
  }}
]

# é‡è¦è§„åˆ™
1. **æ¯æ¡è¯„è®ºå¿…é¡»è‡³å°‘æå–1ä¸ªæ´å¯Ÿ**ï¼Œå³ä½¿è¯„è®ºå¾ˆçŸ­ã€‚
2. å¯¹äºç®€çŸ­çš„æ­£é¢è¯„è®ºï¼ˆå¦‚"Amazing!"ã€"Love it!"ï¼‰ï¼Œæå–ä¸º emotion ç±»å‹ã€‚
3. å¯¹äºç®€çŸ­çš„è´Ÿé¢è¯„è®ºï¼ˆå¦‚"Terrible"ï¼‰ï¼Œæå–ä¸º weakness ç±»å‹ã€‚
4. æå–è¦"é¢—ç²’åº¦ç»†"ï¼Œä¸è¦ç¬¼ç»Ÿåœ°è¯´"è´¨é‡ä¸å¥½"ï¼Œè¦è¯´"å¡‘æ–™æ„Ÿå¼º"æˆ–"æŒ‰é”®æ¾åŠ¨"ã€‚
5. ç»å¯¹ä¸è¦è¿”å›ç©ºæ•°ç»„ []ï¼Œè‡³å°‘è¦æœ‰1ä¸ªæ´å¯Ÿã€‚
6. scenario å¿…é¡»æ˜¯**å…·ä½“çš„è¡Œä¸ºæè¿°**ï¼Œä¸èƒ½æ˜¯ç®€å•çš„åœ°ç‚¹/æ—¶é—´åè¯ã€‚
"""


class InsightType(str, Enum):
    """Insight type enumeration"""
    STRENGTH = "strength"
    WEAKNESS = "weakness"
    SUGGESTION = "suggestion"
    SCENARIO = "scenario"
    EMOTION = "emotion"





# [UPDATED] 5W Model Extraction Prompt (æ— æ ‡ç­¾åº“æ¨¡å¼ - å¼€æ”¾æå–)
THEME_EXTRACTION_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºè¥é”€åˆ†æä¸“å®¶ã€‚è¯·åŸºäº"5Wåˆ†ææ³•"åˆ†æä»¥ä¸‹å•†å“è¯„è®ºï¼Œæå–å…³é”®çš„å¸‚åœºè¦ç´ ã€‚

è¯„è®ºåŸæ–‡ï¼ˆè‹±æ–‡ï¼‰ï¼š
{original_text}

è¯„è®ºç¿»è¯‘ï¼ˆä¸­æ–‡ï¼‰ï¼š
{translated_text}

è¯·ä»è¯„è®ºä¸­æå–ä»¥ä¸‹ 5 ç±»æ ¸å¿ƒè¦ç´ ï¼ˆå¦‚æœæŸç±»æ²¡æœ‰æåŠï¼Œåˆ™ç•™ç©ºï¼‰ï¼š

1. **whoï¼ˆä½¿ç”¨è€…/äººç¾¤ï¼‰**: 
   - å®šä¹‰: è°åœ¨ä½¿ç”¨äº§å“ï¼Ÿ
   - ç¤ºä¾‹: è€å¹´äººã€å­¦ç”Ÿã€å® ç‰©ä¸»ã€å¦»å­ã€å·¥ç¨‹å¸ˆã€‚
2. **whereï¼ˆä½¿ç”¨åœ°ç‚¹ï¼‰**: 
   - å®šä¹‰: åœ¨ä»€ä¹ˆç‰©ç†ç©ºé—´ä½¿ç”¨ï¼Ÿ
   - ç¤ºä¾‹: å§å®¤ã€åŠå…¬å®¤ã€æˆ¿è½¦(RV)ã€è½¦åº“ã€æˆ·å¤–éœ²è¥ã€‚
3. **whenï¼ˆä½¿ç”¨æ—¶åˆ»ï¼‰**: 
   - å®šä¹‰: åœ¨ä»€ä¹ˆæ—¶é—´æˆ–ç‰¹å®šæƒ…å¢ƒä¸‹ä½¿ç”¨ï¼Ÿ
   - ç¤ºä¾‹: ç¡å‰ã€ç´§æ€¥åœç”µæ—¶ã€åœ£è¯èŠ‚æ—©æ™¨ã€è¿åŠ¨åã€‚
4. **whyï¼ˆè´­ä¹°åŠ¨æœºï¼‰**: 
   - å®šä¹‰: ä¿ƒä½¿ç”¨æˆ·ä¸‹å•çš„è§¦å‘ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ(Purchase Driver)
   - ç¤ºä¾‹: æ—§çš„åäº†(æ›¿ä»£)ã€ä½œä¸ºç”Ÿæ—¥ç¤¼ç‰©ã€ä¸ºäº†çœé’±ã€æ¬æ–°å®¶ã€è¢«å¹¿å‘Šç§è‰ã€‚
5. **whatï¼ˆå¾…åŠä»»åŠ¡/ç”¨é€”ï¼‰**: 
   - å®šä¹‰: ç”¨æˆ·ç”¨å®ƒæ¥è§£å†³ä»€ä¹ˆå…·ä½“é—®é¢˜ï¼Ÿ(Jobs to be Done)
   - æ³¨æ„: ä¸æ˜¯åˆ—ä¸¾åŠŸèƒ½ï¼Œè€Œæ˜¯åˆ—ä¸¾ä»»åŠ¡ã€‚
   - ç¤ºä¾‹: æ¸…ç†åœ°æ¯¯ä¸Šçš„çŒ«æ¯›(è€Œä¸æ˜¯"å¸åŠ›å¤§")ã€ç¼“è§£èƒŒç—›(è€Œä¸æ˜¯"äººä½“å·¥å­¦")ã€å“„å­©å­ç¡è§‰ã€‚

æ³¨æ„äº‹é¡¹ï¼š
- æå–çš„å†…å®¹å¿…é¡»ç®€ç»ƒã€å‡†ç¡®ã€‚
- å°½é‡æå–å®Œæ•´è¯­ä¹‰ï¼Œå¦‚"æ¸…ç†çŒ«æ¯›"ä¼˜äº"çŒ«æ¯›"ã€‚
- å¿…é¡»åŸºäºè¯„è®ºäº‹å®ï¼Œä¸å¯ç¼–é€ ã€‚
- å¦‚æœå†…å®¹æ¥è‡ªè‹±æ–‡åŸæ–‡ï¼Œè¯·åŒæ—¶æä¾›è‹±æ–‡åŸæ–‡å’Œä¸­æ–‡ç¿»è¯‘ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "who": [
    {{
      "content": "å­©å­",
      "content_original": "for kids",
      "content_translated": "ç»™å­©å­",
      "explanation": "ç”¨æˆ·ä¹°ç»™å­©å­ä½œä¸ºç¤¼ç‰©"
    }}
  ],
  "what": [],
  "why": [],
  "where": [],
  "when": []
}}
"""


# [UPDATED] 5W æ ‡ç­¾å‘ç° Prompt (å­¦ä¹ é˜¶æ®µ - ç»“åˆäº§å“å®˜æ–¹ä¿¡æ¯ + ç”¨æˆ·è¯„è®º)
CONTEXT_DISCOVERY_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¸‚åœºè¥é”€ä¸“å®¶å’Œç”¨æˆ·ç ”ç©¶å‘˜ã€‚è¯·åŸºäºä»¥ä¸‹**äº§å“å®˜æ–¹ä¿¡æ¯**å’Œ**ç”¨æˆ·è¯„è®ºæ ·æœ¬**ï¼Œæ„å»ºè¯¥äº§å“çš„"5W ç”¨æˆ·ä¸å¸‚åœºæ¨¡å‹"ã€‚

# äº§å“å®˜æ–¹ä¿¡æ¯ï¼ˆå–å®¶å®šä¹‰ï¼‰
- **äº§å“æ ‡é¢˜**: {product_title}
- **æ ¸å¿ƒå–ç‚¹ (Bullet Points)**:
{bullet_points}

# ç”¨æˆ·è¯„è®ºæ ·æœ¬ï¼ˆ{count}æ¡ä¹°å®¶åé¦ˆï¼‰
{reviews_text}

# ä»»åŠ¡
è¯·ç»¼åˆå®˜æ–¹å®šä½ä¸ç”¨æˆ·åé¦ˆï¼Œè¯†åˆ«å¹¶å½’çº³å‡ºä»¥ä¸‹ 5 ç±»æ ¸å¿ƒè¦ç´ ï¼Œæ¯ç±»æå– **Top 5-8 ä¸ªå…¸å‹æ ‡ç­¾**ï¼š

1. **Who (äººç¾¤)**: è°æ˜¯ä¸»è¦ç”¨æˆ·ï¼Ÿ
   - ä¼˜å…ˆå‚è€ƒå®˜æ–¹å®šä½ï¼ˆå¦‚: "Perfect for seniors"ï¼‰
   - ç»“åˆç”¨æˆ·å®é™…åé¦ˆï¼ˆå¦‚: "bought for my mom"ï¼‰
   - è§’è‰²/èº«ä»½ï¼Œå¦‚: è€å¹´äººã€æ–°æ‰‹å¦ˆå¦ˆã€å­¦ç”Ÿã€å® ç‰©ä¸»
   - å®¶åº­å…³ç³»ï¼Œå¦‚: ç»™çˆ¶æ¯ä¹°çš„ã€é€ç»™å¦»å­ã€å­©å­çš„ç¤¼ç‰©

2. **Where (åœ°ç‚¹)**: åœ¨å“ªé‡Œä½¿ç”¨ï¼Ÿ
   - ä¼˜å…ˆå‚è€ƒå®˜æ–¹å®šä½ï¼ˆå¦‚: "for Home Office, Garage"ï¼‰
   - ç»“åˆç”¨æˆ·å®é™…ä½¿ç”¨åœºæ™¯
   - ç‰©ç†ç©ºé—´ï¼Œå¦‚: å§å®¤ã€åŠå…¬å®¤ã€å¨æˆ¿ã€è½¦ä¸Šã€æˆ¿è½¦(RV)ã€æˆ·å¤–éœ²è¥

3. **When (æ—¶åˆ»)**: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ï¼Ÿ
   - æ—¶é—´ç‚¹ï¼Œå¦‚: æ—©ä¸Šã€ç¡å‰ã€æ·±å¤œ
   - è§¦å‘æ—¶æœºï¼Œå¦‚: åœç”µæ—¶ã€æ—…è¡Œæ—¶ã€è¿åŠ¨åã€èŠ‚å‡æ—¥

4. **Why (åŠ¨æœº)**: è´­ä¹°çš„è§¦å‘ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ(Purchase Driver)
   - æ›¿ä»£éœ€æ±‚ï¼Œå¦‚: æ—§çš„åäº†ã€å‡çº§æ¢ä»£
   - é€ç¤¼éœ€æ±‚ï¼Œå¦‚: ç”Ÿæ—¥ç¤¼ç‰©ã€åœ£è¯ç¤¼ç‰©ã€ä¹”è¿é€ç¤¼
   - å¤–éƒ¨é©±åŠ¨ï¼Œå¦‚: è¢«ç§è‰ã€çœ‹äº†è¯„æµ‹ã€æœ‹å‹æ¨è

5. **What (ä»»åŠ¡)**: ç”¨æˆ·è¯•å›¾ç”¨å®ƒå®Œæˆä»€ä¹ˆå…·ä½“ä»»åŠ¡ï¼Ÿ(Jobs to be Done)
   - **é‡ç‚¹å…³æ³¨å®˜æ–¹å®£ä¼ çš„æ ¸å¿ƒç”¨é€”**ï¼ˆå¦‚: "remove pet hair", "eliminate odors"ï¼‰
   - æ³¨æ„: æ˜¯å…·ä½“ä»»åŠ¡ï¼Œä¸æ˜¯äº§å“åŠŸèƒ½
   - å¦‚: æ¸…ç†åœ°æ¯¯ä¸Šçš„å® ç‰©æ¯›ã€ç¼“è§£èƒŒç—›ã€å“„å­©å­ç¡è§‰ã€å»é™¤å¼‚å‘³

# è¦æ±‚
1. **æ ‡ç­¾åç§°ä½¿ç”¨ç®€ç»ƒçš„ä¸­æ–‡**ï¼ˆ2-6ä¸ªå­—æœ€ä½³ï¼‰ã€‚
2. **åˆå¹¶åŒä¹‰è¯**ï¼šå¦‚"å¦ˆå¦ˆ"ã€"è€å¦ˆ"ã€"æ¯äº²"åº”ç»Ÿä¸€ä¸ºä¸€ä¸ªæ ‡ç­¾ã€‚
3. **ä¿æŒé¢—ç²’åº¦ä¸€è‡´**ï¼šä¸è¦å¤ªç²—ï¼ˆå¦‚"å®¶äºº"ï¼‰ä¹Ÿä¸è¦å¤ªç»†ï¼ˆå¦‚"62å²çš„ç‹¬å±…æ¯äº²"ï¼‰ã€‚
4. **å®˜æ–¹ä¿¡æ¯ä¼˜å…ˆ**ï¼šå¦‚æœå®˜æ–¹æ˜ç¡®æåˆ°çš„äººç¾¤/åœºæ™¯/ç”¨é€”ï¼Œå³ä½¿è¯„è®ºä¸­æ²¡æåŠä¹Ÿåº”åˆ—å…¥ã€‚
5. **æä¾›ç®€çŸ­æè¿°**ï¼šç”¨ä¸€å¥è¯è§£é‡Šè¯¥æ ‡ç­¾çš„å«ä¹‰ï¼Œä¾¿äºåç»­å½’ç±»åˆ¤æ–­ã€‚

# è¾“å‡ºæ ¼å¼ (JSON Only)
{{
  "who": [
    {{ "name": "è€å¹´äºº", "description": "å®˜æ–¹å®šä½çš„æ ¸å¿ƒç”¨æˆ·ç¾¤ä½“ï¼Œé€‚åˆéœ€è¦ç…§é¡¾çš„è€äºº" }},
    {{ "name": "å® ç‰©ä¸»", "description": "å…»çŒ«æˆ–å…»ç‹—çš„ç”¨æˆ·" }}
  ],
  "where": [
    {{ "name": "å§å®¤", "description": "å§å®¤/ç¡çœ åœºæ™¯ä¸‹ä½¿ç”¨" }},
    {{ "name": "è½¦åº“", "description": "å®˜æ–¹æ¨èçš„ä½¿ç”¨åœºæ™¯ä¹‹ä¸€" }}
  ],
  "when": [
    {{ "name": "ç¡å‰", "description": "ç¡è§‰å‰ä½¿ç”¨" }}
  ],
  "why": [
    {{ "name": "æ›¿ä»£æ—§å“", "description": "åŸæœ‰äº§å“æŸåéœ€è¦æ›´æ¢" }},
    {{ "name": "é€ç¤¼", "description": "ä½œä¸ºç¤¼ç‰©é€ç»™ä»–äºº" }}
  ],
  "what": [
    {{ "name": "æ¸…ç†å® ç‰©æ¯›", "description": "å®˜æ–¹æ ¸å¿ƒç”¨é€”ï¼šæ¸…ç†å®¶ä¸­çš„çŒ«æ¯›ç‹—æ¯›" }},
    {{ "name": "å»é™¤å¼‚å‘³", "description": "å®˜æ–¹æ ¸å¿ƒç”¨é€”ï¼šæ¶ˆé™¤å® ç‰©æˆ–å…¶ä»–å¼‚å‘³" }}
  ]
}}

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""


# [UPDATED] 5W å®šå‘æå– Prompt (æ‰§è¡Œé˜¶æ®µ - Executionï¼Œå¸¦è¯æ®çš„å¯è§£é‡Šå¼ºåˆ¶å½’ç±»)
THEME_EXTRACTION_PROMPT_WITH_SCHEMA = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºè¥é”€åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å•†å“è¯„è®ºï¼Œè¯†åˆ«å…¶ä¸­æ¶‰åŠçš„ 5W è¦ç´ ã€‚

è¯„è®ºåŸæ–‡ï¼ˆè‹±æ–‡ï¼‰ï¼š
{original_text}

è¯„è®ºç¿»è¯‘ï¼ˆä¸­æ–‡ï¼‰ï¼š
{translated_text}

# æ ‡å‡†æ ‡ç­¾åº“ (Schema - åªèƒ½ä»ä»¥ä¸‹æ ‡ç­¾ä¸­é€‰æ‹©)
{schema_str}

# ä»»åŠ¡è§„åˆ™
1. **å¼ºåˆ¶å½’ç±»**ï¼šä½ æå–çš„ `tag` å­—æ®µå¿…é¡»ä¸¥æ ¼ç­‰äºä¸Šè¿°æ ‡ç­¾åº“ä¸­çš„æ ‡ç­¾åã€‚ä¸è¦ç¼–é€ æ–°æ ‡ç­¾ã€‚
2. **è¯æ®ç•™å­˜**ï¼šå¿…é¡»å¼•ç”¨åŸæ–‡ `quote`ï¼ˆè‹±æ–‡ï¼‰å’Œ `quote_translated`ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰ä½œä¸ºåˆ¤æ–­ä¾æ®ã€‚
3. **è§£é‡Šè¯´æ˜**ï¼šæä¾›ç®€çŸ­çš„ `explanation` è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·å½’ç±»ã€‚
4. **å¤šé€‰**ï¼šå¦‚æœè¯„è®ºæ¶‰åŠå¤šä¸ªæ ‡ç­¾ï¼Œè¯·ç”Ÿæˆå¤šä¸ªå¯¹è±¡ã€‚
5. **å¿½ç•¥æ— å…³**ï¼šå¦‚æœè¯„è®ºå†…å®¹ä¸åŒ¹é…æŸä¸ªç±»åˆ«çš„ä»»ä½•æ ‡ç­¾ï¼Œè¯¥ç±»åˆ«è¿”å›ç©ºæ•°ç»„ã€‚

# è¾“å‡ºæ ¼å¼ (JSON Only)
{{
  "who": [
    {{
      "tag": "è€å¹´äºº", 
      "quote": "bought this for my 80yo dad",
      "quote_translated": "ç»™æˆ‘80å²çš„çˆ¶äº²ä¹°çš„",
      "explanation": "è¯„è®ºæ˜ç¡®æåŠä¹°ç»™80å²çš„çˆ¶äº²"
    }}
  ],
  "where": [],
  "when": [],
  "why": [
    {{
      "tag": "é€ç¤¼",
      "quote": "as a gift for my mom",
      "quote_translated": "ä½œä¸ºç¤¼ç‰©é€ç»™å¦ˆå¦ˆ",
      "explanation": "ç”¨æˆ·æ˜ç¡®è¯´æ˜¯ä½œä¸ºç¤¼ç‰©é€ç»™æ¯äº²"
    }}
  ],
  "what": [
    {{
      "tag": "ç¼“è§£èƒŒç—›",
      "quote": "helps with my lower back pain",
      "quote_translated": "å¸®åŠ©ç¼“è§£æˆ‘çš„è…°ç—›",
      "explanation": "ç”¨æˆ·ä½¿ç”¨è¯¥äº§å“æ¥è§£å†³èƒŒç—›é—®é¢˜"
    }}
  ]
}}

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—ã€‚"""


# [NEW] Helper function for robust JSON parsing
def parse_json_safely(text: str):
    """
    Safely parse JSON from LLM output, handling markdown blocks and extra characters.
    """
    if not text:
        return None
        
    # 1. Try direct parsing
    try:
        return json.loads(text)
    except:
        pass
    
    # 2. Try to extract from ```json ... ``` blocks
    match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', text)
    if match:
        try:
            return json.loads(match.group(1))
        except:
            pass
            
    # 3. Try to find the first [ or { and last ] or }
    try:
        text = text.strip()
        if '}' in text: # Likely an object
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end != -1:
                return json.loads(text[start:end])
        if ']' in text: # Likely an array
            start = text.find('[')
            end = text.rfind(']') + 1
            if start != -1 and end != -1:
                return json.loads(text[start:end])
    except:
        pass
        
    return None


class TranslationService:
    """
    Service for translating Amazon reviews using Qwen API.
    
    Features:
    - Context-aware e-commerce translation
    - Sentiment analysis
    - Automatic retry with exponential backoff
    - Rate limiting awareness
    """
    
    def __init__(self):
        """Initialize the translation service with Qwen API client."""
        if not settings.QWEN_API_KEY:
            logger.warning("QWEN_API_KEY not configured, translation will fail")
            self.client = None
        else:
            self.client = OpenAI(
                api_key=settings.QWEN_API_KEY,
                base_url=settings.QWEN_API_BASE,
            )
        self.model = settings.QWEN_MODEL
    
    def _check_client(self) -> bool:
        """Check if API client is properly configured."""
        if self.client is None:
            logger.error("Translation service not configured: missing API key")
            return False
        return True
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def translate_text(self, text: str) -> str:
        """
        Translate English text to Chinese with e-commerce context.
        """
        if not self._check_client():
            raise RuntimeError("Translation service not configured")
        
        if not text or not text.strip():
            return ""
        
        # Clean text: remove extra whitespace and normalize
        text = " ".join(text.split())
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": TRANSLATION_SYSTEM_PROMPT},
                    {"role": "user", "content": text}
                ],
                temperature=0.3,  # Lower temperature for more consistent translations
                max_tokens=2000,
                timeout=60.0,
            )
            
            translated = response.choices[0].message.content.strip()
            
            # Validate translation result
            if not translated or len(translated.strip()) == 0:
                logger.warning(f"Translation returned empty for text: {text[:100]}")
                # Retry with a more explicit prompt for short text
                if len(text) < 50:
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼Œå³ä½¿æ–‡æœ¬å¾ˆçŸ­ä¹Ÿè¦ç¿»è¯‘ã€‚"},
                            {"role": "user", "content": f"è¯·ç¿»è¯‘ï¼š{text}"}
                        ],
                        temperature=0.3,
                        max_tokens=500,
                    )
                    translated = response.choices[0].message.content.strip()
            
            if not translated or len(translated.strip()) == 0:
                # Fallback: return a note if translation truly fails
                logger.error(f"Translation failed to produce result for: {text[:100]}")
                raise ValueError(f"Translation returned empty for text: {text[:50]}")
            
            logger.debug(f"Translated: {text[:50]}... -> {translated[:50]}...")
            return translated
            
        except Exception as e:
            logger.error(f"Translation failed for text: {text[:100]}... Error: {e}")
            raise
    
    # ==========================================================================
    # ğŸ”¥ æ‰¹é‡ç¿»è¯‘æ–¹æ³•ï¼ˆ10 æ¡ä¸€æ‰¹ï¼Œæå‡ 10 å€æ•ˆç‡ï¼‰
    # ==========================================================================
    
    # æ‰¹é‡ç¿»è¯‘ç³»ç»Ÿæç¤º
    BATCH_TRANSLATION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­ç¾æ–‡åŒ–å·®å¼‚çš„èµ„æ·±äºšé©¬é€Šè·¨å¢ƒç”µå•†ç¿»è¯‘ä¸“å®¶ã€‚

## ä»»åŠ¡
å°†å¤šæ¡äºšé©¬é€Šè‹±æ–‡è¯„è®ºæ‰¹é‡ç¿»è¯‘æˆä¸­æ–‡ã€‚

## ç¿»è¯‘åŸåˆ™
1. **æ‹’ç»ç¿»è¯‘è…”**: ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡è¡¨è¾¾
2. **æƒ…æ„Ÿå¯¹é½**: ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œæƒ…ç»ª
3. **ç”µå•†é£æ ¼**: ä½¿ç”¨ç¬¦åˆä¸­å›½ç”µå•†çš„æ–‡æ¡ˆé£æ ¼

## è¾“å…¥/è¾“å‡ºæ ¼å¼
- è¾“å…¥: JSON å­—å…¸ï¼Œé”®ä¸ºè¯„è®º IDï¼Œå€¼ä¸ºè‹±æ–‡åŸæ–‡
- è¾“å‡º: JSON å­—å…¸ï¼Œé”®ä¸è¾“å…¥ä¸€è‡´ï¼Œå€¼ä¸ºä¸­æ–‡è¯‘æ–‡
- **ä¸¥æ ¼è¦æ±‚**: åªè¿”å› JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€Markdown æ ‡è®°æˆ–å…¶ä»–æ–‡å­—

## ç¤ºä¾‹
è¾“å…¥: {"r1": "Total lemon. Don't waste your money.", "r2": "Game changer for my morning routine."}
è¾“å‡º: {"r1": "ç®€ç›´æ˜¯ä¸ªæ¬¡å“ï¼åˆ«æµªè´¹é’±äº†ã€‚", "r2": "å½»åº•æ”¹å˜äº†æˆ‘æ¯å¤©æ—©ä¸Šçš„ä¹ æƒ¯ï¼ŒçœŸé¦™ï¼"}"""

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=15),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def translate_batch(self, reviews: List[dict]) -> dict:
        """
        æ‰¹é‡ç¿»è¯‘å¤šæ¡è¯„è®ºï¼ˆ10 æ¡ä¸€æ‰¹ï¼‰
        
        ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šä¸€æ¬¡ API è°ƒç”¨ç¿»è¯‘ 10 æ¡è¯„è®º
        - QPS æ¶ˆè€—é™ä½ 10 å€
        - æ€»ä½“æ•ˆç‡æå‡ 8-10 å€
        
        Args:
            reviews: è¯„è®ºåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"id": "xxx", "text": "original text"}
            
        Returns:
            ç¿»è¯‘ç»“æœå­—å…¸ï¼Œæ ¼å¼: {"id1": "translated1", "id2": "translated2", ...}
            
        Example:
            results = translation_service.translate_batch([
                {"id": "r1", "text": "Great product!"},
                {"id": "r2", "text": "Not worth the money."}
            ])
            # è¿”å›: {"r1": "å¾ˆæ£’çš„äº§å“ï¼", "r2": "ä¸å€¼è¿™ä¸ªä»·ã€‚"}
        """
        if not self._check_client():
            raise RuntimeError("Translation service not configured")
        
        if not reviews or len(reviews) == 0:
            return {}
        
        # æ„å»ºè¾“å…¥ JSON
        input_dict = {}
        for review in reviews:
            review_id = str(review.get("id", ""))
            text = review.get("text", "")
            if review_id and text and text.strip():
                # æˆªæ–­è¶…é•¿æ–‡æœ¬ï¼ˆé˜²æ­¢è¶…å‡º token é™åˆ¶ï¼‰
                text = " ".join(text.split())  # æ¸…ç†ç©ºç™½
                if len(text) > 2000:
                    text = text[:2000] + "..."
                input_dict[review_id] = text
        
        if not input_dict:
            return {}
        
        input_json = json.dumps(input_dict, ensure_ascii=False)
        
        logger.info(f"[æ‰¹é‡ç¿»è¯‘] å¼€å§‹ç¿»è¯‘ {len(input_dict)} æ¡è¯„è®º")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self.BATCH_TRANSLATION_SYSTEM_PROMPT},
                    {"role": "user", "content": input_json}
                ],
                temperature=0.3,
                max_tokens=8000,  # æ‰¹é‡ç¿»è¯‘éœ€è¦æ›´å¤š token
                timeout=120.0,   # æ‰¹é‡ç¿»è¯‘éœ€è¦æ›´é•¿è¶…æ—¶
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è§£æ JSON ç»“æœ
            result_dict = self._parse_batch_translation_result(result_text, input_dict)
            
            logger.info(f"[æ‰¹é‡ç¿»è¯‘] å®Œæˆ: è¾“å…¥ {len(input_dict)} æ¡, æˆåŠŸ {len(result_dict)} æ¡")
            
            return result_dict
            
        except Exception as e:
            logger.error(f"[æ‰¹é‡ç¿»è¯‘] API è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _parse_batch_translation_result(self, result_text: str, input_dict: dict) -> dict:
        """
        è§£ææ‰¹é‡ç¿»è¯‘ç»“æœï¼Œå¸¦å®¹é”™å¤„ç†
        
        å¤„ç†å¸¸è§çš„ LLM è¾“å‡ºé—®é¢˜ï¼š
        1. å¤šä½™çš„ Markdown ä»£ç å—æ ‡è®°
        2. å‰åæœ‰è§£é‡Šæ–‡å­—
        3. JSON æ ¼å¼é”™è¯¯
        """
        result_dict = {}
        
        # 1. å°è¯•æ¸…ç† Markdown ä»£ç å—
        clean_text = result_text
        if "```json" in clean_text:
            match = re.search(r'```json\s*(.*?)\s*```', clean_text, re.DOTALL)
            if match:
                clean_text = match.group(1)
        elif "```" in clean_text:
            match = re.search(r'```\s*(.*?)\s*```', clean_text, re.DOTALL)
            if match:
                clean_text = match.group(1)
        
        # 2. å°è¯•æå– JSON å¯¹è±¡
        json_match = re.search(r'\{[^{}]*\}', clean_text, re.DOTALL)
        if json_match:
            clean_text = json_match.group(0)
        
        # 3. å°è¯•è§£æ JSON
        try:
            result_dict = json.loads(clean_text)
            if isinstance(result_dict, dict):
                # éªŒè¯é”®ä¸è¾“å…¥ä¸€è‡´
                valid_result = {}
                for key in input_dict.keys():
                    if key in result_dict and result_dict[key]:
                        valid_result[key] = str(result_dict[key]).strip()
                return valid_result
        except json.JSONDecodeError as e:
            logger.warning(f"[æ‰¹é‡ç¿»è¯‘] JSON è§£æå¤±è´¥: {e}")
        
        # 4. è§£æå¤±è´¥ï¼Œå°è¯• json_repairï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import json_repair
            repaired = json_repair.loads(clean_text)
            if isinstance(repaired, dict):
                valid_result = {}
                for key in input_dict.keys():
                    if key in repaired and repaired[key]:
                        valid_result[key] = str(repaired[key]).strip()
                return valid_result
        except ImportError:
            pass
        except Exception as e:
            logger.warning(f"[æ‰¹é‡ç¿»è¯‘] json_repair å¤±è´¥: {e}")
        
        # 5. æœ€ç»ˆå›é€€ï¼šè¿”å›ç©ºï¼Œè®©è°ƒç”¨æ–¹é™çº§ä¸ºå•æ¡ç¿»è¯‘
        logger.error(f"[æ‰¹é‡ç¿»è¯‘] æ— æ³•è§£æç»“æœï¼ŒåŸå§‹è¾“å‡º: {result_text[:500]}")
        return {}
    
    def translate_batch_with_fallback(self, reviews: List[dict]) -> dict:
        """
        æ‰¹é‡ç¿»è¯‘ï¼Œå¸¦å•æ¡å›é€€æœºåˆ¶
        
        å¦‚æœæ‰¹é‡ç¿»è¯‘å¤±è´¥æˆ–éƒ¨åˆ†å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§ä¸ºå•æ¡ç¿»è¯‘
        
        Args:
            reviews: è¯„è®ºåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {"id": "xxx", "text": "original text"}
            
        Returns:
            ç¿»è¯‘ç»“æœå­—å…¸ï¼Œæ ¼å¼: {"id1": "translated1", "id2": "translated2", ...}
        """
        result = {}
        
        # 1. å°è¯•æ‰¹é‡ç¿»è¯‘
        try:
            batch_result = self.translate_batch(reviews)
            result.update(batch_result)
        except Exception as e:
            logger.warning(f"[æ‰¹é‡ç¿»è¯‘] æ‰¹é‡æ¨¡å¼å¤±è´¥ï¼Œé™çº§ä¸ºå•æ¡: {e}")
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰æœªç¿»è¯‘çš„è¯„è®ºï¼Œå•æ¡å›é€€
        for review in reviews:
            review_id = str(review.get("id", ""))
            text = review.get("text", "")
            
            if review_id and text and review_id not in result:
                try:
                    translated = self.translate_text(text)
                    if translated:
                        result[review_id] = translated
                        logger.debug(f"[æ‰¹é‡ç¿»è¯‘] å•æ¡å›é€€æˆåŠŸ: {review_id}")
                except Exception as e:
                    logger.warning(f"[æ‰¹é‡ç¿»è¯‘] å•æ¡å›é€€å¤±è´¥ {review_id}: {e}")
        
        return result
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def analyze_sentiment(self, text: str) -> Sentiment:
        """
        Analyze the sentiment of a review.
        """
        if not self._check_client():
            return Sentiment.NEUTRAL
        
        if not text or not text.strip():
            return Sentiment.NEUTRAL
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": SENTIMENT_ANALYSIS_PROMPT.format(review_text=text)}
                ],
                temperature=0.1,
                max_tokens=20,
                timeout=30.0,
            )
            
            result = response.choices[0].message.content.strip().lower()
            
            if "positive" in result:
                return Sentiment.POSITIVE
            elif "negative" in result:
                return Sentiment.NEGATIVE
            else:
                return Sentiment.NEUTRAL
                
        except Exception as e:
            logger.warning(f"Sentiment analysis failed: {e}, defaulting to neutral")
            return Sentiment.NEUTRAL
    
    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def learn_dimensions(
        self, 
        reviews_text: List[str],
        product_title: str = "",
        bullet_points: str = ""
    ) -> List[dict]:
        """
        è®© AI ä»äº§å“ä¿¡æ¯å’Œè¯„è®ºæ ·æœ¬ä¸­å­¦ä¹ å¹¶æ€»ç»“äº§å“è¯„ä»·ç»´åº¦ã€‚
        
        Args:
            reviews_text: è¯„è®ºæ–‡æœ¬åˆ—è¡¨ï¼ˆå»ºè®®30-50æ¡ï¼‰
            product_title: äº§å“æ ‡é¢˜ï¼ˆå¯é€‰ï¼Œç”¨äºæä¾›äº§å“ä¸Šä¸‹æ–‡ï¼‰
            bullet_points: äº§å“äº”ç‚¹æè¿°ï¼ˆå¯é€‰ï¼Œç”¨äºè¡¥å……äº§å“å–ç‚¹ï¼‰
            
        Returns:
            ç»´åº¦åˆ—è¡¨ï¼Œæ¯ä¸ªç»´åº¦åŒ…å« name å’Œ description
            
        Example:
            [
                {"name": "ç”µæ± ç»­èˆª", "description": "ä¸å……ç”µé€Ÿåº¦å’Œä½¿ç”¨æ—¶é•¿ç›¸å…³çš„é—®é¢˜"},
                {"name": "å¤–è§‚è®¾è®¡", "description": "äº§å“çš„å¤–è§‚ã€é¢œè‰²ã€æè´¨ç­‰è§†è§‰ç›¸å…³è¯„ä»·"}
            ]
        """
        if not self._check_client():
            logger.error("Translation service not configured for dimension learning")
            return []
        
        if not reviews_text or len(reviews_text) < 5:
            logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦5æ¡è¯„è®ºï¼‰ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ ç»´åº¦")
            return []
        
        # é™åˆ¶æ ·æœ¬é‡é˜²æ­¢è¶… token
        sample_texts = reviews_text[:50]
        combined_text = "\n---\n".join(sample_texts)
        
        # å¤„ç†äº§å“ä¿¡æ¯
        title_text = product_title.strip() if product_title else "ï¼ˆæœªæä¾›ï¼‰"
        bullet_text = bullet_points.strip() if bullet_points else "ï¼ˆæœªæä¾›ï¼‰"
        
        try:
            prompt = DIMENSION_DISCOVERY_PROMPT.format(
                product_title=title_text,
                bullet_points=bullet_text,
                count=len(sample_texts),
                reviews_text=combined_text
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
                max_tokens=2000,
                timeout=90.0,
            )
            
            result = response.choices[0].message.content.strip()
            
            # ä½¿ç”¨å¥å£®çš„ JSON è§£æå™¨
            parsed = parse_json_safely(result)
            
            if not isinstance(parsed, dict) or "dimensions" not in parsed:
                logger.warning(f"ç»´åº¦å‘ç°è¿”å›æ ¼å¼ä¸æ­£ç¡®: {type(parsed)}")
                return []
            
            dimensions = parsed.get("dimensions", [])
            
            # éªŒè¯ç»´åº¦æ ¼å¼
            valid_dimensions = []
            for dim in dimensions:
                if isinstance(dim, dict) and dim.get("name"):
                    valid_dimensions.append({
                        "name": dim["name"].strip(),
                        "description": (dim.get("description") or "").strip()
                    })
            
            logger.info(f"AI æˆåŠŸå­¦ä¹ åˆ° {len(valid_dimensions)} ä¸ªäº§å“ç»´åº¦")
            return valid_dimensions
            
        except Exception as e:
            logger.error(f"ç»´åº¦å­¦ä¹ å¤±è´¥: {e}")
            return []

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def learn_dimensions_from_raw(
        self, 
        raw_reviews: List[str],
        product_title: str = "",
        bullet_points: str = ""
    ) -> List[dict]:
        """
        è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ï¼šä»è‹±æ–‡åŸæ–‡è¯„è®ºç›´æ¥å­¦ä¹ äº§å“ç»´åº¦ï¼ˆè¾“å‡ºä¸­æ–‡ï¼‰ã€‚
        
        è¿™æ˜¯æµå¼å¤„ç†æ¶æ„çš„æ ¸å¿ƒæ–¹æ³•ï¼š
        - ä¸éœ€è¦ç­‰å¾…ç¿»è¯‘å®Œæˆ
        - ç›´æ¥ä½¿ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œå­¦ä¹ 
        - AI è¾“å‡ºä¸­æ–‡ç»´åº¦åç§°å’Œæè¿°
        
        Args:
            raw_reviews: è‹±æ–‡åŸæ–‡è¯„è®ºåˆ—è¡¨ï¼ˆæ¥è‡ª get_scientific_samplesï¼‰
            product_title: äº§å“è‹±æ–‡æ ‡é¢˜
            bullet_points: äº§å“è‹±æ–‡äº”ç‚¹æè¿°
            
        Returns:
            ç»´åº¦åˆ—è¡¨ï¼ˆä¸­æ–‡ï¼‰ï¼Œæ¯ä¸ªç»´åº¦åŒ…å« name å’Œ description
        """
        if not self._check_client():
            logger.error("Translation service not configured for raw dimension learning")
            return []
        
        if not raw_reviews or len(raw_reviews) < 5:
            logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦5æ¡è‹±æ–‡è¯„è®ºï¼‰ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹ ç»´åº¦")
            return []
        
        # é™åˆ¶æ ·æœ¬é‡é˜²æ­¢è¶… token
        sample_texts = raw_reviews[:50]
        combined_text = "\n---\n".join([f"Review {i+1}: {text}" for i, text in enumerate(sample_texts)])
        
        # å¤„ç†äº§å“ä¿¡æ¯
        title_text = product_title.strip() if product_title else "(Not provided)"
        bullet_text = bullet_points.strip() if bullet_points else "(Not provided)"
        
        try:
            prompt = DIMENSION_DISCOVERY_RAW_PROMPT.format(
                product_title=title_text,
                bullet_points=bullet_text,
                count=len(sample_texts),
                reviews_text=combined_text
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000,
                timeout=90.0,
            )
            
            result = response.choices[0].message.content.strip()
            parsed = parse_json_safely(result)
            
            if not isinstance(parsed, dict) or "dimensions" not in parsed:
                logger.warning(f"è·¨è¯­è¨€ç»´åº¦å‘ç°è¿”å›æ ¼å¼ä¸æ­£ç¡®: {type(parsed)}")
                return []
            
            dimensions = parsed.get("dimensions", [])
            
            valid_dimensions = []
            for dim in dimensions:
                if isinstance(dim, dict) and dim.get("name"):
                    valid_dimensions.append({
                        "name": dim["name"].strip(),
                        "description": (dim.get("description") or "").strip()
                    })
            
            logger.info(f"[è·¨è¯­è¨€å­¦ä¹ ] ä» {len(sample_texts)} æ¡è‹±æ–‡è¯„è®ºå­¦ä¹ åˆ° {len(valid_dimensions)} ä¸ªä¸­æ–‡ç»´åº¦")
            return valid_dimensions
            
        except Exception as e:
            logger.error(f"è·¨è¯­è¨€ç»´åº¦å­¦ä¹ å¤±è´¥: {e}")
            return []

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def learn_context_labels_from_raw(
        self, 
        raw_reviews: List[str],
        product_title: str = "",
        bullet_points: List[str] = None
    ) -> dict:
        """
        è·¨è¯­è¨€é›¶æ ·æœ¬å­¦ä¹ ï¼šä»è‹±æ–‡åŸæ–‡è¯„è®ºç›´æ¥å­¦ä¹  5W æ ‡ç­¾åº“ï¼ˆè¾“å‡ºä¸­æ–‡ï¼‰ã€‚
        
        è¿™æ˜¯æµå¼å¤„ç†æ¶æ„çš„æ ¸å¿ƒæ–¹æ³•ï¼š
        - ä¸éœ€è¦ç­‰å¾…ç¿»è¯‘å®Œæˆ
        - ç›´æ¥ä½¿ç”¨è‹±æ–‡åŸæ–‡è¿›è¡Œå­¦ä¹ 
        - AI è¾“å‡ºä¸­æ–‡æ ‡ç­¾åç§°å’Œæè¿°
        
        Args:
            raw_reviews: è‹±æ–‡åŸæ–‡è¯„è®ºåˆ—è¡¨ï¼ˆæ¥è‡ª get_scientific_samplesï¼‰
            product_title: äº§å“è‹±æ–‡æ ‡é¢˜
            bullet_points: äº§å“è‹±æ–‡äº”ç‚¹æè¿°åˆ—è¡¨
            
        Returns:
            5W æ ‡ç­¾å­—å…¸ï¼ˆä¸­æ–‡ï¼‰ï¼Œæ ¼å¼ï¼š
            {
                "who": [{"name": "è€å¹´äºº", "description": "..."}, ...],
                "where": [...],
                "when": [...],
                "why": [...],
                "what": [...]
            }
        """
        if not self._check_client():
            logger.error("Translation service not configured for raw context learning")
            return {}
        
        if not raw_reviews or len(raw_reviews) < 10:
            logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦10æ¡è‹±æ–‡è¯„è®ºï¼‰ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹  5W æ ‡ç­¾")
            return {}
        
        # é™åˆ¶æ ·æœ¬é‡é˜²æ­¢è¶… token
        sample_texts = raw_reviews[:50]
        combined_reviews = "\n---\n".join([f"Review {i+1}: {text}" for i, text in enumerate(sample_texts)])
        
        # æ ¼å¼åŒ–äº§å“å®˜æ–¹ä¿¡æ¯
        formatted_title = product_title.strip() if product_title else "(Not provided)"
        formatted_bullets = "(Not provided)"
        if bullet_points and len(bullet_points) > 0:
            formatted_bullets = "\n".join([f"  - {bp}" for bp in bullet_points if bp and bp.strip()])
        
        logger.info(f"[è·¨è¯­è¨€å­¦ä¹ ] 5Wæ ‡ç­¾å­¦ä¹ ï¼š{len(sample_texts)} æ¡è‹±æ–‡è¯„è®º + äº§å“ä¿¡æ¯")
        
        try:
            prompt = CONTEXT_DISCOVERY_RAW_PROMPT.format(
                product_title=formatted_title,
                bullet_points=formatted_bullets,
                count=len(sample_texts),
                reviews_text=combined_reviews
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=3000,
                timeout=120.0,
            )
            
            result = response.choices[0].message.content.strip()
            parsed = parse_json_safely(result)
            
            if not isinstance(parsed, dict):
                logger.warning(f"è·¨è¯­è¨€ 5W æ ‡ç­¾å‘ç°è¿”å›æ ¼å¼ä¸æ­£ç¡®: {type(parsed)}")
                return {}
            
            valid_types = {"who", "where", "when", "why", "what"}
            valid_result = {}
            
            for context_type in valid_types:
                labels = parsed.get(context_type, [])
                valid_labels = []
                
                for label in labels:
                    if isinstance(label, dict) and label.get("name"):
                        valid_labels.append({
                            "name": label["name"].strip(),
                            "description": (label.get("description") or "").strip()
                        })
                
                if valid_labels:
                    valid_result[context_type] = valid_labels
            
            total_labels = sum(len(v) for v in valid_result.values())
            logger.info(f"[è·¨è¯­è¨€å­¦ä¹ ] ä»è‹±æ–‡è¯„è®ºå­¦ä¹ åˆ° {total_labels} ä¸ªä¸­æ–‡ 5W æ ‡ç­¾ï¼ˆ{len(valid_result)} ä¸ªç±»å‹ï¼‰")
            return valid_result
            
        except Exception as e:
            logger.error(f"è·¨è¯­è¨€ 5W æ ‡ç­¾å­¦ä¹ å¤±è´¥: {e}")
            return {}

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def learn_context_labels(
        self, 
        reviews_text: List[str],
        product_title: str = "",
        bullet_points: List[str] = None
    ) -> dict:
        """
        è®© AI ç»“åˆäº§å“å®˜æ–¹ä¿¡æ¯å’Œè¯„è®ºæ ·æœ¬å­¦ä¹  5W æ ‡å‡†æ ‡ç­¾åº“ï¼ˆDefinition é˜¶æ®µï¼‰ã€‚
        
        è¿™æ˜¯ AI-Native æ¶æ„çš„æ ¸å¿ƒï¼š"å…ˆå­¦ä¹ æ ‡å‡†ï¼Œåå¼ºåˆ¶å½’ç±»"ã€‚
        AI ä¼šåˆ†æäº§å“æ ‡é¢˜ã€äº”ç‚¹å–ç‚¹å’Œè¯„è®ºæ ·æœ¬ï¼Œä¸ºæ¯ä¸ª 5W ç±»å‹ç”Ÿæˆæ ‡å‡†æ ‡ç­¾ã€‚
        
        **[UPDATED] åŠ å…¥äº§å“å®˜æ–¹ä¿¡æ¯ï¼š**
        - æ ‡é¢˜å’Œäº”ç‚¹æ˜¯å•†å®¶çš„"å–å®¶ç§€"ï¼Œå¾€å¾€æ¯”ç”¨æˆ·è¯„è®ºæ›´ç²¾å‡†
        - ç‰¹åˆ«å¯¹ Whoï¼ˆäººç¾¤ï¼‰ã€Whereï¼ˆåœºæ™¯ï¼‰ã€Whatï¼ˆä»»åŠ¡ï¼‰æå‡æ˜¾è‘—
        
        Args:
            reviews_text: è¯„è®ºæ–‡æœ¬åˆ—è¡¨ï¼ˆå»ºè®®30-50æ¡ï¼Œæ··åˆå¥½è¯„å·®è¯„ï¼‰
            product_title: äº§å“æ ‡é¢˜ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
            bullet_points: äº§å“äº”ç‚¹å–ç‚¹åˆ—è¡¨ï¼ˆè‹±æ–‡åŸæ–‡ï¼‰
            
        Returns:
            5W æ ‡ç­¾å­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                "who": [{"name": "è€å¹´äºº", "description": "..."}, ...],
                "where": [...],
                "when": [...],
                "why": [...],
                "what": [...]
            }
            
        Example:
            >>> labels = service.learn_context_labels(
            ...     reviews[:50],
            ...     product_title="LED Light for Seniors",
            ...     bullet_points=["Perfect for elderly", "Home Office use"]
            ... )
        """
        if not self._check_client():
            logger.error("Translation service not configured for context learning")
            return {}
        
        if not reviews_text or len(reviews_text) < 30:
            logger.warning("æ ·æœ¬æ•°é‡ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦30æ¡è¯„è®ºï¼‰ï¼Œæ— æ³•æœ‰æ•ˆå­¦ä¹  5W æ ‡ç­¾")
            return {}
        
        # é™åˆ¶æ ·æœ¬é‡é˜²æ­¢è¶… tokenï¼ˆ50æ¡è¯„è®ºçº¦ 4000-6000 tokensï¼‰
        sample_texts = reviews_text[:50]
        combined_reviews = "\n---\n".join([f"è¯„è®º{i+1}: {text}" for i, text in enumerate(sample_texts)])
        
        # [NEW] æ ¼å¼åŒ–äº§å“å®˜æ–¹ä¿¡æ¯
        formatted_title = product_title.strip() if product_title else "ï¼ˆæ— ï¼‰"
        formatted_bullets = "ï¼ˆæ— ï¼‰"
        if bullet_points and len(bullet_points) > 0:
            formatted_bullets = "\n".join([f"  - {bp}" for bp in bullet_points if bp and bp.strip()])
        
        logger.info(f"5W æ ‡ç­¾å­¦ä¹ ï¼š{len(sample_texts)} æ¡è¯„è®º + äº§å“ä¿¡æ¯ï¼ˆæ ‡é¢˜: {len(formatted_title)}å­—, äº”ç‚¹: {len(bullet_points or [])}æ¡ï¼‰")
        
        try:
            prompt = CONTEXT_DISCOVERY_PROMPT.format(
                product_title=formatted_title,
                bullet_points=formatted_bullets,
                count=len(sample_texts),
                reviews_text=combined_reviews
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # è¾ƒä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
                max_tokens=3000,
                timeout=120.0,  # ç¨é•¿çš„è¶…æ—¶æ—¶é—´
            )
            
            result = response.choices[0].message.content.strip()
            
            # ä½¿ç”¨å¥å£®çš„ JSON è§£æå™¨
            parsed = parse_json_safely(result)
            
            if not isinstance(parsed, dict):
                logger.warning(f"5W æ ‡ç­¾å‘ç°è¿”å›æ ¼å¼ä¸æ­£ç¡®: {type(parsed)}")
                return {}
            
            # éªŒè¯å’Œæ¸…ç†æ¯ä¸ª 5W ç±»å‹çš„æ ‡ç­¾
            valid_types = {"who", "where", "when", "why", "what"}
            valid_result = {}
            
            for context_type in valid_types:
                labels = parsed.get(context_type, [])
                valid_labels = []
                
                for label in labels:
                    if isinstance(label, dict) and label.get("name"):
                        valid_labels.append({
                            "name": label["name"].strip(),
                            "description": (label.get("description") or "").strip()
                        })
                
                if valid_labels:
                    valid_result[context_type] = valid_labels
                    logger.debug(f"  {context_type}: {len(valid_labels)} ä¸ªæ ‡ç­¾")
            
            total_labels = sum(len(v) for v in valid_result.values())
            logger.info(f"AI æˆåŠŸå­¦ä¹ åˆ° {total_labels} ä¸ª 5W æ ‡ç­¾ï¼ˆ{len(valid_result)} ä¸ªç±»å‹ï¼‰")
            return valid_result
            
        except Exception as e:
            logger.error(f"5W æ ‡ç­¾å­¦ä¹ å¤±è´¥: {e}")
            return {}

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def extract_insights(
        self,
        original_text: str,
        translated_text: str,
        dimension_schema: List[dict] = None
    ) -> List[dict]:
        """
        Extract insights from a review.
        
        Args:
            original_text: åŸå§‹è¯„è®ºæ–‡æœ¬
            translated_text: ç¿»è¯‘åçš„æ–‡æœ¬
            dimension_schema: å¯é€‰çš„ç»´åº¦æ¨¡å¼åˆ—è¡¨ï¼Œç”¨äºé™å®š AI åªä½¿ç”¨è¿™äº›ç»´åº¦è¿›è¡Œå½’ç±»
                             æ ¼å¼: [{"name": "ç»´åº¦å", "description": "ç»´åº¦å®šä¹‰"}, ...]
        
        Returns:
            æ´å¯Ÿåˆ—è¡¨ï¼Œæ¯ä¸ªæ´å¯ŸåŒ…å« type, dimension, quote, analysis ç­‰å­—æ®µ
        """
        if not self._check_client():
            return []
        
        # [ä¼˜åŒ–] ç§»é™¤é•¿åº¦é™åˆ¶ - ç¡®ä¿æ¯æ¡è¯„è®ºéƒ½èƒ½æå–æ´å¯Ÿ
        # å³ä½¿æ˜¯çŸ­è¯„è®ºä¹Ÿå¯èƒ½åŒ…å«é‡è¦ä¿¡æ¯
        if not original_text or not original_text.strip():
            return []
        
        try:
            # æ ¹æ®æ˜¯å¦æœ‰ç»´åº¦æ¨¡å¼é€‰æ‹©ä¸åŒçš„ Prompt
            if dimension_schema and len(dimension_schema) > 0:
                # ä½¿ç”¨åŠ¨æ€ç»´åº¦ Prompt - å¼ºåˆ¶ AI æŒ‰æŒ‡å®šç»´åº¦å½’ç±»
                schema_str = "\n".join([
                    f"- {d['name']}: {d.get('description', 'æ— å…·ä½“å®šä¹‰')}" 
                    for d in dimension_schema
                ])
                prompt = INSIGHT_EXTRACTION_PROMPT_DYNAMIC.format(
                    original_text=original_text,
                    translated_text=translated_text or original_text,
                    schema_str=schema_str
                )
                logger.debug(f"ä½¿ç”¨åŠ¨æ€ç»´åº¦ Promptï¼Œå…± {len(dimension_schema)} ä¸ªç»´åº¦")
            else:
                # ä½¿ç”¨åŸæœ‰ Prompt - å…¼å®¹æ—§é€»è¾‘
                prompt = INSIGHT_EXTRACTION_PROMPT.format(
                    original_text=original_text,
                    translated_text=translated_text or original_text
                )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2, # Lower temperature for structural extraction
                max_tokens=1500,
                timeout=60.0,
            )
            
            result = response.choices[0].message.content.strip()
            
            # [UPDATED] Use robust JSON parser
            insights = parse_json_safely(result)
            
            if not isinstance(insights, list):
                logger.warning(f"Parsed insights is not a list: {type(insights)}")
                return []
            
            # Validate insights
            valid_insights = []
            valid_types = {"strength", "weakness", "suggestion", "scenario", "emotion"}
            
            for insight in insights:
                if not isinstance(insight, dict):
                    continue
                if insight.get("type") not in valid_types:
                    continue
                if not insight.get("quote") or not insight.get("analysis"):
                    continue
                
                valid_insights.append({
                    "type": insight["type"],
                    "quote": insight["quote"],
                    "quote_translated": insight.get("quote_translated"),
                    "analysis": insight["analysis"],
                    "dimension": insight.get("dimension")
                })
            
            logger.debug(f"Extracted {len(valid_insights)} insights from review")
            return valid_insights
            
        except Exception as e:
            logger.warning(f"Insight extraction failed: {e}")
            return []
    
    def translate_review(
        self,
        title: Optional[str],
        body: str,
        extract_insights: bool = True
    ) -> Tuple[Optional[str], str, Sentiment, List[dict]]:
        """
        Translate a complete review (title and body), analyze sentiment, and extract insights.
        """
        # Translate title if present
        translated_title = None
        if title and title.strip():
            try:
                translated_title = self.translate_text(title)
            except Exception as e:
                logger.error(f"Failed to translate title: {e}")
                translated_title = None
        
        # Translate body (required)
        try:
            translated_body = self.translate_text(body)
        except Exception as e:
            logger.error(f"Failed to translate body: {e}")
            translated_body = ""
        
        # Analyze sentiment from original text (more accurate)
        sentiment = self.analyze_sentiment(body)
        
        # Extract insights
        insights = []
        if extract_insights and translated_body:
            try:
                insights = self.extract_insights(body, translated_body)
            except Exception as e:
                logger.warning(f"Failed to extract insights: {e}")
                insights = []
        
        return translated_title, translated_body, sentiment, insights
    
    def process_review_parallel(
        self, 
        title: Optional[str], 
        body: str,
        dimension_schema: List[dict] = None,  # [NEW] æ¥æ”¶ä¸“å±ç»´åº¦ï¼ˆç”¨äºæ´å¯Ÿæå–ï¼‰
        context_schema: dict = None           # [NEW] æ¥æ”¶ä¸“å±5Wæ ‡ç­¾ï¼ˆç”¨äºä¸»é¢˜æå–ï¼‰
    ) -> Optional[dict]:
        """
        [High Performance] Execute distinct prompts in parallel to maintain quality while boosting speed.
        
        This method orchestrates parallel execution of translation and analysis tasks:
        - Phase 1: Translate Title, Translate Body, Analyze Sentiment (Parallel - no dependencies)
        - Phase 2: Extract Insights, Extract Themes (Parallel - dependent on Phase 1 translation results)
        
        [UPDATED] Now supports dynamic schemas for personalized analysis:
        - dimension_schema: Product-specific dimensions for insight categorization
        - context_schema: Product-specific 5W labels for theme categorization
        
        Expected speedup: ~50% (from ~6s to ~3.5s per review) while maintaining 100% quality.
        
        Args:
            title: Review title (optional)
            body: Review body (required)
            dimension_schema: Optional list of dimension dicts for insight extraction
                             [{"name": "ç»´åº¦å", "description": "ç»´åº¦å®šä¹‰"}, ...]
            context_schema: Optional 5W label dict for theme extraction
                           {"who": [{"name": "...", "description": "..."}], ...}
            
        Returns:
            Dict with all analysis results, or None if processing fails
            
        Example:
            {
                "title_original": "Great product",
                "body_original": "Love it!",
                "title_translated": "å¾ˆæ£’çš„äº§å“",
                "body_translated": "å¤ªå–œæ¬¢äº†ï¼",
                "sentiment": "positive",
                "insights": [...],
                "themes": {...}
            }
        """
        if not self._check_client() or not body:
            return None

        result = {
            "title_original": title or None,
            "body_original": body,
            "title_translated": None,
            "body_translated": None,
            "sentiment": Sentiment.NEUTRAL.value,
            "insights": [],
            "themes": {}
        }

        # Create thread pool (max_workers=5 balances concurrency with API rate limits)
        with ThreadPoolExecutor(max_workers=5) as executor:
            # --- Phase 1: åŸºç¡€ä»»åŠ¡ (æ— ä¾èµ–ï¼Œå¯ä»¥å¹¶è¡Œ) ---
            future_title = executor.submit(self.translate_text, title) if title and title.strip() else None
            future_body = executor.submit(self.translate_text, body)
            future_sentiment = executor.submit(self.analyze_sentiment, body)

            # Wait for Phase 1 results (blocks until all complete)
            try:
                if future_title:
                    result["title_translated"] = future_title.result()
                
                # Critical: must get body translation before Phase 2 analysis
                result["body_translated"] = future_body.result()
                
                result["sentiment"] = future_sentiment.result().value
                
                logger.debug(f"Phase 1 completed: translation and sentiment analysis done")
            except Exception as e:
                logger.error(f"Phase 1 (translation) failed: {e}")
                # If body translation fails, cannot proceed to Phase 2
                if not result["body_translated"]:
                    logger.warning("Body translation failed, skipping Phase 2 analysis")
                    return result

            # --- Phase 2: é«˜çº§åˆ†æä»»åŠ¡ (ä¾èµ–ç¿»è¯‘ç»“æœï¼Œå¹¶è¡Œæ‰§è¡Œ) ---
            # Now we have both original_text and translated_text
            # We can launch insight extraction and theme extraction in parallel
            
            # [FIXED] å°†ä¸“å±ç»´åº¦ (dimension_schema) é€ä¼ ç»™æå–æ–¹æ³•
            future_insights = executor.submit(
                self.extract_insights, 
                result["body_original"], 
                result["body_translated"],
                dimension_schema  # <--- æ³¨å…¥ç»´åº¦è¡¨
            )
            
            # [FIXED] å°†ä¸“å±æ ‡ç­¾ (context_schema) é€ä¼ ç»™æå–æ–¹æ³•
            future_themes = executor.submit(
                self.extract_themes, 
                result["body_original"], 
                result["body_translated"],
                context_schema  # <--- æ³¨å…¥5Wæ ‡ç­¾åº“
            )

            # Wait for Phase 2 results (both can fail independently)
            try:
                result["insights"] = future_insights.result() or []
                logger.debug(f"Extracted {len(result['insights'])} insights")
            except Exception as e:
                logger.warning(f"Insight extraction failed: {e}")
                result["insights"] = []

            try:
                result["themes"] = future_themes.result() or {}
                logger.debug(f"Extracted {len(result['themes'])} theme categories")
            except Exception as e:
                logger.warning(f"Theme extraction failed: {e}")
                result["themes"] = {}

        logger.info(
            f"Parallel processing completed: "
            f"translation={bool(result['body_translated'])}, "
            f"sentiment={result['sentiment']}, "
            f"insights={len(result['insights'])}, "
            f"themes={len(result['themes'])}"
        )
        return result
    
    def batch_translate(
        self,
        reviews: list[dict]
    ) -> list[dict]:
        """
        Translate a batch of reviews.
        """
        results = []
        
        for review in reviews:
            title = review.get("title") or review.get("title_original")
            body = review.get("body") or review.get("body_original", "")
            
            try:
                translated_title, translated_body, sentiment, insights = self.translate_review(
                    title=title,
                    body=body,
                    extract_insights=True 
                )
                results.append({
                    "title_translated": translated_title,
                    "body_translated": translated_body,
                    "sentiment": sentiment.value,
                    "insights": insights,
                    "success": True
                })
            except Exception as e:
                logger.error(f"Batch translation failed for review: {e}")
                results.append({
                    "title_translated": None,
                    "body_translated": None,
                    "sentiment": Sentiment.NEUTRAL.value,
                    "insights": [],
                    "success": False,
                    "error": str(e)
                })
        
        return results
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def translate_bullet_points(self, bullet_points: List[str]) -> List[str]:
        """
        Translate product bullet points from English to Chinese.
        """
        if not self._check_client():
            raise RuntimeError("Translation service not configured")
        
        if not bullet_points or len(bullet_points) == 0:
            return []
        
        # Combine bullet points for batch translation
        combined_text = "\n".join(bullet_points)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": BULLET_POINTS_SYSTEM_PROMPT},
                    {"role": "user", "content": combined_text}
                ],
                temperature=0.3,
                max_tokens=3000,
                timeout=60.0,
            )
            
            translated_text = response.choices[0].message.content.strip()
            
            # Split back into individual bullet points
            translated_points = [p.strip() for p in translated_text.split("\n") if p.strip()]
            
            # Ensure we have the same number of translations
            if len(translated_points) != len(bullet_points):
                logger.warning(
                    f"Bullet point count mismatch: original {len(bullet_points)}, "
                    f"translated {len(translated_points)}"
                )
                # Pad with empty strings or truncate
                while len(translated_points) < len(bullet_points):
                    translated_points.append("")
                translated_points = translated_points[:len(bullet_points)]
            
            logger.info(f"Translated {len(bullet_points)} bullet points")
            return translated_points
            
        except Exception as e:
            logger.error(f"Bullet points translation failed: {e}")
            raise
    
    def translate_product_title(self, title: str) -> str:
        """
        Translate product title from English to Chinese.
        """
        if not title or not title.strip():
            return ""
        
        try:
            return self.translate_text(title)
        except Exception as e:
            logger.error(f"Product title translation failed: {e}")
            raise

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((Exception,)),
        reraise=True
    )
    def extract_themes(
        self, 
        original_text: str, 
        translated_text: str,
        context_schema: dict = None
    ) -> dict:
        """
        Extract 5W theme content from a review.
        
        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        1. å¼€æ”¾æå–æ¨¡å¼ï¼ˆæ—  context_schemaï¼‰ï¼šAI è‡ªç”±æå– 5W è¦ç´ 
        2. å¼ºåˆ¶å½’ç±»æ¨¡å¼ï¼ˆæœ‰ context_schemaï¼‰ï¼šAI åªèƒ½è¾“å‡ºæ ‡ç­¾åº“ä¸­å·²æœ‰çš„æ ‡ç­¾
        
        Args:
            original_text: è¯„è®ºåŸæ–‡
            translated_text: è¯„è®ºç¿»è¯‘
            context_schema: å¯é€‰çš„ 5W æ ‡ç­¾åº“ï¼Œæ ¼å¼ï¼š
                {
                    "who": [{"name": "è€å¹´äºº", "description": "..."}, ...],
                    "where": [...],
                    ...
                }
                
        Returns:
            æå–çš„ä¸»é¢˜å†…å®¹ï¼Œæ ¼å¼ï¼š
            - å¼€æ”¾æ¨¡å¼ï¼š{"who": [{"content": "...", ...}], ...}
            - å½’ç±»æ¨¡å¼ï¼š{"who": ["è€å¹´äºº", "å® ç‰©ä¸»"], "where": [], ...}
        """
        if not self._check_client():
            return {}
        
        # Skip very short reviews
        if not translated_text or len(translated_text.strip()) < 10:
            return {}
        
        # [UPDATED] Valid theme types for 5W model
        valid_themes = {"who", "where", "when", "why", "what"}
        
        try:
            # æ ¹æ®æ˜¯å¦æœ‰æ ‡ç­¾åº“é€‰æ‹©ä¸åŒçš„ Prompt
            if context_schema and any(context_schema.get(t) for t in valid_themes):
                # å¼ºåˆ¶å½’ç±»æ¨¡å¼ - ä½¿ç”¨æ ‡ç­¾åº“
                schema_lines = []
                for theme_type in valid_themes:
                    labels = context_schema.get(theme_type, [])
                    if labels:
                        label_names = [l["name"] for l in labels if isinstance(l, dict) and l.get("name")]
                        if label_names:
                            schema_lines.append(f"- **{theme_type}**: {', '.join(label_names)}")
                
                schema_str = "\n".join(schema_lines) if schema_lines else "ï¼ˆæ— æ ‡ç­¾åº“ï¼‰"
                
                prompt = THEME_EXTRACTION_PROMPT_WITH_SCHEMA.format(
                    original_text=original_text or "",
                    translated_text=translated_text,
                    schema_str=schema_str
                )
                logger.debug(f"ä½¿ç”¨å¼ºåˆ¶å½’ç±»æ¨¡å¼ï¼Œæ ‡ç­¾åº“åŒ…å« {len(schema_lines)} ä¸ªç±»å‹")
            else:
                # å¼€æ”¾æå–æ¨¡å¼ - è‡ªç”±æå–
                prompt = THEME_EXTRACTION_PROMPT.format(
                    original_text=original_text or "",
                    translated_text=translated_text
                )
                logger.debug("ä½¿ç”¨å¼€æ”¾æå–æ¨¡å¼")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=2000,
                timeout=60.0,
            )
            
            result = response.choices[0].message.content.strip()
            
            # [UPDATED] Use robust JSON parser
            themes = parse_json_safely(result)
            
            if not isinstance(themes, dict):
                logger.warning(f"Parsed themes is not a dict: {type(themes)}")
                return {}
            
            # æ ¹æ®æ¨¡å¼å¤„ç†è¿”å›ç»“æœ
            valid_result = {}
            
            if context_schema and any(context_schema.get(t) for t in valid_themes):
                # [UPDATED] å¼ºåˆ¶å½’ç±»æ¨¡å¼ - æ”¯æŒå¸¦è¯æ®çš„å¯è§£é‡Šå½’ç±»
                # æ–°æ ¼å¼: {"tag": "è€å¹´äºº", "quote": "...", "explanation": "..."}
                for theme_type in valid_themes:
                    items = themes.get(theme_type, [])
                    if not isinstance(items, list):
                        continue
                    
                    # è·å–è¯¥ç±»å‹å…è®¸çš„æ ‡ç­¾
                    allowed_labels = {
                        l["name"] for l in context_schema.get(theme_type, []) 
                        if isinstance(l, dict) and l.get("name")
                    }
                    
                    valid_items = []
                    for item in items:
                        if isinstance(item, dict):
                            # æ–°æ ¼å¼: å¸¦ tag/quote/quote_translated/explanation çš„å¯¹è±¡
                            tag = item.get("tag") or item.get("content")
                            if tag and tag.strip() in allowed_labels:
                                valid_items.append({
                                    "content": tag.strip(),  # æ ‡å‡†æ ‡ç­¾å
                                    "content_original": item.get("quote") or item.get("content_original"),  # åŸæ–‡è¯æ®
                                    "quote_translated": item.get("quote_translated"),  # [NEW] ä¸­æ–‡ç¿»è¯‘è¯æ®
                                    "content_translated": item.get("content_translated"),  # ç¿»è¯‘ï¼ˆå¯é€‰ï¼Œå‘åå…¼å®¹ï¼‰
                                    "explanation": item.get("explanation")  # å½’ç±»ç†ç”±
                                })
                        elif isinstance(item, str):
                            # å…¼å®¹æ—§æ ¼å¼: çº¯å­—ç¬¦ä¸²
                            if item.strip() in allowed_labels:
                                valid_items.append({
                                    "content": item.strip(),
                                    "content_original": None,
                                    "content_translated": None,
                                    "explanation": f"å‘½ä¸­æ ‡ç­¾åº“: {item.strip()}"
                                })
                    
                    if valid_items:
                        valid_result[theme_type] = valid_items
                        logger.debug(f"  {theme_type}: {len(valid_items)} ä¸ªæ ‡ç­¾ (å¸¦è¯æ®)")
            else:
                # å¼€æ”¾æå–æ¨¡å¼ - è¿”å›çš„æ˜¯å®Œæ•´å†…å®¹é¡¹
                for theme_type, items in themes.items():
                    if theme_type not in valid_themes:
                        continue
                    if not isinstance(items, list):
                        continue
                    
                    # Validate each item
                    valid_items = []
                    for item in items:
                        if isinstance(item, dict) and "content" in item:
                            # Ensure content is a non-empty string
                            content = item.get("content", "").strip()
                            if content:
                                # Build valid item
                                valid_item = {
                                    "content": content,
                                    "content_original": item.get("content_original") or None,
                                    "content_translated": item.get("content_translated") or None,
                                    "explanation": item.get("explanation") or None
                                }
                                valid_items.append(valid_item)
                        elif isinstance(item, str):
                            # Backward compatibility: if item is a string, convert to new format
                            if item.strip():
                                valid_items.append({
                                    "content": item.strip(),
                                    "content_original": None,
                                    "content_translated": None,
                                    "explanation": None
                                })
                    
                    if valid_items:
                        valid_result[theme_type] = valid_items
            
            logger.debug(f"Extracted themes: {list(valid_result.keys())}")
            return valid_result
            
        except Exception as e:
            logger.warning(f"Theme extraction failed: {e}")
            return {}


# Singleton instance
translation_service = TranslationService()
