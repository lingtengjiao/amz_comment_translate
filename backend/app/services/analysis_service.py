"""
Analysis Service - VOC äº§å“åˆ†ææœåŠ¡

æ”¯æŒä¸¤ç§åˆ†æç±»å‹ï¼š
1. COMPARISON (å¯¹æ¯”åˆ†æ): å¤šäº§å“æ¨ªå‘å¯¹æ¯”ï¼Œçªå‡ºå·®å¼‚å’Œå®šä½
2. MARKET_INSIGHT (ç»†åˆ†å¸‚åœºæ´å¯Ÿ): å¤šäº§å“èšåˆåˆ†æï¼Œè¯†åˆ«å¸‚åœºå…±æ€§ã€è¶‹åŠ¿ã€æœºä¼š

æ¶æ„ä¼˜åŒ–ï¼š
1. ä½¿ç”¨ AsyncOpenAI å¼‚æ­¥å®¢æˆ·ç«¯ï¼Œéé˜»å¡
2. åˆ†æ­¥éª¤å¤„ç†ï¼šæ¯ä¸ªäº§å“å•ç‹¬åˆ†æ -> ç”Ÿæˆç»´åº¦æ´å¯Ÿ -> åˆå¹¶å¯¹æ¯”/èšåˆ
3. ç²¾ç®€æ•°æ®ï¼šä¿ç•™ Top 20 æ ‡ç­¾ï¼ˆç¡®ä¿åˆ†ææ•°æ®å®Œæ•´æ€§ï¼‰
4. å¢å¼ºé‡è¯•ï¼štenacity è‡ªåŠ¨é‡è¯•
"""
import logging
import json
import asyncio
from uuid import UUID
from typing import List, Dict, Any, Optional

from sqlalchemy import select, desc
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from openai import AsyncOpenAI, APIConnectionError, APITimeoutError, RateLimitError

from app.models.analysis import (
    AnalysisProject, 
    AnalysisProjectItem, 
    AnalysisStatus, 
    AnalysisType
)
from app.models.product import Product
from app.models.project_learning import (
    ProjectDimension,
    ProjectContextLabel,
    ProjectDimensionMapping,
    ProjectLabelMapping
)
from app.services.summary_service import SummaryService
from app.core.config import settings

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å¼‚æ­¥ OpenAI å®¢æˆ·ç«¯
_async_client: Optional[AsyncOpenAI] = None

def get_async_client() -> AsyncOpenAI:
    """è·å–æˆ–åˆ›å»ºå¼‚æ­¥ OpenAI å®¢æˆ·ç«¯"""
    global _async_client
    if _async_client is None:
        if not settings.QWEN_API_KEY:
            raise ValueError("QWEN_API_KEY æœªé…ç½®")
        _async_client = AsyncOpenAI(
            api_key=settings.QWEN_API_KEY,
            base_url=settings.QWEN_API_BASE,
            timeout=60.0,  # å•ä¸ªè¯·æ±‚è¶…æ—¶
            max_retries=3   # å†…ç½®é‡è¯•
        )
    return _async_client


# ==============================================================================
# [PROMPT] VOC å¯¹æ¯”åˆ†æ Prompt
# ==============================================================================

SINGLE_PRODUCT_PROMPT = """åˆ†æäº§å“"{product_name}"çš„ç”¨æˆ·åé¦ˆæ•°æ®ï¼Œè¾“å‡ºç»“æ„åŒ–JSONã€‚

è¾“å…¥æ•°æ®ï¼š{stats_json}

é‡è¦è¯´æ˜ï¼š
- **label** å¿…é¡»æ˜¯æ•°æ®ä¸­çš„å…·ä½“æ ‡ç­¾åç§°ï¼ˆå¦‚"å„¿ç«¥"ã€"å®¶é•¿"ã€"ç„¦è™‘æ—¶"ã€"å®¶ä¸­"ã€"é€ç¤¼"ç­‰ï¼‰ï¼Œä¸è¦ç”¨"ç”¨æˆ·ç±»å‹"ã€"ä½¿ç”¨æ—¶æœº"è¿™ç§é€šç”¨è¯
- **desc** æ˜¯åŸºäºæ•°æ®å½’çº³çš„ä¸€å¥è¯æè¿°
- **count** å¿…é¡»ä»è¾“å…¥æ•°æ®çš„ count å­—æ®µè·å–

è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼š
{{
  "product_name": "{product_name}",
  "asin": "{asin}",
  "five_w": {{
    "who": [
      {{"label": "å„¿ç«¥", "desc": "ä¸»è¦ä½¿ç”¨è€…ï¼Œç”¨äºæ„Ÿç»Ÿè®­ç»ƒ", "count": 42}},
      {{"label": "å®¶é•¿", "desc": "é‡è¦è´­ä¹°ç¾¤ä½“", "count": 30}}
    ],
    "when": [
      {{"label": "ç„¦è™‘æ—¶", "desc": "ä½¿ç”¨é¢‘ç‡æœ€é«˜", "count": 18}},
      {{"label": "å­¦ä¹ æ—¶", "desc": "ç”¨äºé›†ä¸­æ³¨æ„åŠ›", "count": 11}}
    ],
    "where": [
      {{"label": "å®¶åº­", "desc": "æœ€ä¸»è¦åœºæ™¯", "count": 37}},
      {{"label": "å­¦æ ¡", "desc": "ç”¨äºè¯¾å ‚ä¸“æ³¨åŠ›è¾…åŠ©", "count": 17}}
    ],
    "why": [
      {{"label": "æ”¹å–„è¡Œä¸º", "desc": "æ”¹å–„å¤šåŠ¨ã€å†²åŠ¨ç­‰é—®é¢˜", "count": 23}},
      {{"label": "ç¼“è§£ç„¦è™‘", "desc": "æ ¸å¿ƒéœ€æ±‚", "count": 15}}
    ],
    "what": [
      {{"label": "è§¦è§‰åˆºæ¿€", "desc": "é€šè¿‡çº¹ç†ä¿ƒè¿›æ„Ÿå®˜å‘å±•", "count": 38}},
      {{"label": "æƒ…ç»ªå®‰æŠš", "desc": "å¸®åŠ©å®‰æŠšæƒ…ç»ªæ³¢åŠ¨", "count": 19}}
    ]
  }},
  "dimensions": {{
    "pros": [
      {{"label": "ææ–™è´¨æ„Ÿ", "desc": "ç¡…èƒ¶æŸ”è½¯å®‰å…¨", "count": 31}},
      {{"label": "åŠŸèƒ½è¡¨ç°", "desc": "æœ‰æ•ˆç¼“è§£ç„¦è™‘", "count": 30}}
    ],
    "cons": [
      {{"label": "ç»“æ„ç‘•ç–µ", "desc": "è¿æ¥å¤„ä¸ç‰¢å›º", "count": 4}},
      {{"label": "ä½©æˆ´ä¸é€‚", "desc": "é•¿æ—¶é—´ä½¿ç”¨æœ‰å‹è¿«æ„Ÿ", "count": 3}}
    ],
    "suggestion": [
      {{"label": "å¢åŠ é¢œè‰²é€‰æ‹©", "desc": "ç”¨æˆ·å¸Œæœ›æœ‰æ›´å¤šé¢œè‰²æ¬¾å¼", "count": 8}},
      {{"label": "æ”¹è¿›åŒ…è£…", "desc": "å»ºè®®ä½¿ç”¨æ›´ç¯ä¿çš„åŒ…è£…", "count": 5}}
    ],
    "scenario": [
      {{"label": "è¯¾å ‚ä½¿ç”¨", "desc": "å­¦ç”Ÿåœ¨è¯¾å ‚ä¸Šä½¿ç”¨è¾…åŠ©ä¸“æ³¨", "count": 12}},
      {{"label": "é•¿é€”æ—…è¡Œ", "desc": "é£æœº/æ±½è½¦ä¸Šæ‰“å‘æ—¶é—´", "count": 7}}
    ],
    "emotion": [
      {{"label": "æƒŠå–œå¥½è¯„", "desc": "è¶…å‡ºé¢„æœŸï¼Œéå¸¸æ»¡æ„", "count": 15}},
      {{"label": "å¤±æœ›åæ§½", "desc": "è´¨é‡ä¸å¦‚é¢„æœŸï¼Œæœ‰è½å·®æ„Ÿ", "count": 6}}
    ]
  }}
}}

è¦æ±‚ï¼š
1. label å¿…é¡»ä»è¾“å…¥æ•°æ®çš„ "label" å­—æ®µä¸­æå–ï¼Œä¸è¦è‡ªå·±ç¼–é€ 
2. count å¿…é¡»ä»è¾“å…¥æ•°æ®çš„ "count" å­—æ®µè·å–ï¼Œä¿æŒåŸå§‹æ•°å€¼
3. dimensions åŒ…å«5ç±»å£ç¢‘æ´å¯Ÿï¼špros(ä¼˜åŠ¿)ã€cons(ç—›ç‚¹)ã€suggestion(ç”¨æˆ·å»ºè®®)ã€scenario(ä½¿ç”¨åœºæ™¯)ã€emotion(æƒ…ç»ªåé¦ˆ)
4. **æ•°æ®è¡¥å…¨ç­–ç•¥**ï¼šå¦‚æœæŸä¸ªç»´åº¦çš„åŸå§‹æ•°æ®ä¸ºç©ºæ•°ç»„ï¼Œè¯·æ ¹æ®ç›¸å…³ç»´åº¦æ¨æ–­å¹¶ç”Ÿæˆåˆç†å†…å®¹ï¼Œå¹¶æ ‡è®° is_inferred: trueï¼š
   - suggestion ä¸ºç©ºæ—¶ â†’ ä» cons/weakness åå‘æ¨æ–­ç”¨æˆ·æœŸæœ›çš„æ”¹è¿›å»ºè®®
   - scenario ä¸ºç©ºæ—¶ â†’ ä» where/when æ¨æ–­å…·ä½“ä½¿ç”¨åœºæ™¯æ•…äº‹
   - emotion ä¸ºç©ºæ—¶ â†’ ä» pros/cons æ¨æ–­ç”¨æˆ·æƒ…ç»ªå€¾å‘
   - æ¨æ–­ç”Ÿæˆçš„æ¡ç›®æ ¼å¼ï¼š{{"label": "xxx", "desc": "xxx", "count": 0, "is_inferred": true}}
5. éæ¨æ–­æ¡ç›®ä¸è¦æ·»åŠ  is_inferred å­—æ®µ
6. åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
7. ç®€ä½“ä¸­æ–‡"""

DIMENSION_INSIGHT_PROMPT = """åŸºäºä»¥ä¸‹äº§å“çš„å¯¹æ¯”æ•°æ®ï¼Œä¸ºæ¯ä¸ªç»´åº¦ç”Ÿæˆæ´å¯Ÿåˆ†æã€‚

äº§å“æ•°é‡ï¼š{product_count}
äº§å“åˆ—è¡¨ï¼š
{product_summaries}

ä¸º10ä¸ªç»´åº¦ç”Ÿæˆæ´å¯Ÿï¼Œæ¯ä¸ªæ´å¯ŸåŒ…å«ï¼š
1. commonalityï¼šæ‰€æœ‰äº§å“çš„å…±æ€§ç‰¹å¾ï¼ˆ1å¥è¯ï¼‰
2. differencesï¼šæ¯ä¸ªäº§å“çš„å·®å¼‚ç‰¹ç‚¹ï¼ˆæ¯ä¸ªäº§å“1å¥è¯ï¼Œæ ‡æ³¨äº§å“åºå·ï¼‰
3. positioningï¼šæ¯ä¸ªäº§å“çš„å®šä½æ´å¯Ÿï¼ˆæ¯ä¸ªäº§å“1å¥è¯ï¼Œæ ‡æ³¨äº§å“åºå·ï¼‰

10ä¸ªç»´åº¦è¯´æ˜ï¼š
- 5Wç”¨æˆ·ç”»åƒï¼šwho(ç”¨æˆ·æ˜¯è°), when(ä½•æ—¶ä½¿ç”¨), where(åœ¨å“ªé‡Œç”¨), why(è´­ä¹°åŠ¨æœº), what(å…·ä½“ç”¨é€”)
- 5ç±»å£ç¢‘æ´å¯Ÿï¼špros(ä¼˜åŠ¿å–ç‚¹), cons(ç—›ç‚¹é—®é¢˜), suggestion(ç”¨æˆ·å»ºè®®), scenario(ä½¿ç”¨åœºæ™¯), emotion(æƒ…ç»ªåé¦ˆ)

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "dimension_insights": {{
    "who": {{
      "name": "ç”¨æˆ·æ˜¯è°",
      "commonality": "äº”æ¬¾äº§å“å‡å®šä½äºå‡å‹è§£å‹èµ›é“...",
      "differences": [
        {{"product": 1, "text": "å…¨å¹´é¾„è¦†ç›–ï¼Œå¤§ä¼—å¸‚åœºé€šç”¨å‹äº§å“"}},
        {{"product": 2, "text": "æ·±è€•ç‰¹æ®Šå„¿ç«¥å¸‚åœº"}}
      ],
      "positioning": [
        {{"product": 1, "text": "å¤§ä¼—å‡å‹å·¥å…·ï¼Œè¿½æ±‚å¸‚åœºè¦†ç›–æœ€å¤§åŒ–"}},
        {{"product": 2, "text": "åŒ»ç–—åº·å¤èµ›é“ï¼Œå»ºç«‹ä¸“ä¸šæŠ¤åŸæ²³"}}
      ]
    }},
    "when": {{ ... }},
    "where": {{ ... }},
    "why": {{ ... }},
    "what": {{ ... }},
    "pros": {{ ... }},
    "cons": {{ ... }},
    "suggestion": {{
      "name": "ç”¨æˆ·å»ºè®®",
      "commonality": "ç”¨æˆ·æ™®éæœŸæœ›äº§å“åœ¨é¢œè‰²ã€å°ºå¯¸æ–¹é¢æä¾›æ›´å¤šé€‰æ‹©...",
      "differences": [...],
      "positioning": [...]
    }},
    "scenario": {{
      "name": "ä½¿ç”¨åœºæ™¯",
      "commonality": "äº§å“åœ¨å®¶åº­å’ŒåŠå…¬åœºæ™¯å‡æœ‰è¾ƒé«˜ä½¿ç”¨é¢‘ç‡...",
      "differences": [...],
      "positioning": [...]
    }},
    "emotion": {{
      "name": "æƒ…ç»ªåé¦ˆ",
      "commonality": "æ•´ä½“ç”¨æˆ·æƒ…ç»ªåæ­£å‘ï¼Œä½†å¯¹è´¨é‡é—®é¢˜ååº”å¼ºçƒˆ...",
      "differences": [...],
      "positioning": [...]
    }}
  }}
}}

è¦æ±‚ï¼š
1. åŸºäºå®é™…æ•°æ®åˆ†æï¼Œä¸è¦ç¼–é€ 
2. å·®å¼‚å’Œå®šä½æ´å¯Ÿçš„äº§å“åºå·ä»1å¼€å§‹
3. æ´å¯Ÿè¦æœ‰å•†ä¸šä»·å€¼ï¼Œå¸®åŠ©ç†è§£ç«äº‰æ ¼å±€
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

STRATEGY_SUMMARY_PROMPT = """åŸºäºä»¥ä¸‹äº§å“å¯¹æ¯”åˆ†æï¼Œç”Ÿæˆç«å“ç­–ç•¥æ€»ç»“ã€‚

äº§å“æ•°é‡ï¼š{product_count}
äº§å“åˆ—è¡¨ï¼š
{product_summaries}

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "market_summary": "æ•´ä½“å¸‚åœºæ¦‚è¿°ï¼ˆ100å­—å†…ï¼‰",
  "strategy_summary": {{
    "market_positioning": {{
      "title": "å¸‚åœºå®šä½ç­–ç•¥",
      "emoji": "ğŸ¯",
      "content": "åˆ†æå„äº§å“çš„å¸‚åœºå®šä½å·®å¼‚å’Œç«äº‰ç­–ç•¥ï¼ˆ150å­—å†…ï¼‰"
    }},
    "scenario_deep_dive": {{
      "title": "åœºæ™¯åŒ–æ·±è€•",
      "emoji": "ğŸ’¼",
      "content": "åˆ†æå„äº§å“åœ¨ä½¿ç”¨åœºæ™¯å’Œæ—¶æœºä¸Šçš„å·®å¼‚åŒ–ç­–ç•¥ï¼ˆ150å­—å†…ï¼‰"
    }},
    "growth_opportunities": {{
      "title": "å¢é•¿æœºä¼šç‚¹",
      "emoji": "âš¡",
      "content": "åŸºäºåˆ†æè¯†åˆ«çš„å¸‚åœºæœºä¼šå’Œå¢é•¿å»ºè®®ï¼ˆ150å­—å†…ï¼‰"
    }}
  }}
}}

è¦æ±‚ï¼š
1. åŸºäº10ç»´åˆ†ææ•°æ®è¿›è¡Œå½’çº³ï¼ˆ5Wç”¨æˆ·ç”»åƒ + 5ç±»å£ç¢‘æ´å¯Ÿï¼‰
2. å†…å®¹è¦æœ‰å•†ä¸šæ´å¯Ÿä»·å€¼
3. ä½¿ç”¨äº§å“åºå·æ ‡æ³¨å…·ä½“å»ºè®®
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""


# ==============================================================================
# [PROMPT] ç»†åˆ†å¸‚åœºæ´å¯Ÿ Prompt
# ==============================================================================

MARKET_AGGREGATION_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±å¸‚åœºåˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºçš„å¤šäº§å“èšåˆæ•°æ®ï¼Œç”Ÿæˆå¸‚åœºæ´å¯Ÿåˆ†æã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}
- å¸‚åœºåç§°ï¼š{market_name}

# èšåˆæ•°æ®
{aggregated_stats}

# å„äº§å“æ•°æ®æ‘˜è¦
{product_summaries}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„å¸‚åœºæ´å¯Ÿåˆ†æï¼š

{{
  "market_overview": {{
    "summary": "å¸‚åœºæ•´ä½“æ¦‚è¿°ï¼ˆ100å­—å†…ï¼‰",
    "market_size_indicator": "å¸‚åœºè§„æ¨¡æŒ‡æ ‡ï¼ˆå¦‚ï¼šé«˜éœ€æ±‚/ä¸­ç­‰éœ€æ±‚/å°ä¼—å¸‚åœºï¼‰",
    "maturity_level": "å¸‚åœºæˆç†Ÿåº¦ï¼ˆæ–°å…´å¸‚åœº/æˆé•¿æœŸ/æˆç†ŸæœŸ/é¥±å’ŒæœŸï¼‰",
    "competition_intensity": "ç«äº‰æ¿€çƒˆç¨‹åº¦ï¼ˆä½/ä¸­/é«˜ï¼‰",
    "data_support": {{
      "cited_statistics": ["å¼•ç”¨çš„æ•°æ®ç»Ÿè®¡1", "å¼•ç”¨çš„æ•°æ®ç»Ÿè®¡2"]
    }}
  }},
  "common_needs": {{
    "description": "å¸‚åœºå…±æ€§éœ€æ±‚æ€»ç»“ï¼ˆ150å­—å†…ï¼‰",
    "top_needs": [
      {{"need": "éœ€æ±‚æè¿°", "frequency": "å‡ºç°é¢‘ç‡", "importance": "high/medium/low", "count": æ•°å­—}},
      ...
    ],
    "confidence": "high/medium/low",
    "data_support": {{
      "cited_statistics": ["å¼•ç”¨çš„5Wæ•°æ®: xxxå‡ºç°Næ¬¡ï¼Œå æ¯”X%", ...]
    }}
  }},
  "common_pain_points": {{
    "description": "å¸‚åœºå…±æ€§ç—›ç‚¹æ€»ç»“ï¼ˆ150å­—å†…ï¼‰",
    "top_pain_points": [
      {{"pain_point": "ç—›ç‚¹æè¿°", "severity": "high/medium/low", "count": æ•°å­—}},
      ...
    ],
    "confidence": "high/medium/low",
    "data_support": {{
      "cited_statistics": ["å¼•ç”¨çš„ç—›ç‚¹æ•°æ®: xxxå‡ºç°Næ¬¡ï¼Œå æ¯”X%", ...]
    }}
  }},
  "market_concentration": {{
    "description": "å¸‚åœºéœ€æ±‚é›†ä¸­åº¦åˆ†æ",
    "concentration_level": "high/medium/low",
    "dominant_dimensions": ["æœ€é›†ä¸­çš„ç»´åº¦1", "æœ€é›†ä¸­çš„ç»´åº¦2"],
    "fragmented_dimensions": ["åˆ†æ•£çš„ç»´åº¦1", "åˆ†æ•£çš„ç»´åº¦2"]
  }}
}}

è¦æ±‚ï¼š
1. æ‰€æœ‰æ´å¯Ÿå¿…é¡»åŸºäºè¾“å…¥æ•°æ®ï¼Œä¸è¦ç¼–é€ 
2. count å­—æ®µå¿…é¡»ä»æ•°æ®ä¸­è·å–çœŸå®æ•°å€¼
3. data_support.cited_statistics å¿…é¡»å¼•ç”¨å…·ä½“çš„æ•°æ®ç»Ÿè®¡ï¼Œæ ¼å¼å¦‚ï¼š"ç”¨æˆ·ç±»å‹-å„¿ç«¥ å‡ºç°80æ¬¡ï¼Œå æ¯”45%"
4. æ¯ä¸ªåˆ†æè§‚ç‚¹å¿…é¡»æœ‰æ•°æ®æ”¯æ’‘
5. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

MARKET_SEGMENT_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºçš„èšåˆæ•°æ®ï¼Œç”Ÿæˆå¸‚åœºç”¨æˆ·ç”»åƒåˆ†æã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# 5Wç”¨æˆ·ç”»åƒèšåˆæ•°æ®
{five_w_stats}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„å¸‚åœºç»†åˆ†ç”»åƒï¼š

{{
  "market_persona": {{
    "primary_buyers": {{
      "description": "ä¸»è¦è´­ä¹°è€…ç¾¤ä½“æè¿°",
      "segments": [
        {{"segment": "ç¾¤ä½“åç§°", "percentage": "å æ¯”", "characteristics": ["ç‰¹å¾1", "ç‰¹å¾2"]}}
      ],
      "confidence": "high/medium/low",
      "data_support": {{
        "cited_statistics": ["buyeræ•°æ®ï¼šxxxå æ¯”Y%ï¼Œå‡ºç°Næ¬¡", ...]
      }}
    }},
    "primary_users": {{
      "description": "ä¸»è¦ä½¿ç”¨è€…ç¾¤ä½“æè¿°",
      "segments": [
        {{"segment": "ç¾¤ä½“åç§°", "percentage": "å æ¯”", "characteristics": ["ç‰¹å¾1", "ç‰¹å¾2"]}}
      ],
      "confidence": "high/medium/low",
      "data_support": {{
        "cited_statistics": ["useræ•°æ®ï¼šxxxå æ¯”Y%ï¼Œå‡ºç°Næ¬¡", ...]
      }}
    }},
    "usage_scenarios": {{
      "description": "å…¸å‹ä½¿ç”¨åœºæ™¯æ€»ç»“",
      "top_scenarios": [
        {{"scenario": "åœºæ™¯æè¿°", "frequency": "é¢‘ç‡", "count": æ•°å­—}}
      ],
      "confidence": "high/medium/low",
      "data_support": {{
        "cited_statistics": ["whereæ•°æ®ï¼šxxxå æ¯”Y%ï¼Œå‡ºç°Næ¬¡", ...]
      }}
    }},
    "purchase_motivations": {{
      "description": "ä¸»è¦è´­ä¹°åŠ¨æœºæ€»ç»“",
      "top_motivations": [
        {{"motivation": "åŠ¨æœºæè¿°", "importance": "high/medium/low", "count": æ•°å­—}}
      ],
      "confidence": "high/medium/low",
      "data_support": {{
        "cited_statistics": ["whyæ•°æ®ï¼šxxxå æ¯”Y%ï¼Œå‡ºç°Næ¬¡", ...]
      }}
    }},
    "jobs_to_be_done": {{
      "description": "ç”¨æˆ·æ ¸å¿ƒä»»åŠ¡/JTBD",
      "primary_jtbd": "æ ¸å¿ƒä»»åŠ¡æè¿°",
      "secondary_jtbd": ["æ¬¡è¦ä»»åŠ¡1", "æ¬¡è¦ä»»åŠ¡2"],
      "data_support": {{
        "cited_statistics": ["whatæ•°æ®ï¼šxxxå æ¯”Y%ï¼Œå‡ºç°Næ¬¡", ...]
      }}
    }}
  }},
  "typical_user_story": "ä¸€æ®µæè¿°å…¸å‹ç”¨æˆ·ç”»åƒçš„æ•…äº‹ï¼ˆ100å­—å†…ï¼‰"
}}

è¦æ±‚ï¼š
1. åŸºäºå®é™…æ•°æ®åˆ†æï¼Œä¸è¦ç¼–é€ 
2. ä½¿ç”¨å…·ä½“æ•°æ®æ”¯æ’‘ç»“è®ºï¼Œæ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
3. å¼•ç”¨æ ¼å¼ï¼š"ç»´åº¦åç§°-æ ‡ç­¾å å‡ºç°Næ¬¡ï¼Œå æ¯”X%"
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

MARKET_OPPORTUNITY_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºçš„åˆ†ææ•°æ®ï¼ŒæŒ–æ˜å¸‚åœºæœºä¼šã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# äº§å“æ•°æ®æ‘˜è¦
{product_summaries}

# å¸‚åœºç—›ç‚¹å’Œå»ºè®®æ•°æ®
{pain_points_data}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„å¸‚åœºæœºä¼šåˆ†æï¼š

{{
  "unmet_needs": {{
    "description": "æœªè¢«æ»¡è¶³çš„éœ€æ±‚åˆ†æ",
    "opportunities": [
      {{
        "need": "éœ€æ±‚æè¿°",
        "gap_analysis": "å½“å‰äº§å“ç¼ºå£åˆ†æ",
        "market_potential": "high/medium/low",
        "recommendation": "å»ºè®®åˆ‡å…¥ç‚¹",
        "evidence_count": æ•°å­—
      }}
    ],
    "confidence": "high/medium/low",
    "data_support": {{
      "cited_statistics": ["ç—›ç‚¹æ•°æ®ï¼šxxxå‡ºç°Næ¬¡", "å»ºè®®æ•°æ®ï¼šxxxå‡ºç°Næ¬¡", ...]
    }}
  }},
  "white_space_opportunities": {{
    "description": "å¸‚åœºç©ºç™½æœºä¼š",
    "opportunities": [
      {{
        "area": "ç©ºç™½é¢†åŸŸ",
        "description": "æœºä¼šæè¿°",
        "target_segment": "ç›®æ ‡äººç¾¤",
        "entry_barrier": "low/medium/high"
      }}
    ],
    "data_support": {{
      "cited_statistics": ["åŸºäºxxxç—›ç‚¹ï¼ˆNæ¬¡ï¼‰æ¨æ–­çš„æœºä¼š", ...]
    }}
  }},
  "differentiation_opportunities": {{
    "description": "å·®å¼‚åŒ–æœºä¼š",
    "opportunities": [
      {{
        "dimension": "å·®å¼‚åŒ–ç»´åº¦",
        "current_leader": "å½“å‰é¢†å…ˆäº§å“ï¼ˆåºå·ï¼‰",
        "opportunity": "å·®å¼‚åŒ–æœºä¼šæè¿°",
        "implementation_difficulty": "low/medium/high"
      }}
    ],
    "data_support": {{
      "cited_statistics": ["äº§å“Nåœ¨xxxç»´åº¦é¢†å…ˆï¼ˆNæ¬¡ï¼‰", ...]
    }}
  }},
  "product_positioning_map": {{
    "description": "äº§å“å®šä½åˆ†æ",
    "positions": [
      {{"product": åºå·, "positioning": "å®šä½æè¿°", "strengths": ["ä¼˜åŠ¿1"], "gaps": ["ä¸è¶³1"]}}
    ],
    "market_leader": åºå·,
    "niche_players": [åºå·],
    "positioning_advice": "å®šä½å»ºè®®ï¼ˆ100å­—å†…ï¼‰"
  }}
}}

è¦æ±‚ï¼š
1. åŸºäºæ•°æ®åˆ†æè¯†åˆ«çœŸå®æœºä¼š
2. æœºä¼šåˆ†æè¦æœ‰å•†ä¸šå¯è¡Œæ€§
3. æ¯ä¸ªæœºä¼šåˆ†æå¿…é¡»åŒ…å« data_support.cited_statisticsï¼Œå¼•ç”¨ç—›ç‚¹æˆ–å»ºè®®æ•°æ®
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

MARKET_TREND_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºçš„å¤šäº§å“æ•°æ®ï¼Œåˆ†æå¸‚åœºè¶‹åŠ¿ã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# äº§å“æ•°æ®æ‘˜è¦ï¼ˆæŒ‰è¯„è®ºæ•°æ’åºï¼‰
{product_summaries}

# éœ€æ±‚å’Œç—›ç‚¹åˆ†å¸ƒ
{needs_distribution}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„å¸‚åœºè¶‹åŠ¿åˆ†æï¼š

{{
  "emerging_needs": {{
    "description": "æ–°å…´éœ€æ±‚åˆ†æ",
    "trends": [
      {{
        "trend": "è¶‹åŠ¿æè¿°",
        "signal_strength": "strong/medium/weak",
        "affected_products": [åºå·],
        "recommendation": "åº”å¯¹å»ºè®®"
      }}
    ],
    "confidence": "high/medium/low",
    "data_support": {{
      "cited_statistics": ["éœ€æ±‚åˆ†å¸ƒï¼šxxxå æ¯”Y%", "ä¼˜åŠ¿æ•°æ®ï¼šxxxå‡ºç°Næ¬¡", ...]
    }}
  }},
  "declining_patterns": {{
    "description": "è¡°é€€è¶‹åŠ¿åˆ†æ",
    "patterns": [
      {{
        "pattern": "è¡°é€€è¶‹åŠ¿æè¿°",
        "risk_level": "high/medium/low",
        "recommendation": "è§„é¿å»ºè®®"
      }}
    ],
    "data_support": {{
      "cited_statistics": ["ç—›ç‚¹åˆ†å¸ƒï¼šxxxå æ¯”Y%", ...]
    }}
  }},
  "market_dynamics": {{
    "growth_drivers": ["å¢é•¿é©±åŠ¨å› ç´ 1", "å¢é•¿é©±åŠ¨å› ç´ 2"],
    "inhibitors": ["æŠ‘åˆ¶å› ç´ 1", "æŠ‘åˆ¶å› ç´ 2"],
    "disruption_risks": ["é¢ è¦†é£é™©1"],
    "data_support": {{
      "cited_statistics": ["åŸºäºxxxæ•°æ®ï¼ˆNæ¬¡ï¼‰æ¨æ–­", ...]
    }}
  }},
  "future_outlook": {{
    "short_term": "çŸ­æœŸå±•æœ›ï¼ˆ3-6ä¸ªæœˆï¼‰",
    "medium_term": "ä¸­æœŸå±•æœ›ï¼ˆ6-12ä¸ªæœˆï¼‰",
    "strategic_recommendation": "æˆ˜ç•¥å»ºè®®ï¼ˆ100å­—å†…ï¼‰",
    "data_support": {{
      "cited_statistics": ["åŸºäºå½“å‰å¸‚åœºæ•°æ®æ¨æ–­", ...]
    }}
  }}
}}

è¦æ±‚ï¼š
1. åŸºäºæ•°æ®åˆ†å¸ƒå’Œäº§å“å¯¹æ¯”æ¨æ–­è¶‹åŠ¿
2. è¶‹åŠ¿åˆ†æè¦æœ‰ä¾æ®ï¼Œæ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
3. å¼•ç”¨æ ¼å¼ï¼š"ç»´åº¦åç§°-æ ‡ç­¾å å‡ºç°Næ¬¡ï¼Œå æ¯”X%"
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""


# ==============================================================================
# [PROMPT] å¸‚åœºæ´å¯Ÿæ‰©å±•æ¨¡å— Promptï¼ˆ8æ¨¡å—å®Œæ•´ç‰ˆï¼‰
# ==============================================================================

STRATEGIC_POSITIONING_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºæ•°æ®ï¼Œç”Ÿæˆæˆ˜ç•¥å®šä½ä¸SWOTåˆ†æã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# äº§å“æ•°æ®æ‘˜è¦
{product_summaries}

# èšåˆæ•°æ®ï¼ˆä¼˜åŠ¿å’Œç—›ç‚¹ï¼‰
{aggregated_pros_cons}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„æˆ˜ç•¥å®šä½åˆ†æï¼š

{{
  "strategic_positioning": {{
    "market_positioning": {{
      "description": "æ•´ä½“å¸‚åœºå®šä½åˆ†æï¼ˆ150å­—å†…ï¼‰",
      "positioning_map": [
        {{
          "product": åºå·,
          "positioning_statement": "ä¸€å¥è¯å®šä½",
          "target_segment": "ç›®æ ‡äººç¾¤",
          "value_proposition": "æ ¸å¿ƒä»·å€¼ä¸»å¼ "
        }}
      ],
      "data_support": {{
        "cited_statistics": ["äº§å“Nçš„ä¼˜åŠ¿xxxå‡ºç°Mæ¬¡", ...]
      }}
    }},
    "swot_matrix": {{
      "strengths": [
        {{"item": "ä¼˜åŠ¿æè¿°", "evidence_count": æ•°å­—, "confidence": "high/medium/low"}}
      ],
      "weaknesses": [
        {{"item": "åŠ£åŠ¿æè¿°", "evidence_count": æ•°å­—, "confidence": "high/medium/low"}}
      ],
      "opportunities": [
        {{"item": "æœºä¼šæè¿°", "source": "æ¥æºåˆ†æ", "confidence": "high/medium/low"}}
      ],
      "threats": [
        {{"item": "å¨èƒæè¿°", "risk_level": "high/medium/low", "confidence": "high/medium/low"}}
      ],
      "data_support": {{
        "cited_statistics": ["ä¼˜åŠ¿æ¥è‡ªï¼šxxxï¼ˆNæ¬¡ï¼‰", "ç—›ç‚¹æ¥è‡ªï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }},
    "competitive_advantage": {{
      "description": "å¸‚åœºç«äº‰ä¼˜åŠ¿åˆ†æ",
      "leader_products": [åºå·],
      "differentiators": ["å·®å¼‚åŒ–å› ç´ 1", "å·®å¼‚åŒ–å› ç´ 2"],
      "data_support": {{
        "cited_statistics": ["åŸºäºä¼˜åŠ¿æ•°æ®åˆ†æ", ...]
      }}
    }}
  }}
}}

è¦æ±‚ï¼š
1. SWOT åˆ†æå¿…é¡»åŸºäºå®é™…æ•°æ®
2. æ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
3. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

USAGE_CONTEXT_ANALYSIS_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºæ•°æ®ï¼Œç”Ÿæˆä½¿ç”¨åœºæ™¯ä¸ç—›ç‚¹æ·±åº¦åˆ†æã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# 5Wä½¿ç”¨åœºæ™¯æ•°æ®
{five_w_context}

# ç—›ç‚¹æ•°æ®
{pain_points_data}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„ä½¿ç”¨åœºæ™¯åˆ†æï¼š

{{
  "usage_context_analysis": {{
    "scene_mapping": {{
      "description": "ä½¿ç”¨åœºæ™¯å…¨æ™¯å›¾",
      "primary_scenes": [
        {{
          "scene": "åœºæ™¯æè¿°",
          "when": "ä½¿ç”¨æ—¶æœº",
          "where": "ä½¿ç”¨åœ°ç‚¹",
          "user_type": "ç”¨æˆ·ç±»å‹",
          "frequency": "é«˜é¢‘/ä¸­é¢‘/ä½é¢‘",
          "count": æ•°å­—
        }}
      ],
      "confidence": "high/medium/low",
      "data_support": {{
        "cited_statistics": ["whereæ•°æ®ï¼šxxxï¼ˆNæ¬¡ï¼‰", "whenæ•°æ®ï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }},
    "pain_point_by_scene": {{
      "description": "åœºæ™¯åŒ–ç—›ç‚¹åˆ†æ",
      "scene_issues": [
        {{
          "scene": "åœºæ™¯",
          "pain_point": "ç—›ç‚¹æè¿°",
          "severity": "high/medium/low",
          "affected_users": "å—å½±å“ç”¨æˆ·ç¾¤",
          "improvement_suggestion": "æ”¹è¿›å»ºè®®"
        }}
      ],
      "data_support": {{
        "cited_statistics": ["ç—›ç‚¹xxxåœ¨åœºæ™¯yyyä¸­å‡ºç°Næ¬¡", ...]
      }}
    }},
    "user_journey_gaps": {{
      "description": "ç”¨æˆ·æ—…ç¨‹ç¼ºå£åˆ†æ",
      "gaps": [
        {{
          "journey_stage": "æ—…ç¨‹é˜¶æ®µï¼ˆè´­ä¹°å‰/ä½¿ç”¨ä¸­/ä½¿ç”¨åï¼‰",
          "gap_description": "ç¼ºå£æè¿°",
          "impact": "å½±å“ç¨‹åº¦",
          "recommendation": "å»ºè®®"
        }}
      ],
      "data_support": {{
        "cited_statistics": ["åŸºäºç—›ç‚¹åˆ†å¸ƒæ¨æ–­", ...]
      }}
    }}
  }}
}}

è¦æ±‚ï¼š
1. åœºæ™¯åˆ†æå¿…é¡»åŸºäº where/when/what æ•°æ®
2. ç—›ç‚¹åˆ†æå¿…é¡»å…³è”å…·ä½“ä½¿ç”¨åœºæ™¯
3. æ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

QUALITY_ROADMAP_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºæ•°æ®ï¼Œç”Ÿæˆè´¨é‡æ ‡æ†ä¸äº§å“è¿­ä»£æ–¹å‘åˆ†æã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# äº§å“æ•°æ®æ‘˜è¦
{product_summaries}

# ä¼˜åŠ¿å’Œç—›ç‚¹æ•°æ®
{pros_cons_data}

# ç”¨æˆ·å»ºè®®æ•°æ®
{suggestion_data}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„è´¨é‡ä¸è¿­ä»£åˆ†æï¼š

{{
  "quality_roadmap": {{
    "quality_benchmark": {{
      "description": "äº§å“è´¨é‡æ ‡æ†å¯¹æ¯”",
      "quality_leaders": [
        {{
          "product": åºå·,
          "excellence_areas": ["ä¼˜ç§€é¢†åŸŸ1", "ä¼˜ç§€é¢†åŸŸ2"],
          "quality_score_indicators": "è´¨é‡æŒ‡æ ‡æè¿°"
        }}
      ],
      "quality_laggards": [
        {{
          "product": åºå·,
          "improvement_areas": ["å¾…æ”¹è¿›é¢†åŸŸ1"],
          "priority": "high/medium/low"
        }}
      ],
      "data_support": {{
        "cited_statistics": ["äº§å“Nä¼˜åŠ¿xxxï¼ˆMæ¬¡ï¼‰", "äº§å“Nç—›ç‚¹xxxï¼ˆMæ¬¡ï¼‰", ...]
      }}
    }},
    "critical_issues": {{
      "description": "è‡´å‘½ç¼ºé™·ä¸ç´§æ€¥ä¿®å¤é¡¹",
      "issues": [
        {{
          "issue": "é—®é¢˜æè¿°",
          "severity": "critical/high/medium",
          "affected_products": [åºå·],
          "evidence_count": æ•°å­—,
          "fix_recommendation": "ä¿®å¤å»ºè®®"
        }}
      ],
      "data_support": {{
        "cited_statistics": ["ç—›ç‚¹æ•°æ®ï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }},
    "product_roadmap": {{
      "description": "äº§å“è¿­ä»£æ–¹å‘å»ºè®®",
      "short_term_actions": [
        {{
          "action": "è¡ŒåŠ¨é¡¹",
          "priority": "P0/P1/P2",
          "expected_impact": "é¢„æœŸå½±å“",
          "based_on": "åŸºäºçš„æ•°æ®"
        }}
      ],
      "mid_term_features": [
        {{
          "feature": "åŠŸèƒ½ç‰¹æ€§",
          "user_demand": "ç”¨æˆ·éœ€æ±‚æ¥æº",
          "demand_count": æ•°å­—
        }}
      ],
      "data_support": {{
        "cited_statistics": ["ç”¨æˆ·å»ºè®®ï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }},
    "design_recommendations": {{
      "description": "è®¾è®¡æ”¹è¿›å»ºè®®",
      "usability_improvements": ["æ˜“ç”¨æ€§æ”¹è¿›1", "æ˜“ç”¨æ€§æ”¹è¿›2"],
      "feature_requests": ["åŠŸèƒ½éœ€æ±‚1", "åŠŸèƒ½éœ€æ±‚2"],
      "data_support": {{
        "cited_statistics": ["å»ºè®®æ•°æ®ï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }}
  }}
}}

è¦æ±‚ï¼š
1. è´¨é‡å¯¹æ¯”å¿…é¡»åŸºäºä¼˜åŠ¿å’Œç—›ç‚¹æ•°æ®
2. è¿­ä»£æ–¹å‘å¿…é¡»åŸºäºç”¨æˆ·å»ºè®®
3. æ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
4. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""

ACTION_PRIORITIES_PROMPT = """åŸºäºä»¥ä¸‹ç»†åˆ†å¸‚åœºçš„å®Œæ•´åˆ†ææ•°æ®ï¼Œç”Ÿæˆä¾›åº”é“¾é£é™©ä¸è·¨éƒ¨é—¨è¡ŒåŠ¨ä¼˜å…ˆçº§ã€‚

# å¸‚åœºæ¦‚å†µ
- äº§å“æ•°é‡ï¼š{product_count}
- æ€»è¯„è®ºæ•°ï¼š{total_reviews}

# äº§å“æ•°æ®æ‘˜è¦
{product_summaries}

# ç—›ç‚¹æ•°æ®ï¼ˆä¾›åº”é“¾ç›¸å…³ï¼‰
{pain_points_data}

# ç”¨æˆ·æƒ…ç»ªæ•°æ®
{emotion_data}

è¯·è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„è¡ŒåŠ¨ä¼˜å…ˆçº§åˆ†æï¼š

{{
  "action_priorities": {{
    "supply_chain_risks": {{
      "description": "ä¾›åº”é“¾ä¸è´¨é‡é£é™©é¢„è­¦",
      "quality_risks": [
        {{
          "risk": "é£é™©æè¿°",
          "risk_level": "high/medium/low",
          "evidence_count": æ•°å­—,
          "mitigation": "ç¼“è§£æªæ–½"
        }}
      ],
      "packaging_issues": [
        {{
          "issue": "åŒ…è£…é—®é¢˜",
          "frequency": "é«˜/ä¸­/ä½",
          "recommendation": "å»ºè®®"
        }}
      ],
      "estimated_return_factors": ["é€€è´§å› ç´ 1", "é€€è´§å› ç´ 2"],
      "data_support": {{
        "cited_statistics": ["ç—›ç‚¹æ•°æ®ï¼šxxxï¼ˆNæ¬¡ï¼‰", ...]
      }}
    }},
    "department_directives": {{
      "description": "å„éƒ¨é—¨æŒ‡ä»¤",
      "product_team": {{
        "priority_actions": ["è¡ŒåŠ¨é¡¹1", "è¡ŒåŠ¨é¡¹2"],
        "focus_areas": ["å…³æ³¨é¢†åŸŸ1"],
        "data_support": {{"cited_statistics": ["ç—›ç‚¹/å»ºè®®æ•°æ®"]}}
      }},
      "marketing_team": {{
        "key_messages": ["è¥é”€ä¿¡æ¯1", "è¥é”€ä¿¡æ¯2"],
        "target_segments": ["ç›®æ ‡äººç¾¤1"],
        "avoid_claims": ["éœ€è§„é¿çš„å®£ä¼ ç‚¹"],
        "data_support": {{"cited_statistics": ["ä¼˜åŠ¿æ•°æ®"]}}
      }},
      "customer_service": {{
        "expected_issues": ["é¢„æœŸé—®é¢˜1", "é¢„æœŸé—®é¢˜2"],
        "response_templates": ["å›å¤æ¨¡æ¿å»ºè®®1"],
        "data_support": {{"cited_statistics": ["ç—›ç‚¹æ•°æ®"]}}
      }},
      "supply_chain": {{
        "qc_focus": ["è´¨æ£€é‡ç‚¹1", "è´¨æ£€é‡ç‚¹2"],
        "supplier_feedback": ["ä¾›åº”å•†åé¦ˆç‚¹"],
        "data_support": {{"cited_statistics": ["è´¨é‡ç—›ç‚¹"]}}
      }}
    }},
    "priority_action_list": {{
      "description": "ä¼˜å…ˆè¡ŒåŠ¨é¡¹æ±‡æ€»ï¼ˆæŒ‰ç´§æ€¥ç¨‹åº¦æ’åºï¼‰",
      "p0_critical": [
        {{
          "action": "ç´§æ€¥è¡ŒåŠ¨é¡¹",
          "owner": "è´Ÿè´£éƒ¨é—¨",
          "reason": "åŸå› ",
          "evidence_count": æ•°å­—
        }}
      ],
      "p1_high": [
        {{
          "action": "é«˜ä¼˜å…ˆçº§è¡ŒåŠ¨é¡¹",
          "owner": "è´Ÿè´£éƒ¨é—¨",
          "reason": "åŸå› "
        }}
      ],
      "p2_medium": [
        {{
          "action": "ä¸­ä¼˜å…ˆçº§è¡ŒåŠ¨é¡¹",
          "owner": "è´Ÿè´£éƒ¨é—¨"
        }}
      ],
      "data_support": {{
        "cited_statistics": ["åŸºäºç—›ç‚¹é¢‘æ¬¡å’Œä¸¥é‡ç¨‹åº¦æ’åº", ...]
      }}
    }},
    "risk_level_summary": {{
      "overall_risk": "low/medium/high/critical",
      "main_concerns": ["ä¸»è¦æ‹…å¿§1", "ä¸»è¦æ‹…å¿§2"],
      "positive_signals": ["ç§¯æä¿¡å·1", "ç§¯æä¿¡å·2"]
    }}
  }}
}}

è¦æ±‚ï¼š
1. ä¾›åº”é“¾é£é™©åŸºäºè´¨é‡ç›¸å…³ç—›ç‚¹
2. éƒ¨é—¨æŒ‡ä»¤è¦å…·ä½“å¯æ‰§è¡Œ
3. ä¼˜å…ˆçº§æ’åºåŸºäºæ•°æ®é¢‘æ¬¡å’Œä¸¥é‡ç¨‹åº¦
4. æ¯ä¸ªåˆ†æå¿…é¡»åŒ…å« data_support.cited_statistics
5. åªè¾“å‡ºJSONï¼Œç®€ä½“ä¸­æ–‡"""


class AnalysisService:
    """
    VOC äº§å“å¯¹æ¯”åˆ†ææœåŠ¡
    """
    
    def __init__(self, db: AsyncSession):
        self.db = db
        self.summary_service = SummaryService(db)

    # ==========================================
    # é¡¹ç›®ç®¡ç†
    # ==========================================
    
    async def create_comparison_project(
        self, 
        title: str, 
        product_ids: List[UUID],
        description: Optional[str] = None,
        role_labels: Optional[List[str]] = None
    ) -> AnalysisProject:
        """åˆ›å»ºåˆ†æé¡¹ç›®"""
        if len(product_ids) < 2:
            raise ValueError("è‡³å°‘éœ€è¦ 2 ä¸ªäº§å“")
        
        if len(product_ids) > 5:
            raise ValueError("æœ€å¤šæ”¯æŒ 5 ä¸ªäº§å“")

        stmt = select(Product).where(Product.id.in_(product_ids))
        result = await self.db.execute(stmt)
        products = result.scalars().all()
        
        if len(products) != len(product_ids):
            found_ids = {p.id for p in products}
            missing_ids = [pid for pid in product_ids if pid not in found_ids]
            raise ValueError(f"éƒ¨åˆ†äº§å“ä¸å­˜åœ¨: {missing_ids}")

        project = AnalysisProject(
            title=title,
            description=description,
            analysis_type=AnalysisType.COMPARISON.value,
            status=AnalysisStatus.PENDING.value
        )
        self.db.add(project)
        await self.db.flush()

        for i, pid in enumerate(product_ids):
            if role_labels and i < len(role_labels):
                label = role_labels[i]
            else:
                label = f"Product {i + 1}"
            
            item = AnalysisProjectItem(
                project_id=project.id,
                product_id=pid,
                role_label=label,
                display_order=i
            )
            self.db.add(item)
        
        await self.db.commit()
        await self.db.refresh(project)
        return project

    async def create_market_insight_project(
        self, 
        title: str, 
        product_ids: List[UUID],
        description: Optional[str] = None,
        role_labels: Optional[List[str]] = None
    ) -> AnalysisProject:
        """åˆ›å»ºç»†åˆ†å¸‚åœºæ´å¯Ÿé¡¹ç›®"""
        if len(product_ids) < 2:
            raise ValueError("å¸‚åœºæ´å¯Ÿè‡³å°‘éœ€è¦ 2 ä¸ªäº§å“")
        
        if len(product_ids) > 10:
            raise ValueError("å¸‚åœºæ´å¯Ÿæœ€å¤šæ”¯æŒ 10 ä¸ªäº§å“")

        stmt = select(Product).where(Product.id.in_(product_ids))
        result = await self.db.execute(stmt)
        products = result.scalars().all()
        
        if len(products) != len(product_ids):
            found_ids = {p.id for p in products}
            missing_ids = [pid for pid in product_ids if pid not in found_ids]
            raise ValueError(f"éƒ¨åˆ†äº§å“ä¸å­˜åœ¨: {missing_ids}")

        project = AnalysisProject(
            title=title,
            description=description,
            analysis_type=AnalysisType.MARKET_INSIGHT.value,
            status=AnalysisStatus.PENDING.value
        )
        self.db.add(project)
        await self.db.flush()

        for i, pid in enumerate(product_ids):
            if role_labels and i < len(role_labels):
                label = role_labels[i]
            else:
                label = f"Product {i + 1}"
            
            item = AnalysisProjectItem(
                project_id=project.id,
                product_id=pid,
                role_label=label,
                display_order=i
            )
            self.db.add(item)
        
        await self.db.commit()
        await self.db.refresh(project)
        return project

    async def get_project(self, project_id: UUID) -> Optional[AnalysisProject]:
        """è·å–é¡¹ç›®è¯¦æƒ…"""
        stmt = (
            select(AnalysisProject)
            .options(selectinload(AnalysisProject.items).selectinload(AnalysisProjectItem.product))
            .where(AnalysisProject.id == project_id)
        )
        result = await self.db.execute(stmt)
        return result.scalar_one_or_none()

    async def list_projects(
        self, 
        limit: int = 20, 
        offset: int = 0,
        status: Optional[str] = None,
        admin_only: bool = False,
        user_id: Optional[UUID] = None
    ) -> List[AnalysisProject]:
        """
        è·å–é¡¹ç›®åˆ—è¡¨
        
        Args:
            limit: æ¯é¡µæ•°é‡
            offset: åç§»é‡
            status: æŒ‰çŠ¶æ€ç­›é€‰
            admin_only: åªè¿”å›åŒ…å«ç®¡ç†å‘˜å…³æ³¨äº§å“çš„é¡¹ç›®ï¼ˆç”¨äºå¸‚åœºæ´å¯Ÿå¹¿åœºï¼‰
            user_id: åªè¿”å›æŒ‡å®šç”¨æˆ·åˆ›å»ºçš„é¡¹ç›®ï¼ˆç”¨äºæˆ‘çš„é¡¹ç›®ï¼‰
        """
        from app.models.user import User
        from app.models.user_project import UserProject
        
        if admin_only:
            # å¸‚åœºæ´å¯Ÿå¹¿åœºï¼šåªæ˜¾ç¤ºåŒ…å«ç®¡ç†å‘˜å…³æ³¨äº§å“çš„é¡¹ç›®
            # æŸ¥è¯¢é€»è¾‘ï¼šanalysis_projects -> analysis_project_items -> products -> user_projects -> users(is_admin=true)
            stmt = (
                select(AnalysisProject)
                .options(selectinload(AnalysisProject.items).selectinload(AnalysisProjectItem.product))
                .join(AnalysisProjectItem, AnalysisProject.id == AnalysisProjectItem.project_id)
                .join(UserProject, AnalysisProjectItem.product_id == UserProject.product_id)
                .join(User, UserProject.user_id == User.id)
                .where(User.is_admin == True)
                .where(UserProject.is_deleted == False)
                .distinct()
                .order_by(desc(AnalysisProject.created_at))
                .limit(limit)
                .offset(offset)
            )
            
            if status:
                stmt = stmt.where(AnalysisProject.status == status)
        elif user_id:
            # æˆ‘çš„é¡¹ç›®ï¼šåªæ˜¾ç¤ºå½“å‰ç”¨æˆ·åˆ›å»ºçš„é¡¹ç›®
            stmt = (
                select(AnalysisProject)
                .options(selectinload(AnalysisProject.items).selectinload(AnalysisProjectItem.product))
                .where(AnalysisProject.user_id == user_id)
                .order_by(desc(AnalysisProject.created_at))
                .limit(limit)
                .offset(offset)
            )
            
            if status:
                stmt = stmt.where(AnalysisProject.status == status)
        else:
            # æ™®é€šåˆ—è¡¨æŸ¥è¯¢
            stmt = (
                select(AnalysisProject)
                .options(selectinload(AnalysisProject.items).selectinload(AnalysisProjectItem.product))
                .order_by(desc(AnalysisProject.created_at))
                .limit(limit)
                .offset(offset)
            )
            
            if status:
                stmt = stmt.where(AnalysisProject.status == status)
        
        result = await self.db.execute(stmt)
        return list(result.scalars().unique().all())

    async def delete_project(self, project_id: UUID) -> bool:
        """åˆ é™¤é¡¹ç›®"""
        project = await self.db.get(AnalysisProject, project_id)
        if not project:
            return False
        
        await self.db.delete(project)
        await self.db.commit()
        return True

    # ==========================================
    # æ ¸å¿ƒåˆ†æé€»è¾‘ - è·¯ç”±å…¥å£
    # ==========================================
    
    async def run_analysis(self, project_id: UUID) -> AnalysisProject:
        """
        åˆ†æå…¥å£ï¼šæ ¹æ® analysis_type è·¯ç”±åˆ°ä¸åŒçš„åˆ†ææ–¹æ³•
        """
        project = await self.get_project(project_id)
        if not project or not project.items:
            raise ValueError("é¡¹ç›®æ— æ•ˆ")
        
        # æ ¹æ®åˆ†æç±»å‹è·¯ç”±
        if project.analysis_type == AnalysisType.MARKET_INSIGHT.value:
            return await self._run_market_insight_analysis(project)
        else:
            # é»˜è®¤æ‰§è¡Œå¯¹æ¯”åˆ†æ
            return await self._run_comparison_analysis(project)

    # ==========================================
    # å¯¹æ¯”åˆ†æé€»è¾‘ (Comparison Analysis)
    # ==========================================
    
    async def _run_comparison_analysis(self, project: AnalysisProject) -> AnalysisProject:
        """
        æ‰§è¡Œ VOC å¯¹æ¯”åˆ†æ
        
        ä¼˜åŒ–æ¶æ„ï¼š
        1. ä½¿ç”¨ AsyncOpenAI å¼‚æ­¥å®¢æˆ·ç«¯
        2. æ¯ä¸ªäº§å“ç‹¬ç«‹åˆ†æï¼ˆå°è¯·æ±‚ï¼Œç¨³å®šï¼‰
        3. å¹¶è¡Œè°ƒç”¨ AIï¼ˆå¤šäº§å“åŒæ—¶åˆ†æï¼‰
        4. ç”Ÿæˆç»´åº¦æ´å¯Ÿå’Œç­–ç•¥æ€»ç»“
        """

        try:
            # æ›´æ–°çŠ¶æ€
            project.status = AnalysisStatus.PROCESSING.value
            await self.db.commit()

            # 1. æ”¶é›†äº§å“æ•°æ®ï¼ˆé¡ºåºï¼Œå› ä¸º SQLAlchemy é™åˆ¶ï¼‰
            products_info = []
            product_data_map = {}
            product_count = len(project.items)  # è·å–äº§å“æ€»æ•°ï¼Œç”¨äºåŠ¨æ€è°ƒæ•´æ ‡ç­¾æ•°é‡
            
            for item in project.items:
                res = await self._fetch_product_data(item, product_count=product_count)
                products_info.append(res)
                product_data_map[res['name']] = res['data']
                product_data_map[res['name']]['asin'] = res['asin']
            
            # ä¿å­˜å¿«ç…§
            project.raw_data_snapshot = product_data_map
            await self.db.commit()
            
            # 2. è·å–å¼‚æ­¥å®¢æˆ·ç«¯
            client = get_async_client()
            
            # 3. å¹¶è¡Œåˆ†ææ¯ä¸ªäº§å“
            logger.info(f"å¼€å§‹å¹¶è¡Œåˆ†æ {len(products_info)} ä¸ªäº§å“...")
            
            async def analyze_single_product(info: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:
                """åˆ†æå•ä¸ªäº§å“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œç¡®ä¿ç¨³å¥æ€§ï¼‰"""
                prompt = SINGLE_PRODUCT_PROMPT.format(
                    product_name=info['name'],
                    asin=info['asin'],
                    stats_json=json.dumps(info['data'], ensure_ascii=False)
                )
                
                last_error = None
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=4000,  # å¢åŠ  token é™åˆ¶ï¼Œç¡®ä¿å®Œæ•´è¾“å‡º
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        result = json.loads(content.replace("```json", "").replace("```", "").strip())
                        
                        # éªŒè¯ç»“æœå®Œæ•´æ€§
                        if not result.get("five_w") or not result.get("dimensions"):
                            raise ValueError("AI è¿”å›çš„æ•°æ®ç»“æ„ä¸å®Œæ•´")
                        
                        return result
                        
                    except json.JSONDecodeError as e:
                        last_error = e
                        logger.warning(f"äº§å“ {info['asin']} ç¬¬ {attempt+1}/{max_retries} æ¬¡ JSON è§£æå¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    except Exception as e:
                        last_error = e
                        logger.warning(f"äº§å“ {info['asin']} ç¬¬ {attempt+1}/{max_retries} æ¬¡åˆ†æå¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 * (attempt + 1))
                
                # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åçš„é”™è¯¯
                raise last_error or Exception("æœªçŸ¥é”™è¯¯")
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰äº§å“åˆ†æ
            product_profiles = await asyncio.gather(
                *[analyze_single_product(info) for info in products_info],
                return_exceptions=True
            )
            
            # è¿‡æ»¤é”™è¯¯å¹¶æ·»åŠ  image_url
            valid_profiles = []
            for i, result in enumerate(product_profiles):
                if isinstance(result, Exception):
                    logger.error(f"äº§å“ {i+1} åˆ†æå¤±è´¥: {result}")
                    # åˆ›å»ºç©ºçš„å ä½ç»“æœ
                    valid_profiles.append({
                        "product_name": products_info[i]['name'],
                        "asin": products_info[i]['asin'],
                        "image_url": products_info[i].get('image_url'),
                        "five_w": {"who": [], "when": [], "where": [], "why": [], "what": []},
                        "dimensions": {"pros": [], "cons": []},
                        "error": str(result)
                    })
                else:
                    # æ·»åŠ  image_url åˆ°ç»“æœä¸­
                    result["image_url"] = products_info[i].get('image_url')
                    valid_profiles.append(result)
            
            logger.info(f"äº§å“åˆ†æå®Œæˆï¼ŒæˆåŠŸ {len([p for p in valid_profiles if 'error' not in p])} ä¸ª")
            
            # 4. ç”Ÿæˆäº§å“æ‘˜è¦ç”¨äºåç»­åˆ†æ
            product_summaries = self._generate_product_summaries(valid_profiles)
            
            # 5. åˆ†æ‰¹ç”Ÿæˆç»´åº¦æ´å¯Ÿå’Œç­–ç•¥æ€»ç»“
            async def generate_dimension_insights_batch(dimensions: List[str], batch_name: str) -> Dict[str, Any]:
                """åˆ†æ‰¹ç”Ÿæˆç»´åº¦æ´å¯Ÿï¼ˆæ¯æ‰¹3-5ä¸ªç»´åº¦ï¼‰"""
                dimension_names = {
                    "who": "ç”¨æˆ·æ˜¯è°", "when": "ä½•æ—¶ä½¿ç”¨", "where": "åœ¨å“ªé‡Œç”¨",
                    "why": "è´­ä¹°åŠ¨æœº", "what": "å…·ä½“ç”¨é€”", "pros": "ä¼˜åŠ¿å–ç‚¹",
                    "cons": "ç—›ç‚¹é—®é¢˜", "suggestion": "ç”¨æˆ·å»ºè®®", 
                    "scenario": "ä½¿ç”¨åœºæ™¯", "emotion": "æƒ…ç»ªåé¦ˆ"
                }
                
                dim_list = ", ".join([f"{d}({dimension_names[d]})" for d in dimensions])
                
                batch_prompt = f"""åŸºäºä»¥ä¸‹äº§å“çš„å¯¹æ¯”æ•°æ®ï¼Œä¸ºæŒ‡å®šç»´åº¦ç”Ÿæˆæ´å¯Ÿåˆ†æã€‚

äº§å“æ•°é‡ï¼š{len(valid_profiles)}
äº§å“åˆ—è¡¨ï¼š
{product_summaries}

è¯·ä¸ºä»¥ä¸‹ç»´åº¦ç”Ÿæˆæ´å¯Ÿï¼š{dim_list}

æ¯ä¸ªç»´åº¦çš„æ´å¯ŸåŒ…å«ï¼š
1. nameï¼šç»´åº¦ä¸­æ–‡åç§°
2. commonalityï¼šæ‰€æœ‰äº§å“çš„å…±æ€§ç‰¹å¾ï¼ˆ1å¥è¯ï¼‰
3. differencesï¼šæ¯ä¸ªäº§å“çš„å·®å¼‚ç‰¹ç‚¹ï¼ˆæ•°ç»„ï¼Œæ¯é¡¹åŒ…å« product åºå·å’Œ text æè¿°ï¼‰
4. positioningï¼šæ¯ä¸ªäº§å“çš„å®šä½æ´å¯Ÿï¼ˆæ•°ç»„ï¼Œæ¯é¡¹åŒ…å« product åºå·å’Œ text æè¿°ï¼‰

è¾“å‡ºJSONæ ¼å¼ï¼ˆåªè¾“å‡ºæŒ‡å®šç»´åº¦ï¼‰ï¼š
{{
  "dimension_insights": {{
    "{dimensions[0]}": {{
      "name": "{dimension_names[dimensions[0]]}",
      "commonality": "...",
      "differences": [{{"product": 1, "text": "..."}}, ...],
      "positioning": [{{"product": 1, "text": "..."}}, ...]
    }},
    ...
  }}
}}

è¦æ±‚ï¼šç®€ä½“ä¸­æ–‡ï¼Œåªè¾“å‡ºJSONã€‚"""

                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        logger.info(f"ç”Ÿæˆç»´åº¦æ´å¯Ÿæ‰¹æ¬¡ [{batch_name}]: {dimensions}")
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": batch_prompt}
                            ],
                            temperature=0.3,
                            max_tokens=2500,  # æ¯æ‰¹åªéœ€è¦è¾ƒå°‘çš„ token
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        logger.info(f"ç»´åº¦æ´å¯Ÿæ‰¹æ¬¡ [{batch_name}] å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")
                        
                        result = json.loads(content.replace("```json", "").replace("```", "").strip())
                        return result.get("dimension_insights", {})
                    except json.JSONDecodeError as e:
                        logger.error(f"ç»´åº¦æ´å¯Ÿæ‰¹æ¬¡ [{batch_name}] JSON è§£æå¤±è´¥: {e}")
                        return {}
                    except Exception as e:
                        logger.warning(f"ç»´åº¦æ´å¯Ÿæ‰¹æ¬¡ [{batch_name}] å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"ç»´åº¦æ´å¯Ÿæ‰¹æ¬¡ [{batch_name}] æœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_all_dimension_insights() -> Dict[str, Any]:
                """åˆ†3æ‰¹ç”Ÿæˆæ‰€æœ‰10ä¸ªç»´åº¦çš„æ´å¯Ÿ"""
                # å°†10ä¸ªç»´åº¦åˆ†æˆ3æ‰¹ï¼š5Wç”»åƒ(5ä¸ª) + æ­£é¢å£ç¢‘(2ä¸ª) + è´Ÿé¢/å»ºè®®å£ç¢‘(3ä¸ª)
                batches = [
                    (["who", "when", "where", "why", "what"], "5Wç”¨æˆ·ç”»åƒ"),
                    (["pros", "cons"], "ä¼˜åŠ¿ç—›ç‚¹"),
                    (["suggestion", "scenario", "emotion"], "å»ºè®®åœºæ™¯æƒ…ç»ª"),
                ]
                
                all_insights = {}
                for dimensions, batch_name in batches:
                    batch_result = await generate_dimension_insights_batch(dimensions, batch_name)
                    all_insights.update(batch_result)
                    # æ‰¹æ¬¡ä¹‹é—´ç¨ä½œåœé¡¿ï¼Œé¿å… API é™æµ
                    await asyncio.sleep(1)
                
                logger.info(f"ç»´åº¦æ´å¯Ÿç”Ÿæˆå®Œæˆï¼Œå…± {len(all_insights)} ä¸ªç»´åº¦")
                return {"dimension_insights": all_insights}
            
            async def generate_strategy_summary() -> Dict[str, Any]:
                """ç”Ÿæˆç­–ç•¥æ€»ç»“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
                prompt = STRATEGY_SUMMARY_PROMPT.format(
                    product_count=len(valid_profiles),
                    product_summaries=product_summaries
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=1500,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"ç­–ç•¥æ€»ç»“ç”Ÿæˆå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
                        else:
                            logger.error(f"ç­–ç•¥æ€»ç»“ç”Ÿæˆæœ€ç»ˆå¤±è´¥: {e}")
                            return {"market_summary": "", "strategy_summary": {}}
            
            # å¹¶è¡Œæ‰§è¡Œæ´å¯Ÿå’Œæ€»ç»“ç”Ÿæˆ
            insights_result, strategy_result = await asyncio.gather(
                generate_all_dimension_insights(),
                generate_strategy_summary(),
                return_exceptions=True
            )
            
            # å¤„ç†ç»“æœ
            dimension_insights = {}
            if isinstance(insights_result, Exception):
                logger.error(f"ç»´åº¦æ´å¯Ÿç”Ÿæˆå¤±è´¥: {insights_result}")
            else:
                dimension_insights = insights_result.get("dimension_insights", {})
            
            strategy_summary = {}
            market_summary = ""
            if isinstance(strategy_result, Exception):
                logger.error(f"ç­–ç•¥æ€»ç»“ç”Ÿæˆå¤±è´¥: {strategy_result}")
            else:
                market_summary = strategy_result.get("market_summary", "")
                strategy_summary = strategy_result.get("strategy_summary", {})
            
            # 6. ç»„è£…æœ€ç»ˆç»“æœ
            result_data = {
                "product_profiles": valid_profiles,
                "dimension_insights": dimension_insights,
                "market_summary": market_summary,
                "strategy_summary": strategy_summary
            }
            
            project.result_content = result_data
            project.status = AnalysisStatus.COMPLETED.value
            project.error_message = None
            
            logger.info(f"å¯¹æ¯”åˆ†æå®Œæˆ: {project.id}")
            
        except Exception as e:
            logger.error(f"Analysis Error: {e}", exc_info=True)
            project.status = AnalysisStatus.FAILED.value
            project.error_message = str(e)
        
        await self.db.commit()
        await self.db.refresh(project)
        return project

    def _generate_product_summaries(self, profiles: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆäº§å“æ‘˜è¦ç”¨äºåç»­ promptï¼ˆ10ç»´ï¼‰"""
        summaries = []
        for i, p in enumerate(profiles, 1):
            name = p.get("product_name", f"äº§å“{i}")
            asin = p.get("asin", "")
            
            # æå–å…³é”®æ ‡ç­¾ - 5Wç”¨æˆ·ç”»åƒ
            five_w = p.get("five_w", {})
            who_tags = [t.get("label", "") for t in five_w.get("who", [])[:3]]
            when_tags = [t.get("label", "") for t in five_w.get("when", [])[:3]]
            where_tags = [t.get("label", "") for t in five_w.get("where", [])[:3]]
            why_tags = [t.get("label", "") for t in five_w.get("why", [])[:3]]
            what_tags = [t.get("label", "") for t in five_w.get("what", [])[:3]]
            
            # æå–å…³é”®æ ‡ç­¾ - 5ç±»å£ç¢‘æ´å¯Ÿ
            dims = p.get("dimensions", {})
            pros_tags = [t.get("label", "") for t in dims.get("pros", [])[:3]]
            cons_tags = [t.get("label", "") for t in dims.get("cons", [])[:3]]
            suggestion_tags = [t.get("label", "") for t in dims.get("suggestion", [])[:3]]
            scenario_tags = [t.get("label", "") for t in dims.get("scenario", [])[:3]]
            emotion_tags = [t.get("label", "") for t in dims.get("emotion", [])[:3]]
            
            summary = f"""äº§å“{i}: {name} ({asin})
  ã€5Wç”¨æˆ·ç”»åƒã€‘
  - ç”¨æˆ·(Who): {', '.join(who_tags) or 'æ— æ•°æ®'}
  - æ—¶æœº(When): {', '.join(when_tags) or 'æ— æ•°æ®'}
  - åœºæ™¯(Where): {', '.join(where_tags) or 'æ— æ•°æ®'}
  - åŠ¨æœº(Why): {', '.join(why_tags) or 'æ— æ•°æ®'}
  - ç”¨é€”(What): {', '.join(what_tags) or 'æ— æ•°æ®'}
  ã€5ç±»å£ç¢‘æ´å¯Ÿã€‘
  - ä¼˜åŠ¿(Pros): {', '.join(pros_tags) or 'æ— æ•°æ®'}
  - ç—›ç‚¹(Cons): {', '.join(cons_tags) or 'æ— æ•°æ®'}
  - å»ºè®®(Suggestion): {', '.join(suggestion_tags) or 'æ— æ•°æ®'}
  - åœºæ™¯(Scenario): {', '.join(scenario_tags) or 'æ— æ•°æ®'}
  - æƒ…ç»ª(Emotion): {', '.join(emotion_tags) or 'æ— æ•°æ®'}"""
            summaries.append(summary)
        
        return "\n\n".join(summaries)

    async def _fetch_product_data(self, item: AnalysisProjectItem, product_count: int = 1) -> Dict[str, Any]:
        """
        [Helper] å¼‚æ­¥è·å–å•ä¸ªäº§å“çš„å…¨é‡æ•°æ®
        
        Args:
            item: åˆ†æé¡¹ç›®æ¡ç›®
            product_count: æ€»äº§å“æ•°é‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´æ ‡ç­¾æ•°é‡ï¼‰
        """
        product = item.product
        
        # æ„å»ºå®‰å…¨çš„äº§å“åç§°
        raw_name = product.title_translated or product.title or product.asin
        safe_name = raw_name[:30].replace('"', '').replace("'", "").strip() + f" ({product.asin[-4:]})"
        
        # èšåˆæ ¸å¿ƒæ•°æ® (5W + Insight)
        context_stats = await self.summary_service._aggregate_5w_stats(product.id)
        insight_stats = await self.summary_service._aggregate_insight_stats(product.id)
        
        return {
            "name": safe_name,
            "asin": product.asin,
            "image_url": product.image_url,
            "data": {
                "user_context": self._simplify_stats(context_stats, product_count=product_count),
                "key_insights": self._simplify_stats(insight_stats, product_count=product_count)
            }
        }

    def _simplify_stats(self, data: Dict[str, Any], max_items: int = 15, product_count: int = 1) -> Dict[str, List[Dict[str, Any]]]:
        """
        ç²¾ç®€æ•°æ®ï¼šæ¯ç±»åªä¿ç•™ Top Nï¼Œåªä¿ç•™ label å’Œ count
        
        åŠ¨æ€è°ƒæ•´ç­–ç•¥ï¼ˆç¡®ä¿ Token ä¸è¶…é™ï¼‰ï¼š
        - 2ä¸ªäº§å“: æ¯ç»´åº¦æœ€å¤š 20 ä¸ªæ ‡ç­¾
        - 3ä¸ªäº§å“: æ¯ç»´åº¦æœ€å¤š 15 ä¸ªæ ‡ç­¾
        - 4-5ä¸ªäº§å“: æ¯ç»´åº¦æœ€å¤š 12 ä¸ªæ ‡ç­¾
        """
        # æ ¹æ®äº§å“æ•°é‡åŠ¨æ€è°ƒæ•´æ ‡ç­¾æ•°é‡
        if product_count <= 2:
            max_items = 20
        elif product_count == 3:
            max_items = 15
        else:
            max_items = 12
        simplified = {}
        
        for category, content in data.items():
            if not isinstance(content, dict): 
                continue
            
            items = content.get("items", [])
            # åªä¿ç•™å¿…è¦å­—æ®µï¼Œå‡å°‘ Token
            simplified[category] = [
                {"label": item.get("name"), "count": item.get("value")}
                for item in items[:max_items]
                if isinstance(item, dict)
            ]
        
        return simplified

    # ==========================================
    # é¢„è§ˆåŠŸèƒ½
    # ==========================================
    
    async def get_comparison_preview(self, product_ids: List[UUID]) -> Dict[str, Any]:
        """
        è·å–å¯¹æ¯”é¢„è§ˆæ•°æ®ï¼ˆä¸è°ƒç”¨ AIï¼Œä»…è¿”å›èšåˆæ•°æ®ï¼‰
        """
        if len(product_ids) < 2:
            raise ValueError("å¯¹æ¯”åˆ†æè‡³å°‘éœ€è¦ 2 ä¸ªäº§å“")
        
        preview_data = {}
        
        for pid in product_ids:
            product = await self.db.get(Product, pid)
            if not product:
                continue
            
            total_reviews = await self.summary_service._count_translated_reviews(pid)
            
            preview_data[str(pid)] = {
                "product": {
                    "id": str(product.id),
                    "asin": product.asin,
                    "title": product.title_translated or product.title,
                    "image_url": product.image_url,
                    "marketplace": product.marketplace
                },
                "total_reviews": total_reviews,
                "ready": total_reviews > 0
            }
        
        return {
            "success": True,
            "products": preview_data,
            "can_compare": len(preview_data) >= 2 and all(p.get("ready", False) for p in preview_data.values())
        }

    # ==========================================
    # ç»†åˆ†å¸‚åœºæ´å¯Ÿåˆ†æ (Market Insight Analysis)
    # ==========================================
    
    async def _run_market_insight_analysis(self, project: AnalysisProject) -> AnalysisProject:
        """
        æ‰§è¡Œç»†åˆ†å¸‚åœºæ´å¯Ÿåˆ†æ
        
        [UPDATED 2026-01-17] é‡æ„æ¶æ„ï¼š
        0. [NEW] é¡¹ç›®çº§å­¦ä¹ ï¼šå­¦ä¹ ç»Ÿä¸€ç»´åº¦/æ ‡ç­¾å¹¶å»ºç«‹æ˜ å°„
        1. æ”¶é›†æ‰€æœ‰äº§å“æ•°æ®
        2. èšåˆå¸‚åœºçº§åˆ«æ•°æ®ï¼ˆåŸºäºæ˜ å°„å…³ç³»ï¼‰
        3. ç”Ÿæˆå¸‚åœºå…±æ€§æ´å¯Ÿ
        4. ç”Ÿæˆå¸‚åœºç”¨æˆ·ç”»åƒ
        5. æŒ–æ˜å¸‚åœºæœºä¼š
        6. åˆ†æå¸‚åœºè¶‹åŠ¿
        7. ä¿ç•™äº§å“å¯¹æ¯”è§†è§’
        """
        try:
            # æ›´æ–°çŠ¶æ€
            project.status = AnalysisStatus.PROCESSING.value
            await self.db.commit()
            
            # è·å–äº§å“ ID åˆ—è¡¨
            product_ids = [item.product_id for item in project.items]
            product_count = len(product_ids)
            
            # =================================================================
            # [NEW] Step 0: é¡¹ç›®çº§ç»´åº¦/æ ‡ç­¾å­¦ä¹ 
            # =================================================================
            from app.services.project_learning_service import ProjectLearningService
            
            logger.info(f"ğŸ“ å¼€å§‹é¡¹ç›®çº§å­¦ä¹ ï¼ˆ{product_count} ä¸ªäº§å“ï¼‰...")
            
            learning_service = ProjectLearningService(self.db)
            
            # æ‰§è¡Œé¡¹ç›®çº§å­¦ä¹ 
            learning_result = await learning_service.learn_project_dimensions_and_labels(
                project_id=project.id,
                product_ids=product_ids,
                sample_per_product=40,  # æ¯ä¸ªäº§å“é‡‡æ · 40 æ¡
                max_total_samples=100  # æœ€å¤š 100 æ¡æ€»æ ·æœ¬
            )
            
            logger.info(f"âœ… é¡¹ç›®çº§å­¦ä¹ å®Œæˆï¼š{learning_result.get('sample_stats', {}).get('total_reviews', 0)} æ¡è¯„è®º")
            
            # =================================================================
            # Step 1: æ”¶é›†äº§å“æ•°æ®
            # =================================================================
            products_info = []
            product_data_map = {}
            total_reviews = 0
            
            for item in project.items:
                res = await self._fetch_product_data_full(item)
                products_info.append(res)
                product_data_map[res['name']] = res['data']
                product_data_map[res['name']]['asin'] = res['asin']
                total_reviews += res.get('review_count', 0)
            
            # ä¿å­˜å¿«ç…§
            project.raw_data_snapshot = product_data_map
            await self.db.commit()
            
            # 2. èšåˆå¸‚åœºçº§åˆ«æ•°æ®
            market_aggregated = self._aggregate_market_data(products_info)
            
            # 3. è·å–å¼‚æ­¥å®¢æˆ·ç«¯
            client = get_async_client()
            
            # 4. ç”Ÿæˆäº§å“æ‘˜è¦
            product_summaries = self._generate_product_summaries_for_market(products_info)
            
            # 5. å¹¶è¡Œæ‰§è¡Œ AI åˆ†æ
            logger.info(f"å¼€å§‹å¸‚åœºæ´å¯Ÿåˆ†æï¼Œ{product_count} ä¸ªäº§å“ï¼Œ{total_reviews} æ¡è¯„è®º...")
            
            async def generate_market_overview() -> Dict[str, Any]:
                """ç”Ÿæˆå¸‚åœºæ¦‚è§ˆåˆ†æ"""
                prompt = MARKET_AGGREGATION_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    market_name=project.title,
                    aggregated_stats=json.dumps(market_aggregated, ensure_ascii=False),
                    product_summaries=product_summaries
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3000,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"å¸‚åœºæ¦‚è§ˆç”Ÿæˆå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"å¸‚åœºæ¦‚è§ˆç”Ÿæˆæœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_market_persona() -> Dict[str, Any]:
                """ç”Ÿæˆå¸‚åœºç”¨æˆ·ç”»åƒ"""
                prompt = MARKET_SEGMENT_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    five_w_stats=json.dumps(market_aggregated.get("five_w", {}), ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=2500,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"å¸‚åœºç”»åƒç”Ÿæˆå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"å¸‚åœºç”»åƒç”Ÿæˆæœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_market_opportunities() -> Dict[str, Any]:
                """æŒ–æ˜å¸‚åœºæœºä¼š"""
                pain_points_data = {
                    "cons": market_aggregated.get("dimensions", {}).get("cons", []),
                    "suggestion": market_aggregated.get("dimensions", {}).get("suggestion", [])
                }
                
                prompt = MARKET_OPPORTUNITY_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    product_summaries=product_summaries,
                    pain_points_data=json.dumps(pain_points_data, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3000,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"å¸‚åœºæœºä¼šæŒ–æ˜å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"å¸‚åœºæœºä¼šæŒ–æ˜æœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_market_trends() -> Dict[str, Any]:
                """åˆ†æå¸‚åœºè¶‹åŠ¿"""
                needs_distribution = {
                    "pros": market_aggregated.get("dimensions", {}).get("pros", []),
                    "cons": market_aggregated.get("dimensions", {}).get("cons", []),
                    "what": market_aggregated.get("five_w", {}).get("what", []),
                    "why": market_aggregated.get("five_w", {}).get("why", [])
                }
                
                prompt = MARKET_TREND_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    product_summaries=product_summaries,
                    needs_distribution=json.dumps(needs_distribution, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=2500,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"å¸‚åœºè¶‹åŠ¿åˆ†æå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"å¸‚åœºè¶‹åŠ¿åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            # ============== æ–°å¢4ä¸ªåˆ†ææ¨¡å— ==============
            
            async def generate_strategic_positioning() -> Dict[str, Any]:
                """ç”Ÿæˆæˆ˜ç•¥å®šä½ä¸SWOTåˆ†æ"""
                aggregated_pros_cons = {
                    "pros": market_aggregated.get("dimensions", {}).get("pros", []),
                    "cons": market_aggregated.get("dimensions", {}).get("cons", [])
                }
                
                prompt = STRATEGIC_POSITIONING_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    product_summaries=product_summaries,
                    aggregated_pros_cons=json.dumps(aggregated_pros_cons, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3000,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"æˆ˜ç•¥å®šä½åˆ†æå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"æˆ˜ç•¥å®šä½åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_usage_context_analysis() -> Dict[str, Any]:
                """ç”Ÿæˆä½¿ç”¨åœºæ™¯ä¸ç—›ç‚¹åˆ†æ"""
                five_w_context = {
                    "where": market_aggregated.get("five_w", {}).get("where", []),
                    "when": market_aggregated.get("five_w", {}).get("when", []),
                    "what": market_aggregated.get("five_w", {}).get("what", []),
                    "user": market_aggregated.get("five_w", {}).get("user", [])
                }
                pain_points = {
                    "cons": market_aggregated.get("dimensions", {}).get("cons", []),
                    "scenario": market_aggregated.get("dimensions", {}).get("scenario", [])
                }
                
                prompt = USAGE_CONTEXT_ANALYSIS_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    five_w_context=json.dumps(five_w_context, ensure_ascii=False),
                    pain_points_data=json.dumps(pain_points, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3000,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"ä½¿ç”¨åœºæ™¯åˆ†æå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"ä½¿ç”¨åœºæ™¯åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_quality_roadmap() -> Dict[str, Any]:
                """ç”Ÿæˆè´¨é‡æ ‡æ†ä¸äº§å“è¿­ä»£æ–¹å‘"""
                pros_cons = {
                    "pros": market_aggregated.get("dimensions", {}).get("pros", []),
                    "cons": market_aggregated.get("dimensions", {}).get("cons", [])
                }
                suggestions = market_aggregated.get("dimensions", {}).get("suggestion", [])
                
                prompt = QUALITY_ROADMAP_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    product_summaries=product_summaries,
                    pros_cons_data=json.dumps(pros_cons, ensure_ascii=False),
                    suggestion_data=json.dumps(suggestions, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3000,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"è´¨é‡è¿­ä»£åˆ†æå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"è´¨é‡è¿­ä»£åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            async def generate_action_priorities() -> Dict[str, Any]:
                """ç”Ÿæˆä¾›åº”é“¾é£é™©ä¸è¡ŒåŠ¨ä¼˜å…ˆçº§"""
                pain_points = {
                    "cons": market_aggregated.get("dimensions", {}).get("cons", []),
                    "suggestion": market_aggregated.get("dimensions", {}).get("suggestion", [])
                }
                emotions = market_aggregated.get("dimensions", {}).get("emotion", [])
                
                prompt = ACTION_PRIORITIES_PROMPT.format(
                    product_count=product_count,
                    total_reviews=total_reviews,
                    product_summaries=product_summaries,
                    pain_points_data=json.dumps(pain_points, ensure_ascii=False),
                    emotion_data=json.dumps(emotions, ensure_ascii=False)
                )
                
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = await client.chat.completions.create(
                            model=settings.QWEN_ANALYSIS_MODEL,
                            messages=[
                                {"role": "system", "content": "è¾“å‡ºçº¯JSONï¼Œç®€ä½“ä¸­æ–‡ã€‚"},
                                {"role": "user", "content": prompt}
                            ],
                            temperature=0.3,
                            max_tokens=3500,
                            response_format={"type": "json_object"}
                        )
                        
                        content = response.choices[0].message.content
                        return json.loads(content.replace("```json", "").replace("```", "").strip())
                    except Exception as e:
                        logger.warning(f"è¡ŒåŠ¨ä¼˜å…ˆçº§åˆ†æå°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3 * (attempt + 1))
                        else:
                            logger.error(f"è¡ŒåŠ¨ä¼˜å…ˆçº§åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                            return {}
            
            # ============== ä¸¤è½®å¹¶è¡Œæ‰§è¡Œç­–ç•¥ ==============
            # Round 1: åŸºç¡€åˆ†ææ¨¡å—ï¼ˆmarket_overview, market_persona, market_opportunities, market_trendsï¼‰
            # Round 2: æ·±åº¦åˆ†ææ¨¡å—ï¼ˆstrategic_positioning, usage_context_analysis, quality_roadmap, action_prioritiesï¼‰
            
            logger.info("ğŸ“Š å¼€å§‹ç¬¬ä¸€è½®å¹¶è¡Œåˆ†æï¼ˆå¸‚åœºæ¦‚è§ˆã€ç”¨æˆ·ç”»åƒã€æœºä¼šæŒ–æ˜ã€è¶‹åŠ¿åˆ†æï¼‰...")
            
            overview_result, persona_result, opportunity_result, trend_result = await asyncio.gather(
                generate_market_overview(),
                generate_market_persona(),
                generate_market_opportunities(),
                generate_market_trends(),
                return_exceptions=True
            )
            
            # å¤„ç†ç¬¬ä¸€è½®ç»“æœ
            market_overview = overview_result if not isinstance(overview_result, Exception) else {}
            market_persona = persona_result if not isinstance(persona_result, Exception) else {}
            market_opportunities = opportunity_result if not isinstance(opportunity_result, Exception) else {}
            market_trends = trend_result if not isinstance(trend_result, Exception) else {}
            
            # è®°å½•ç¬¬ä¸€è½®å®ŒæˆçŠ¶æ€
            round1_completed = 0
            for name, result in [("å¸‚åœºæ¦‚è§ˆ", overview_result), ("å¸‚åœºç”¨æˆ·ç”»åƒ", persona_result), 
                                 ("å¸‚åœºæœºä¼šæŒ–æ˜", opportunity_result), ("å¸‚åœºè¶‹åŠ¿åˆ†æ", trend_result)]:
                if isinstance(result, Exception):
                    logger.error(f"{name}ç”Ÿæˆå¤±è´¥: {result}")
                else:
                    logger.info(f"âœ… {name}å®Œæˆ")
                    round1_completed += 1
            
            logger.info(f"ğŸ“Š ç¬¬ä¸€è½®å®Œæˆ ({round1_completed}/4)ï¼Œå¼€å§‹ç¬¬äºŒè½®å¹¶è¡Œåˆ†æï¼ˆæˆ˜ç•¥å®šä½ã€åœºæ™¯åˆ†æã€è´¨é‡è¿­ä»£ã€è¡ŒåŠ¨ä¼˜å…ˆçº§ï¼‰...")
            
            positioning_result, context_result, roadmap_result, actions_result = await asyncio.gather(
                generate_strategic_positioning(),
                generate_usage_context_analysis(),
                generate_quality_roadmap(),
                generate_action_priorities(),
                return_exceptions=True
            )
            
            # å¤„ç†ç¬¬äºŒè½®ç»“æœ
            strategic_positioning = positioning_result if not isinstance(positioning_result, Exception) else {}
            usage_context_analysis = context_result if not isinstance(context_result, Exception) else {}
            quality_roadmap = roadmap_result if not isinstance(roadmap_result, Exception) else {}
            action_priorities = actions_result if not isinstance(actions_result, Exception) else {}
            
            # è®°å½•ç¬¬äºŒè½®å®ŒæˆçŠ¶æ€
            round2_completed = 0
            for name, result in [("æˆ˜ç•¥å®šä½", positioning_result), ("ä½¿ç”¨åœºæ™¯åˆ†æ", context_result), 
                                 ("è´¨é‡è¿­ä»£æ–¹å‘", roadmap_result), ("è¡ŒåŠ¨ä¼˜å…ˆçº§", actions_result)]:
                if isinstance(result, Exception):
                    logger.error(f"{name}ç”Ÿæˆå¤±è´¥: {result}")
                else:
                    logger.info(f"âœ… {name}å®Œæˆ")
                    round2_completed += 1
            
            logger.info(f"ğŸ‰ å…¨éƒ¨8ä¸ªåˆ†ææ¨¡å—å®Œæˆ (ç¬¬ä¸€è½®: {round1_completed}/4, ç¬¬äºŒè½®: {round2_completed}/4)")
            
            # 6. æ„å»ºäº§å“å¯¹æ¯”æ•°æ®ï¼ˆä¿ç•™å¯¹æ¯”è§†è§’ï¼‰
            product_profiles = []
            for i, info in enumerate(products_info, 1):
                product_profiles.append({
                    "product_index": i,
                    "product_name": info['name'],
                    "asin": info['asin'],
                    "image_url": info.get('image_url'),
                    "review_count": info.get('review_count', 0),
                    "five_w": info['data'].get('user_context', {}),
                    "dimensions": info['data'].get('key_insights', {})
                })
            
            # 7. æ„å»º data_statisticsï¼ˆçº¯æ•°æ®ç»Ÿè®¡ï¼Œå¯ç‚¹å‡»æŸ¥çœ‹åŸæ–‡ï¼‰
            data_statistics = self._build_data_statistics(market_aggregated)
            
            # 8. ç»„è£…æœ€ç»ˆç»“æœ
            result_data = {
                "analysis_type": "market_insight",
                "market_name": project.title,
                "product_count": product_count,
                "total_reviews": total_reviews,
                
                # ç¬¬ä¸€éƒ¨åˆ†ï¼šæ•°æ®ç»Ÿè®¡ï¼ˆçº¯æ•°æ®å±•ç¤ºï¼Œå¯ç‚¹å‡»æŸ¥çœ‹åŸæ–‡ï¼‰
                "data_statistics": data_statistics,
                
                # ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºäºæ•°æ®çš„æ¨ç†ï¼ˆAI åˆ†æè§‚ç‚¹ï¼Œ8æ¨¡å—å®Œæ•´ç‰ˆï¼‰
                "market_analysis": {
                    # æ¿å—A: å¸‚åœºæ ¼å±€
                    "market_overview": market_overview,
                    "strategic_positioning": strategic_positioning,
                    
                    # æ¿å—B: ç”¨æˆ·æ´å¯Ÿ
                    "market_persona": market_persona,
                    "usage_context_analysis": usage_context_analysis,
                    
                    # æ¿å—C: äº§å“ç­–ç•¥
                    "market_opportunities": market_opportunities,
                    "quality_roadmap": quality_roadmap,
                    
                    # æ¿å—D: è¿è¥è¡ŒåŠ¨
                    "market_trends": market_trends,
                    "action_priorities": action_priorities
                },
                
                # ç”Ÿæˆè¿›åº¦è¿½è¸ª
                "generation_progress": {
                    "total_modules": 8,
                    "completed_modules": round1_completed + round2_completed,
                    "round1_completed": round1_completed,
                    "round2_completed": round2_completed
                },
                
                # ä¿ç•™åŸæœ‰å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                "market_overview": market_overview,
                "market_persona": market_persona,
                "market_opportunities": market_opportunities,
                "market_trends": market_trends,
                "strategic_positioning": strategic_positioning,
                "usage_context_analysis": usage_context_analysis,
                "quality_roadmap": quality_roadmap,
                "action_priorities": action_priorities,
                "aggregated_data": market_aggregated,
                "product_profiles": product_profiles,
                # [NEW] é¡¹ç›®çº§å­¦ä¹ ç»“æœ
                "project_learning": {
                    "dimensions": learning_result.get("dimensions", {}),
                    "labels": learning_result.get("labels", {}),
                    "sample_stats": learning_result.get("sample_stats", {})
                }
            }
            
            project.result_content = result_data
            project.status = AnalysisStatus.COMPLETED.value
            project.error_message = None
            
            logger.info(f"å¸‚åœºæ´å¯Ÿåˆ†æå®Œæˆ: {project.id}")
            
        except Exception as e:
            logger.error(f"Market Insight Analysis Error: {e}", exc_info=True)
            project.status = AnalysisStatus.FAILED.value
            project.error_message = str(e)
        
        await self.db.commit()
        await self.db.refresh(project)
        return project

    async def _fetch_product_data_full(self, item: AnalysisProjectItem) -> Dict[str, Any]:
        """
        è·å–å•ä¸ªäº§å“çš„å®Œæ•´æ•°æ®ï¼ˆç”¨äºå¸‚åœºæ´å¯Ÿï¼‰
        """
        product = item.product
        
        # æ„å»ºå®‰å…¨çš„äº§å“åç§°
        raw_name = product.title_translated or product.title or product.asin
        safe_name = raw_name[:30].replace('"', '').replace("'", "").strip() + f" ({product.asin[-4:]})"
        
        # èšåˆæ ¸å¿ƒæ•°æ® (5W + Insight)
        context_stats = await self.summary_service._aggregate_5w_stats(product.id)
        insight_stats = await self.summary_service._aggregate_insight_stats(product.id)
        
        # è·å–è¯„è®ºæ•°
        review_count = await self.summary_service._count_translated_reviews(product.id)
        
        return {
            "name": safe_name,
            "asin": product.asin,
            "image_url": product.image_url,
            "review_count": review_count,
            "data": {
                "user_context": context_stats,
                "key_insights": insight_stats
            }
        }

    def _aggregate_market_data(self, products_info: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        èšåˆå¸‚åœºçº§åˆ«æ•°æ®ï¼ˆè·¨äº§å“æ ‡ç­¾åˆå¹¶ï¼‰
        
        å°†å¤šä¸ªäº§å“çš„5Wæ•°æ®å’Œæ´å¯Ÿæ•°æ®èšåˆï¼Œè®¡ç®—å¸‚åœºçº§åˆ«çš„ç»Ÿè®¡
        """
        from collections import defaultdict
        
        # 5Wç»´åº¦èšåˆ
        five_w_aggregated = defaultdict(lambda: defaultdict(int))
        # æ´å¯Ÿç»´åº¦èšåˆ
        insight_aggregated = defaultdict(lambda: defaultdict(int))
        
        for info in products_info:
            # èšåˆ5Wæ•°æ®
            context = info['data'].get('user_context', {})
            for dim_key in ['who', 'buyer', 'user', 'when', 'where', 'why', 'what']:
                dim_data = context.get(dim_key, {})
                items = dim_data.get('items', []) if isinstance(dim_data, dict) else []
                for item in items:
                    label = item.get('name', '')
                    count = item.get('value', 0)
                    if label:
                        five_w_aggregated[dim_key][label] += count
            
            # èšåˆæ´å¯Ÿæ•°æ®
            insights = info['data'].get('key_insights', {})
            for dim_key in ['strength', 'weakness', 'suggestion', 'scenario', 'emotion']:
                dim_data = insights.get(dim_key, {})
                items = dim_data.get('items', []) if isinstance(dim_data, dict) else []
                for item in items:
                    label = item.get('name', '')
                    count = item.get('value', 0)
                    if label:
                        insight_aggregated[dim_key][label] += count
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼å¹¶æ’åº
        def to_sorted_list(data: Dict[str, int], limit: int = 20) -> List[Dict[str, Any]]:
            sorted_items = sorted(data.items(), key=lambda x: x[1], reverse=True)[:limit]
            total = sum(data.values())
            return [
                {
                    "label": label,
                    "count": count,
                    "percentage": f"{(count / total * 100):.1f}%" if total > 0 else "0%"
                }
                for label, count in sorted_items
            ]
        
        # æ„å»ºèšåˆç»“æœ
        five_w_result = {}
        for dim_key in ['who', 'buyer', 'user', 'when', 'where', 'why', 'what']:
            five_w_result[dim_key] = to_sorted_list(dict(five_w_aggregated[dim_key]))
        
        dimensions_result = {}
        # æ˜ å°„æ´å¯Ÿç±»å‹åˆ°å‰ç«¯å±•ç¤ºåç§°
        insight_mapping = {
            'strength': 'pros',
            'weakness': 'cons',
            'suggestion': 'suggestion',
            'scenario': 'scenario',
            'emotion': 'emotion'
        }
        for dim_key, display_key in insight_mapping.items():
            dimensions_result[display_key] = to_sorted_list(dict(insight_aggregated[dim_key]))
        
        return {
            "five_w": five_w_result,
            "dimensions": dimensions_result,
            "total_products": len(products_info),
            "total_labels": sum(len(v) for v in five_w_aggregated.values()) + sum(len(v) for v in insight_aggregated.values())
        }

    def _generate_product_summaries_for_market(self, products_info: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆäº§å“æ‘˜è¦ï¼ˆç”¨äºå¸‚åœºæ´å¯Ÿï¼‰"""
        summaries = []
        for i, info in enumerate(products_info, 1):
            name = info.get("name", f"äº§å“{i}")
            asin = info.get("asin", "")
            review_count = info.get("review_count", 0)
            
            # æå–å…³é”®æ ‡ç­¾ - 5Wç”¨æˆ·ç”»åƒ
            context = info['data'].get('user_context', {})
            who_tags = self._extract_top_labels(context.get('who', {}))
            when_tags = self._extract_top_labels(context.get('when', {}))
            where_tags = self._extract_top_labels(context.get('where', {}))
            why_tags = self._extract_top_labels(context.get('why', {}))
            what_tags = self._extract_top_labels(context.get('what', {}))
            
            # æå–å…³é”®æ ‡ç­¾ - æ´å¯Ÿ
            insights = info['data'].get('key_insights', {})
            pros_tags = self._extract_top_labels(insights.get('strength', {}))
            cons_tags = self._extract_top_labels(insights.get('weakness', {}))
            
            summary = f"""äº§å“{i}: {name} ({asin}) - {review_count}æ¡è¯„è®º
  ã€ç”¨æˆ·ç”»åƒã€‘ç”¨æˆ·: {', '.join(who_tags) or 'æ— '} | æ—¶æœº: {', '.join(when_tags) or 'æ— '} | åœºæ™¯: {', '.join(where_tags) or 'æ— '}
  ã€è´­ä¹°åŠ¨æœºã€‘{', '.join(why_tags) or 'æ— '} | ç”¨é€”: {', '.join(what_tags) or 'æ— '}
  ã€å£ç¢‘ã€‘ä¼˜åŠ¿: {', '.join(pros_tags) or 'æ— '} | ç—›ç‚¹: {', '.join(cons_tags) or 'æ— '}"""
            summaries.append(summary)
        
        return "\n\n".join(summaries)

    def _extract_top_labels(self, data: Dict[str, Any], limit: int = 3) -> List[str]:
        """ä»èšåˆæ•°æ®ä¸­æå–Topæ ‡ç­¾"""
        if not isinstance(data, dict):
            return []
        items = data.get('items', [])
        return [item.get('name', '') for item in items[:limit] if isinstance(item, dict) and item.get('name')]

    def _build_data_statistics(self, aggregated_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ„å»ºæ•°æ®ç»Ÿè®¡ç»“æ„ï¼ˆç”¨äºæŠ¥å‘Šç¬¬ä¸€éƒ¨åˆ†å±•ç¤ºï¼‰
        
        å°† aggregated_data è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„ data_statistics æ ¼å¼ï¼š
        - five_w: 5W ç”¨æˆ·ç”»åƒæ•°æ®ç»Ÿè®¡
        - insights: æ´å¯Ÿæ•°æ®ç»Ÿè®¡
        
        æ¯ä¸ªç»´åº¦åŒ…å«ï¼šæ ‡ç­¾åç§°ã€å‡ºç°æ¬¡æ•°ã€å æ¯”
        """
        five_w = aggregated_data.get("five_w", {})
        dimensions = aggregated_data.get("dimensions", {})
        
        # 5W ç”¨æˆ·ç”»åƒæ•°æ®ç»Ÿè®¡
        five_w_stats = {}
        for dim_key in ['buyer', 'user', 'where', 'when', 'why', 'what']:
            dim_data = five_w.get(dim_key, [])
            if isinstance(dim_data, list):
                five_w_stats[dim_key] = [
                    {
                        "label": item.get("label", ""),
                        "count": item.get("count", 0),
                        "percentage": item.get("percentage", "0%")
                    }
                    for item in dim_data
                    if isinstance(item, dict) and item.get("label")
                ]
            else:
                five_w_stats[dim_key] = []
        
        # æ´å¯Ÿæ•°æ®ç»Ÿè®¡
        # æ˜ å°„ pros/cons åˆ° strength/weakness
        insight_mapping = {
            'strength': 'pros',
            'weakness': 'cons',
            'suggestion': 'suggestion',
            'scenario': 'scenario',
            'emotion': 'emotion'
        }
        
        insight_stats = {}
        for insight_key, source_key in insight_mapping.items():
            dim_data = dimensions.get(source_key, [])
            if isinstance(dim_data, list):
                insight_stats[insight_key] = [
                    {
                        "label": item.get("label", ""),
                        "count": item.get("count", 0),
                        "percentage": item.get("percentage", "0%")
                    }
                    for item in dim_data
                    if isinstance(item, dict) and item.get("label")
                ]
            else:
                insight_stats[insight_key] = []
        
        return {
            "five_w": five_w_stats,
            "insights": insight_stats,
            "total_products": aggregated_data.get("total_products", 0),
            "total_labels": aggregated_data.get("total_labels", 0)
        }
