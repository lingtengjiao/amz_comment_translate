"""
è¯„è®ºå…¥åº“æœåŠ¡ (Ingestion Service)

å¤„ç†ä» Redis é˜Ÿåˆ—æ¶ˆè´¹çš„è¯„è®ºæ•°æ®ï¼Œæ‰¹é‡å…¥åº“åˆ° PostgreSQLã€‚

è®¾è®¡ç‰¹ç‚¹ï¼š
1. æŒ‰ ASIN åˆ†ç»„å¤„ç†ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢
2. ä¸‰å±‚å»é‡ï¼šRedis Set â†’ å†…å­˜ Set â†’ DB ON CONFLICT
3. å…¥åº“æˆåŠŸåæ›´æ–° Redis Set å’Œæ‰¹æ¬¡çŠ¶æ€
"""
import logging
from collections import defaultdict
from typing import List, Dict, Tuple
from datetime import datetime
import json

from sqlalchemy import select, and_, func
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.orm import Session

from app.models.product import Product
from app.models.review import Review, TranslationStatus
from app.services.deduplicator import ReviewDeduplicatorSync, deduplicate_in_memory
from app.core.redis import BatchStatusTrackerSync, get_sync_redis

logger = logging.getLogger(__name__)


class IngestionService:
    """
    è¯„è®ºå…¥åº“æœåŠ¡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äº Celery Workerï¼‰
    """
    
    def __init__(self, db: Session, redis_client=None):
        self.db = db
        self.redis = redis_client or get_sync_redis()
        self.deduplicator = ReviewDeduplicatorSync(self.redis)
        self.batch_tracker = BatchStatusTrackerSync(self.redis)
    
    def process_queue_items(self, items: List[dict]) -> Dict[str, dict]:
        """
        å¤„ç†ä»é˜Ÿåˆ—ä¸­å–å‡ºçš„æ•°æ®
        
        Args:
            items: ä» Redis é˜Ÿåˆ—å–å‡ºçš„åŸå§‹æ•°æ®åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡ {asin: {inserted: N, skipped: N}}
        """
        if not items:
            return {}
        
        # Step 1: æŒ‰ ASIN åˆ†ç»„
        grouped = defaultdict(list)
        product_info_map = {}
        batch_ids = set()
        user_id_map = {}  # [NEW] ASIN -> user_id æ˜ å°„
        
        for item in items:
            asin = item.get("asin")
            if not asin:
                logger.warning("Item without ASIN, skipping")
                continue
            
            grouped[asin].append(item)
            
            # ä¿å­˜äº§å“ä¿¡æ¯ï¼ˆå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ï¼‰
            if asin not in product_info_map:
                product_info_map[asin] = {
                    "title": item.get("title"),
                    "image_url": item.get("image_url"),
                    "marketplace": item.get("marketplace", "US"),
                    "average_rating": item.get("average_rating"),
                    "price": item.get("price"),
                    "bullet_points": item.get("bullet_points"),
                    "categories": item.get("categories")  # [NEW] äº§å“ç±»ç›®
                }
            
            # [NEW] æ”¶é›† user_idï¼ˆå–ç¬¬ä¸€ä¸ªéç©ºçš„ï¼‰
            if asin not in user_id_map and item.get("user_id"):
                user_id_map[asin] = item["user_id"]
            
            # æ”¶é›†æ‰¹æ¬¡ ID
            if item.get("batch_id"):
                batch_ids.add(item["batch_id"])
        
        # Step 2: é€ ASIN å¤„ç†
        results = {}
        
        for asin, asin_items in grouped.items():
            try:
                inserted, skipped = self._process_asin(
                    asin=asin,
                    items=asin_items,
                    product_info=product_info_map.get(asin, {}),
                    user_id=user_id_map.get(asin)  # [NEW] ä¼ é€’ user_id
                )
                results[asin] = {"inserted": inserted, "skipped": skipped}
                
            except Exception as e:
                logger.error(f"Error processing ASIN {asin}: {e}")
                self.db.rollback()
                results[asin] = {"inserted": 0, "skipped": 0, "error": str(e)}
        
        # Step 3: æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
        for batch_id in batch_ids:
            # æ±‡æ€»è¯¥æ‰¹æ¬¡çš„ç»“æœ
            total_inserted = sum(r.get("inserted", 0) for r in results.values())
            total_skipped = sum(r.get("skipped", 0) for r in results.values())
            self.batch_tracker.update(batch_id, "completed", total_inserted, total_skipped)
        
        # Step 4: ğŸš€ ç¼“å­˜å¤±æ•ˆ - æ¸…é™¤æœ‰æ–°æ•°æ®å…¥åº“çš„äº§å“ç¼“å­˜
        from app.core.cache import get_cache_service_sync
        cache = get_cache_service_sync()
        
        for asin, result in results.items():
            if result.get("inserted", 0) > 0:
                cache.invalidate_all_for_product(asin)
                logger.info(f"[Cache] Invalidated caches for product {asin}")
        
        # æ¸…é™¤ç›¸å…³ç”¨æˆ·çš„é¡¹ç›®åˆ—è¡¨ç¼“å­˜
        for user_id in set(user_id_map.values()):
            if user_id:
                cache.delete_pattern(f"cache:user_projects:{user_id}:*")
                logger.info(f"[Cache] Invalidated user projects cache for user {user_id}")
        
        return results
    
    def _process_asin(
        self,
        asin: str,
        items: List[dict],
        product_info: dict,
        user_id: str = None  # [NEW] ç”¨æˆ· IDï¼ˆå¯é€‰ï¼‰
    ) -> Tuple[int, int]:
        """
        å¤„ç†å•ä¸ª ASIN çš„æ•°æ®
        
        Returns:
            (inserted_count, skipped_count)
        """
        # åˆå¹¶æ‰€æœ‰è¯„è®º
        all_reviews = []
        for item in items:
            reviews = item.get("reviews", [])
            all_reviews.extend(reviews)
        
        if not all_reviews:
            return 0, 0
        
        # Step 1: Redis é¢„è¿‡æ»¤
        filtered_reviews, skipped_redis, new_ids = self.deduplicator.filter_new_reviews(
            asin, all_reviews
        )
        
        if not filtered_reviews:
            logger.info(f"[{asin}] å…¨éƒ¨ {len(all_reviews)} æ¡è¯„è®ºå·²å­˜åœ¨ï¼Œè·³è¿‡")
            
            # [FIXED] å³ä½¿æ²¡æœ‰æ–°è¯„è®ºï¼Œä¹Ÿè¦åˆ›å»º UserProject å…³è”
            # è¿™æ ·ç”¨æˆ·é‡‡é›†å·²æœ‰äº§å“æ—¶ï¼Œä¹Ÿèƒ½åœ¨"æˆ‘çš„é¡¹ç›®"ä¸­çœ‹åˆ°
            if user_id:
                # éœ€è¦å…ˆè·å–äº§å“ ID
                product = self._get_or_create_product(asin, product_info)
                self._create_or_update_user_project(user_id, product.id, 0)
            
            return 0, skipped_redis
        
        # Step 2: å†…å­˜å»é‡
        unique_reviews = deduplicate_in_memory(filtered_reviews)
        skipped_memory = len(filtered_reviews) - len(unique_reviews)
        
        # Step 3: è·å–æˆ–åˆ›å»ºäº§å“
        product = self._get_or_create_product(asin, product_info)
        
        # Step 4: æ‰¹é‡å…¥åº“
        inserted, skipped_db = self._bulk_insert_reviews(product.id, unique_reviews)
        
        # Step 5: æ›´æ–° Redis Setï¼ˆåªæ ‡è®°çœŸæ­£å…¥åº“çš„ï¼‰
        if inserted > 0:
            # è·å–å®é™…å…¥åº“çš„ review_id
            inserted_ids = [r.get("review_id") for r in unique_reviews[:inserted] if r.get("review_id")]
            self.deduplicator.mark_as_seen(asin, inserted_ids)
        
        # [FIXED] Step 6: åˆ›å»ºç”¨æˆ·é¡¹ç›®å…³è”
        # æ— è®ºæ˜¯å¦æœ‰æ–°è¯„è®ºå…¥åº“ï¼Œåªè¦ç”¨æˆ·é‡‡é›†äº†äº§å“ï¼Œå°±åˆ›å»ºå…³è”
        # è¿™æ ·ç”¨æˆ· B é‡‡é›†å·²æœ‰äº§å“æ—¶ï¼Œä¹Ÿèƒ½åœ¨"æˆ‘çš„é¡¹ç›®"ä¸­çœ‹åˆ°
        if user_id:
            self._create_or_update_user_project(user_id, product.id, inserted)
        
        total_skipped = skipped_redis + skipped_memory + skipped_db
        
        logger.info(
            f"[{asin}] å¤„ç†å®Œæˆ: æ”¶åˆ° {len(all_reviews)}, "
            f"Redisè¿‡æ»¤ {skipped_redis}, å†…å­˜å»é‡ {skipped_memory}, "
            f"DBå»é‡ {skipped_db}, å…¥åº“ {inserted}"
        )
        
        return inserted, total_skipped
    
    def _get_or_create_product(self, asin: str, info: dict) -> Product:
        """è·å–æˆ–åˆ›å»ºäº§å“"""
        result = self.db.execute(
            select(Product).where(Product.asin == asin)
        )
        product = result.scalar_one_or_none()
        
        if product:
            # ğŸ”§ [FIX] æ™ºèƒ½æ›´æ–°å­—æ®µï¼šå…è®¸ç”¨æ›´å®Œæ•´çš„æ•°æ®è¦†ç›–å ä½æ•°æ®
            # æ›´æ–° titleï¼šå¦‚æœæ–°æ•°æ®æ›´é•¿ï¼ˆè¯´æ˜æ›´å®Œæ•´ï¼‰ï¼Œå°±æ›´æ–°
            if info.get("title"):
                if not product.title or len(info["title"]) > len(product.title):
                    product.title = info["title"]
                    product.title_translated = None  # æ¸…ç©ºç¿»è¯‘
            if info.get("image_url") and not product.image_url:
                product.image_url = info["image_url"]
            if info.get("average_rating") is not None:
                product.average_rating = str(info["average_rating"])
            if info.get("price") and not product.price:
                product.price = info["price"]
            # ğŸ”§ [FIX] æ™ºèƒ½æ›´æ–° bullet_pointsï¼š
            # 1. å¦‚æœæ²¡æœ‰æ—§æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
            # 2. å¦‚æœæ—§æ•°æ®æ˜¯å ä½æ•°æ®ï¼ˆæ€»å­—ç¬¦æ•°<100ï¼‰ï¼Œç”¨æ–°æ•°æ®è¦†ç›–
            # 3. å¦‚æœæ–°æ•°æ®æ¯”æ—§æ•°æ®å†…å®¹æ›´ä¸°å¯Œï¼ˆæ€»å­—ç¬¦æ•°å¤š2å€ä»¥ä¸Šï¼‰ï¼Œç”¨æ–°æ•°æ®è¦†ç›–
            if info.get("bullet_points"):
                bp = info["bullet_points"]
                new_bp_json = None
                
                # ç»Ÿä¸€è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
                if isinstance(bp, list):
                    new_bp_json = json.dumps(bp, ensure_ascii=False)
                elif isinstance(bp, str):
                    try:
                        parsed = json.loads(bp)
                        if isinstance(parsed, list):
                            new_bp_json = bp
                        else:
                            new_bp_json = json.dumps([bp], ensure_ascii=False)
                    except json.JSONDecodeError:
                        new_bp_json = json.dumps([bp], ensure_ascii=False) if bp else None
                
                if new_bp_json:
                    should_update = False
                    
                    if not product.bullet_points:
                        # æ²¡æœ‰æ—§æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨æ–°æ•°æ®
                        should_update = True
                    else:
                        # åˆ¤æ–­æ—§æ•°æ®æ˜¯å¦æ˜¯å ä½æ•°æ®
                        old_len = len(product.bullet_points)
                        new_len = len(new_bp_json)
                        
                        # å ä½æ•°æ®åˆ¤æ–­ï¼šæ—§æ•°æ®æ€»é•¿åº¦ < 100 å­—ç¬¦ï¼ˆå¦‚ '["Feature 1","Feature 2"]'ï¼‰
                        if old_len < 100:
                            should_update = True
                        # æ–°æ•°æ®æ˜æ˜¾æ›´ä¸°å¯Œï¼šæ˜¯æ—§æ•°æ®çš„ 2 å€ä»¥ä¸Š
                        elif new_len > old_len * 2:
                            should_update = True
                    
                    if should_update:
                        product.bullet_points = new_bp_json
                        # æ¸…ç©ºç¿»è¯‘ï¼Œç­‰å¾…é‡æ–°ç¿»è¯‘
                        product.bullet_points_translated = None
            # ğŸ”§ [FIX] æ™ºèƒ½æ›´æ–°ç±»ç›®ä¿¡æ¯ï¼šå…è®¸ç”¨æ›´å®Œæ•´çš„æ•°æ®è¦†ç›–
            if info.get("categories"):
                cats = info["categories"]
                new_cats_json = None
                
                # ç¡®ä¿ categories æ˜¯ JSON å­—ç¬¦ä¸²æ ¼å¼
                if isinstance(cats, list):
                    new_cats_json = json.dumps(cats, ensure_ascii=False)
                elif isinstance(cats, str):
                    try:
                        json.loads(cats)
                        new_cats_json = cats
                    except json.JSONDecodeError:
                        pass
                
                if new_cats_json:
                    # å¦‚æœæ²¡æœ‰æ—§æ•°æ®ï¼Œæˆ–æ–°æ•°æ®æ›´ä¸°å¯Œï¼Œå°±æ›´æ–°
                    if not product.categories or len(new_cats_json) > len(product.categories):
                        product.categories = new_cats_json
            self.db.flush()
            return product
        
        # åˆ›å»ºæ–°äº§å“
        bullet_points = info.get("bullet_points")
        # ç»Ÿä¸€å­˜å‚¨ä¸º JSON å­—ç¬¦ä¸²æ ¼å¼
        if isinstance(bullet_points, list):
            bullet_points = json.dumps(bullet_points, ensure_ascii=False)
        elif isinstance(bullet_points, str):
            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON æ•°ç»„
            try:
                parsed = json.loads(bullet_points)
                if not isinstance(parsed, list):
                    bullet_points = json.dumps([bullet_points], ensure_ascii=False)
            except json.JSONDecodeError:
                bullet_points = json.dumps([bullet_points], ensure_ascii=False) if bullet_points else None
        else:
            bullet_points = None
        
        # [NEW] å¤„ç†ç±»ç›®ä¿¡æ¯
        categories = info.get("categories")
        if isinstance(categories, list):
            categories = json.dumps(categories, ensure_ascii=False)
        elif isinstance(categories, str):
            try:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆ JSON
                json.loads(categories)
            except json.JSONDecodeError:
                categories = None
        else:
            categories = None
        
        product = Product(
            asin=asin,
            title=info.get("title"),
            image_url=info.get("image_url"),
            marketplace=info.get("marketplace", "US"),
            average_rating=str(info["average_rating"]) if info.get("average_rating") else None,
            price=info.get("price"),
            bullet_points=bullet_points,
            categories=categories
        )
        self.db.add(product)
        self.db.flush()
        
        logger.info(f"Created new product: {asin}")
        return product
    
    def _bulk_insert_reviews(
        self,
        product_id,
        reviews: List[dict]
    ) -> Tuple[int, int]:
        """
        æ‰¹é‡æ’å…¥è¯„è®º
        
        Returns:
            (inserted_count, skipped_count)
        """
        if not reviews:
            return 0, 0
        
        # å‡†å¤‡è®°å½•
        records = []
        skipped_invalid = 0
        
        for r in reviews:
            review_id = r.get("review_id")
            if not review_id:
                skipped_invalid += 1
                continue
            
            body = r.get("body", "")
            if not body and r.get("rating", 0) == 0:
                skipped_invalid += 1
                continue
            
            # è§£ææ—¥æœŸ
            review_date = None
            date_str = r.get("review_date")
            if date_str:
                for fmt in ["%B %d, %Y", "%Y-%m-%d", "%d %B %Y", "%b %d, %Y", "%d/%m/%Y"]:
                    try:
                        review_date = datetime.strptime(date_str, fmt).date()
                        break
                    except ValueError:
                        continue
            
            # å¤„ç†å›¾ç‰‡
            image_urls = r.get("image_urls")
            image_urls_json = None
            if image_urls and isinstance(image_urls, list):
                image_urls_json = json.dumps(image_urls)
            
            # æˆªæ–­è¿‡é•¿å­—æ®µ
            author = r.get("author")
            if author and len(author) > 500:
                author = author[:497] + "..."
            
            video_url = r.get("video_url")
            if video_url and len(video_url) > 500:
                video_url = video_url[:500]
            
            review_url = r.get("review_url")
            if review_url and len(review_url) > 500:
                review_url = review_url[:500]
            if not review_url and review_id.startswith('R'):
                review_url = f"https://www.amazon.com/gp/customer-reviews/{review_id}"
            
            import uuid
            records.append({
                "id": uuid.uuid4(),
                "product_id": product_id,
                "review_id": review_id,
                "author": author,
                "rating": r.get("rating", 0),
                "title_original": r.get("title"),
                "body_original": body,
                "review_date": review_date,
                "verified_purchase": r.get("verified_purchase", False),
                "helpful_votes": r.get("helpful_votes", 0),
                "has_video": r.get("has_video", False),
                "has_images": r.get("has_images", False),
                "image_urls": image_urls_json,
                "video_url": video_url,
                "review_url": review_url,
                "sentiment": "neutral",
                "translation_status": TranslationStatus.PENDING.value
            })
        
        if not records:
            return 0, skipped_invalid
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„ review_id
        review_ids = [r["review_id"] for r in records]
        existing_result = self.db.execute(
            select(Review.review_id).where(
                and_(
                    Review.product_id == product_id,
                    Review.review_id.in_(review_ids)
                )
            )
        )
        existing_ids = set(existing_result.scalars().all())
        skipped_existing = len(existing_ids)
        
        # ä½¿ç”¨ ON CONFLICT DO NOTHING
        stmt = insert(Review).values(records)
        stmt = stmt.on_conflict_do_nothing(
            index_elements=['product_id', 'review_id']
        )
        
        self.db.execute(stmt)
        self.db.commit()
        
        inserted = len(records) - skipped_existing
        
        return inserted, skipped_invalid + skipped_existing
    
    def _create_or_update_user_project(self, user_id: str, product_id, reviews_count: int):
        """
        åˆ›å»ºæˆ–æ›´æ–°ç”¨æˆ·é¡¹ç›®å…³è”
        
        [FIXED] æ— è®º reviews_count æ˜¯å¦ > 0ï¼Œéƒ½ä¼šåˆ›å»º/æ›´æ–°å…³è”
        è¿™æ ·ç”¨æˆ·é‡‡é›†å·²æœ‰äº§å“æ—¶ï¼Œä¹Ÿèƒ½åœ¨"æˆ‘çš„é¡¹ç›®"ä¸­çœ‹åˆ°
        
        Args:
            user_id: ç”¨æˆ· UUIDï¼ˆå­—ç¬¦ä¸²ï¼‰
            product_id: äº§å“ UUID
            reviews_count: æœ¬æ¬¡è´¡çŒ®çš„è¯„è®ºæ•°ï¼ˆå¯èƒ½ä¸º 0ï¼‰
        """
        from app.models.user_project import UserProject
        from uuid import UUID
        from datetime import datetime
        
        try:
            # è½¬æ¢ user_id ä¸º UUID
            user_uuid = UUID(user_id) if isinstance(user_id, str) else user_id
            
            # æŸ¥æ‰¾ç°æœ‰å…³è”
            result = self.db.execute(
                select(UserProject).where(
                    and_(
                        UserProject.user_id == user_uuid,
                        UserProject.product_id == product_id
                    )
                )
            )
            user_project = result.scalar_one_or_none()
            
            if user_project:
                # å·²å­˜åœ¨ï¼šæ›´æ–°è´¡çŒ®æ•°å’Œè®¿é—®æ—¶é—´
                if reviews_count > 0:
                    user_project.reviews_contributed = (user_project.reviews_contributed or 0) + reviews_count
                    logger.info(f"[UserProject] æ›´æ–°ç”¨æˆ· {user_id} çš„é¡¹ç›®å…³è”ï¼Œæ–°å¢è´¡çŒ® {reviews_count} æ¡")
                else:
                    # å³ä½¿æ²¡æœ‰æ–°è¯„è®ºï¼Œä¹Ÿæ›´æ–°è®¿é—®æ—¶é—´
                    logger.info(f"[UserProject] ç”¨æˆ· {user_id} é‡æ–°é‡‡é›†äº§å“ï¼Œè¯„è®ºå…¨éƒ¨å·²å­˜åœ¨ï¼Œæ›´æ–°è®¿é—®æ—¶é—´")
                user_project.last_viewed_at = datetime.now()
            else:
                # ä¸å­˜åœ¨ï¼šåˆ›å»ºæ–°å…³è”
                user_project = UserProject(
                    user_id=user_uuid,
                    product_id=product_id,
                    reviews_contributed=reviews_count  # å¯èƒ½ä¸º 0
                )
                self.db.add(user_project)
                if reviews_count > 0:
                    logger.info(f"[UserProject] åˆ›å»ºç”¨æˆ· {user_id} çš„é¡¹ç›®å…³è”ï¼Œè´¡çŒ® {reviews_count} æ¡")
                else:
                    logger.info(f"[UserProject] åˆ›å»ºç”¨æˆ· {user_id} çš„é¡¹ç›®å…³è”ï¼ˆäº§å“å·²æœ‰å†å²æ•°æ®ï¼‰")
            
            self.db.commit()
            
        except Exception as e:
            logger.error(f"[UserProject] åˆ›å»º/æ›´æ–°å¤±è´¥: {e}")
            self.db.rollback()
    
    def sync_redis_from_db(self, asin: str):
        """
        ä»æ•°æ®åº“åŒæ­¥ review_id åˆ° Redis
        ç”¨äºå†·å¯åŠ¨æˆ– Redis é‡å¯åæ¢å¤
        """
        result = self.db.execute(
            select(Product).where(Product.asin == asin)
        )
        product = result.scalar_one_or_none()
        
        if not product:
            return
        
        # è·å–æ‰€æœ‰ review_id
        review_result = self.db.execute(
            select(Review.review_id).where(Review.product_id == product.id)
        )
        review_ids = [r[0] for r in review_result.all()]
        
        if review_ids:
            self.deduplicator.sync_from_db(asin, review_ids)
